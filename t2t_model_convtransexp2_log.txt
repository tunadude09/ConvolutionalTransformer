nohup: ignoring input
WARNING: Logging before flag parsing goes to stderr.
W0901 08:07:02.063106 139951894841152 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0901 08:07:02.973531 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/expert_utils.py:68: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0901 08:07:04.021321 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/rl/gym_utils.py:235: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

W0901 08:07:04.023476 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-datagen:27: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W0901 08:07:04.023591 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-datagen:27: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W0901 08:07:04.023732 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-datagen:28: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

I0901 08:07:04.024030 139951894841152 usr_dir.py:43] Importing user module Language_Model_April2019_Restart from path /home/chrisf/t2t_user_dir/DEFENSE_langage_model_experiements
W0901 08:07:04.027014 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/adafactor.py:27: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0901 08:07:04.027348 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/multistep_optimizer.py:32: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0901 08:07:04.030021 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/bin/t2t_datagen.py:204: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

I0901 08:07:04.030175 139951894841152 t2t_datagen.py:207] Generating problems:
    translate:
      * translate_ende_wmt8k
W0901 08:07:04.030295 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/bin/t2t_datagen.py:156: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

I0901 08:07:04.030708 139951894841152 t2t_datagen.py:280] Generating data for translate_ende_wmt8k.
W0901 08:07:04.031041 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/data_generators/translate.py:170: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.

I0901 08:07:04.031129 139951894841152 translate.py:172] Skipping compile data, found files:
/home/chrisf/t2t_datagen/translate_ende_wmt8k-compiled-train.lang1
/home/chrisf/t2t_datagen/translate_ende_wmt8k-compiled-train.lang2
I0901 08:07:04.031254 139951894841152 generator_utils.py:346] Found vocab file: /home/chrisf/t2t_data/vocab.translate_ende_wmt8k.8192.subwords
W0901 08:07:04.031354 139951894841152 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/data_generators/text_encoder.py:940: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.

I0901 08:07:04.049733 139951894841152 generator_utils.py:153] Skipping generator because outputs files exists at ['/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00000-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00001-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00002-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00003-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00004-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00005-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00006-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00007-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00008-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00009-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00010-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00011-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00012-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00013-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00014-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00015-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00016-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00017-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00018-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00019-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00020-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00021-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00022-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00023-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00024-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00025-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00026-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00027-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00028-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00029-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00030-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00031-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00032-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00033-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00034-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00035-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00036-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00037-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00038-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00039-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00040-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00041-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00042-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00043-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00044-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00045-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00046-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00047-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00048-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00049-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00050-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00051-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00052-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00053-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00054-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00055-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00056-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00057-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00058-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00059-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00060-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00061-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00062-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00063-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00064-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00065-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00066-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00067-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00068-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00069-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00070-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00071-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00072-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00073-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00074-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00075-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00076-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00077-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00078-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00079-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00080-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00081-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00082-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00083-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00084-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00085-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00086-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00087-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00088-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00089-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00090-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00091-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00092-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00093-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00094-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00095-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00096-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00097-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00098-of-00100', '/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-train-00099-of-00100']
I0901 08:07:04.051706 139951894841152 translate.py:172] Skipping compile data, found files:
/home/chrisf/t2t_datagen/translate_ende_wmt8k-compiled-dev.lang1
/home/chrisf/t2t_datagen/translate_ende_wmt8k-compiled-dev.lang2
I0901 08:07:04.051825 139951894841152 generator_utils.py:346] Found vocab file: /home/chrisf/t2t_data/vocab.translate_ende_wmt8k.8192.subwords
I0901 08:07:04.069691 139951894841152 generator_utils.py:153] Skipping generator because outputs files exists at ['/home/chrisf/t2t_data/translate_ende_wmt8k-unshuffled-dev-00000-of-00001']
I0901 08:07:04.071374 139951894841152 generator_utils.py:527] Skipping shuffle because output files exist
WARNING: Logging before flag parsing goes to stderr.
W0901 08:07:05.130887 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/expert_utils.py:68: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0901 08:07:05.433019 140646288807744 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0901 08:07:06.800926 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/adafactor.py:27: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0901 08:07:06.801254 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/multistep_optimizer.py:32: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0901 08:07:06.813663 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/mesh_tensorflow/ops.py:4237: The name tf.train.CheckpointSaverListener is deprecated. Please use tf.estimator.CheckpointSaverListener instead.

W0901 08:07:06.813837 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/mesh_tensorflow/ops.py:4260: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

W0901 08:07:06.824575 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/models/research/neural_stack.py:38: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

W0901 08:07:06.846369 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/rl/gym_utils.py:235: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

W0901 08:07:06.856435 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:111: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0901 08:07:06.864125 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_gan/python/contrib_utils.py:305: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.

W0901 08:07:06.864235 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_gan/python/contrib_utils.py:310: The name tf.estimator.tpu.TPUEstimatorSpec is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimatorSpec instead.

W0901 08:07:07.239286 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-trainer:32: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W0901 08:07:07.239425 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-trainer:32: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W0901 08:07:07.239538 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-trainer:33: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

I0901 08:07:07.239881 140646288807744 usr_dir.py:43] Importing user module Language_Model_April2019_Restart from path /home/chrisf/t2t_user_dir/DEFENSE_langage_model_experiements
I0901 08:07:07.240998 140646288807744 t2t_trainer.py:155] Found unparsed command-line arguments. Checking if any start with --hp_ and interpreting those as hparams settings.
W0901 08:07:07.241153 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/bin/t2t_trainer.py:165: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.

W0901 08:07:07.241241 140646288807744 t2t_trainer.py:165] Found unknown flag: --allow_growth=True
W0901 08:07:07.241545 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/hparams_lib.py:49: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.

W0901 08:07:07.241722 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:839: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

W0901 08:07:07.242315 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:123: The name tf.GraphOptions is deprecated. Please use tf.compat.v1.GraphOptions instead.

W0901 08:07:07.242491 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:129: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0901 08:07:07.242626 140646288807744 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:242: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.
Instructions for updating:
When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.
I0901 08:07:07.242718 140646288807744 trainer_lib.py:265] Configuring DataParallelism to replicate the model.
I0901 08:07:07.242762 140646288807744 devices.py:76] schedule=continuous_train_and_eval
I0901 08:07:07.242798 140646288807744 devices.py:77] worker_gpu=1
I0901 08:07:07.242860 140646288807744 devices.py:78] sync=False
W0901 08:07:07.242894 140646288807744 devices.py:141] Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.
I0901 08:07:07.242964 140646288807744 devices.py:170] datashard_devices: ['gpu:0']
I0901 08:07:07.243138 140646288807744 devices.py:171] caching_devices: None
I0901 08:07:07.243208 140646288807744 devices.py:172] ps_devices: ['gpu:0']
W0901 08:07:07.243325 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/data_generators/text_encoder.py:940: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.

I0901 08:07:07.261751 140646288807744 estimator.py:209] Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fea60eb4b10>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {
  per_process_gpu_memory_fraction: 0.95
}
allow_soft_placement: true
graph_options {
  optimizer_options {
    global_jit_level: OFF
  }
}
isolate_session_state: true
, '_save_checkpoints_steps': 1000, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fea60eb4b90>}
W0901 08:07:07.261975 140646288807744 model_fn.py:630] Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7fea61777d40>) includes params argument, but params are not passed to Estimator.
W0901 08:07:07.262090 140646288807744 trainer_lib.py:783] ValidationMonitor only works with --schedule=train_and_evaluate
W0901 08:07:07.262411 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/bin/t2t_trainer.py:328: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

I0901 08:07:07.268365 140646288807744 estimator_training.py:186] Not using Distribute Coordinator.
I0901 08:07:07.268587 140646288807744 training.py:612] Running training and evaluation locally (non-distributed).
I0901 08:07:07.268780 140646288807744 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.
W0901 08:07:07.271750 140646288807744 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0901 08:07:07.276752 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-train*
I0901 08:07:07.277988 140646288807744 problem.py:670] partition: 0 num_data_files: 100
W0901 08:07:07.279281 140646288807744 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/data_generators/problem.py:680: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0901 08:07:07.316595 140646288807744 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/data_reader.py:275: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and: 
`tf.data.TFRecordDataset(path)`
W0901 08:07:07.361349 140646288807744 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/data_reader.py:37: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0901 08:07:07.388617 140646288807744 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/grouping.py:193: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0901 08:07:07.416802 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/data_reader.py:231: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0901 08:07:07.423435 140646288807744 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/data_reader.py:233: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
I0901 08:07:07.456321 140646288807744 estimator.py:1145] Calling model_fn.
I0901 08:07:07.463811 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'train'
W0901 08:07:07.509115 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py:244: The name tf.summary.text is deprecated. Please use tf.compat.v1.summary.text instead.

I0901 08:07:07.992164 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 08:07:08.251921 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 08:07:08.337822 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 08:07:08.345650 140646288807744 t2t_model.py:2248] Building model body
W0901 08:07:08.364105 140646288807744 deprecation.py:506] From /home/chrisf/t2t_user_dir/DEFENSE_langage_model_experiements/Language_Model_April2019_Restart/ConvTransformer_T2TApril2019_2.py:2677: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0901 08:07:08.390531 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/layers/common_layers.py:3077: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.

W0901 08:07:08.622190 140646288807744 deprecation.py:506] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0901 08:07:08.687195 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/layers/common_attention.py:1249: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

I0901 08:07:11.998860 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
W0901 08:07:12.066810 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/learning_rate.py:120: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

I0901 08:07:12.067735 140646288807744 learning_rate.py:29] Base learning rate: 0.100000
I0901 08:07:12.075135 140646288807744 optimize.py:338] Trainable Variables Total size: 61136632
I0901 08:07:12.075323 140646288807744 optimize.py:338] Non-trainable variables Total size: 5
I0901 08:07:12.075517 140646288807744 optimize.py:193] Using optimizer adam
I0901 08:07:18.871071 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 08:07:18.872188 140646288807744 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
I0901 08:07:21.188378 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 08:07:21.188631: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-09-01 08:07:21.212256: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz
2019-09-01 08:07:21.213281: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5632f8e491e0 executing computations on platform Host. Devices:
2019-09-01 08:07:21.213394: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-09-01 08:07:21.214478: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-09-01 08:07:21.239348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:07:21.239729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 08:07:21.239886: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 08:07:21.240836: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 08:07:21.241838: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 08:07:21.242054: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 08:07:21.243034: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 08:07:21.243646: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 08:07:21.245561: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 08:07:21.245699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:07:21.246050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:07:21.246331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 08:07:21.246373: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 08:07:21.306246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 08:07:21.306281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 08:07:21.306294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 08:07:21.306497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:07:21.306839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:07:21.307207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:07:21.307545: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2019-09-01 08:07:21.307585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-09-01 08:07:21.308784: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5633045f4170 executing computations on platform CUDA. Devices:
2019-09-01 08:07:21.308818: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-09-01 08:07:23.019501: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
I0901 08:07:24.188928 140646288807744 session_manager.py:500] Running local_init_op.
I0901 08:07:24.367757 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 08:07:30.802317 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
2019-09-01 08:07:39.981413: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 08:07:41.386375: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
I0901 08:07:42.670624 140646288807744 basic_session_run_hooks.py:262] loss = 8.271112, step = 0
2019-09-01 08:07:50.298409: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.298447: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-09-01 08:07:50.298879: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.298893: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-09-01 08:07:50.487806: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.487842: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-09-01 08:07:50.488575: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.488591: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-09-01 08:07:50.489950: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.489985: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-09-01 08:07:50.490643: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.490676: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-09-01 08:07:50.581162: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.581194: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-09-01 08:07:50.581623: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.581635: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-09-01 08:07:50.659271: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.659303: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.91GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-09-01 08:07:50.659732: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.659745: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.91GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-09-01 08:07:50.661400: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.661845: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.664268: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.664711: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.666230: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.666676: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.788480: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.788940: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.790490: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.790931: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.887092: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.887558: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.943301: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:50.943748: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.067671: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.068350: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.070285: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.070918: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.197146: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.197799: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.199786: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.200429: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.564703: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.565753: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.569776: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.570314: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.739833: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.740301: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.742079: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.742516: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.877606: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.878105: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.879921: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:51.880473: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.112391: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.113078: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.114558: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.114997: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.256202: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.256686: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.258411: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.258853: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.347837: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.348388: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.350187: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.350706: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.442617: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.443071: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.444891: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.445339: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.563844: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.564299: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.566041: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.566473: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.700302: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.700749: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.702526: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:52.702965: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.304638: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.305108: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.399473: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.400154: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.436448: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.437009: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.438914: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.439459: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.509477: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.509916: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.511718: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:53.512153: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.167442: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.168198: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.259835: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.260315: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.294379: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.294877: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.296616: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.297133: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.361620: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.362075: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.363656: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.364092: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.461440: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.461920: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.514250: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.514794: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.609334: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.609789: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.690207: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.690661: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.692483: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.692990: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.695297: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.695738: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.697531: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.697970: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.833616: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.834159: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.836147: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:55.836658: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.476496: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.530142: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.531737: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.532301: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.647491: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.647941: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.649559: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.650001: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.766269: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.766783: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.768503: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:56.769032: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:57.836824: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:57.837356: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:57.989631: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:57.990300: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:57.992159: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:57.992832: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.141327: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.141794: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.198557: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.199217: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.320298: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.320983: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.322902: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.323619: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.447981: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.448753: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.450698: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.451442: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.945522: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:58.945982: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.038988: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.039453: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.074456: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.074955: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.076731: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.077214: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.144806: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.145265: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.146918: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.147367: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.585547: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.586029: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.680288: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.680739: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.714549: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.715126: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.716924: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.717485: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.782401: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.782855: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.784542: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:07:59.784981: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:03.681028: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:03.681549: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:03.830110: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:03.830800: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:03.832773: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:03.833416: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:03.874570: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:03.875052: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.010197: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.010655: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.012251: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.012688: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.170243: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.170923: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.266925: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.267395: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.352354: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.352818: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.425522: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.426083: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.427840: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:04.428330: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:08:09.818297 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 3.68348
I0901 08:08:09.818971 140646288807744 basic_session_run_hooks.py:260] loss = 7.5312343, step = 100 (27.148 sec)
2019-09-01 08:08:12.021036: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.021496: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.074026: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.074488: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.110640: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.111323: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.113373: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.114038: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.185044: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.185486: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.187248: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:12.187681: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:20.816985: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:20.817450: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:20.915899: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:20.916384: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:20.952578: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:20.953150: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:20.955132: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:20.955678: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:21.028735: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:21.029186: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:21.031065: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:21.031496: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:08:25.822679 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.24829
I0901 08:08:25.823299 140646288807744 basic_session_run_hooks.py:260] loss = 7.216251, step = 200 (16.004 sec)
2019-09-01 08:08:26.166336: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:26.166796: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:26.313801: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:26.314256: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:26.316041: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:26.316486: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:26.926255: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:26.926820: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:27.017950: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:27.018620: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:27.053335: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:27.053787: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:27.055311: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:27.055746: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:27.120799: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:27.121261: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:27.122814: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:27.123248: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.322537: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.322998: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.414077: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.414606: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.448895: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.449415: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.451039: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.451541: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.515808: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.516253: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.517806: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:31.518245: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:08:41.160866 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.51968
I0901 08:08:41.161560 140646288807744 basic_session_run_hooks.py:260] loss = 6.926969, step = 300 (15.338 sec)
2019-09-01 08:08:44.698299: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:44.698789: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:44.796077: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:44.796542: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:44.832433: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:44.832883: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:44.907499: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:44.907960: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:44.909733: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:44.910168: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.008092: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.008571: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.103687: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.104160: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.140368: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.140892: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.142770: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.143278: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.214545: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.214992: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.216815: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:08:45.217253: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:08:56.071624 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.70657
I0901 08:08:56.072449 140646288807744 basic_session_run_hooks.py:260] loss = 6.872378, step = 400 (14.911 sec)
2019-09-01 08:09:01.112107: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.112574: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.208638: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.209088: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.245165: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.245645: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.247474: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.247939: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.320424: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.320875: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.322689: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:01.323121: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:09:10.860673 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.76176
I0901 08:09:10.861382 140646288807744 basic_session_run_hooks.py:260] loss = 6.5886087, step = 500 (14.789 sec)
2019-09-01 08:09:18.397932: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.398447: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.495241: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.495713: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.532327: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.532901: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.534870: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.535422: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.607850: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.608312: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.610188: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:18.610634: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:09:25.290929 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.92988
I0901 08:09:25.291679 140646288807744 basic_session_run_hooks.py:260] loss = 6.411321, step = 600 (14.430 sec)
2019-09-01 08:09:32.207385: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.207872: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.301727: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.302253: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.337676: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.338124: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.339868: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.340303: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.409270: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.409714: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.411418: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:32.411851: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:09:39.951052 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.82122
I0901 08:09:39.951722 140646288807744 basic_session_run_hooks.py:260] loss = 6.331458, step = 700 (14.660 sec)
2019-09-01 08:09:40.838778: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:40.839245: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:40.936400: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:40.936861: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:40.972582: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:40.973036: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:40.974861: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:40.975297: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:41.046437: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:41.046899: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:41.048657: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:41.049096: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.027803: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.028369: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.122239: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.122689: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.158617: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.159107: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.160770: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.161239: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.229860: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.230304: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.231985: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:09:53.232433: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:09:55.225482 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.54689
I0901 08:09:55.226159 140646288807744 basic_session_run_hooks.py:260] loss = 6.3738728, step = 800 (15.274 sec)
I0901 08:10:09.388330 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.06073
I0901 08:10:09.388932 140646288807744 basic_session_run_hooks.py:260] loss = 5.999334, step = 900 (14.163 sec)
I0901 08:10:23.960938 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 1000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:10:25.117997 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 08:10:25.119027 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 08:10:25.229324 140646288807744 estimator.py:1145] Calling model_fn.
I0901 08:10:25.229905 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 08:10:25.230199 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 08:10:25.230263 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 08:10:25.230341 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 08:10:25.230411 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 08:10:25.230454 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 08:10:25.230490 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 08:10:25.283574 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 08:10:25.331568 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 08:10:25.415663 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 08:10:25.423430 140646288807744 t2t_model.py:2248] Building model body
I0901 08:10:28.592788 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
W0901 08:10:28.685163 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/metrics.py:637: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.

W0901 08:10:28.904705 140646288807744 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/bleu_hook.py:151: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0901 08:10:29.010757 140646288807744 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py:1674: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

I0901 08:10:29.012034 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 08:10:29.023200 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T08:10:29Z
I0901 08:10:29.849779 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 08:10:29.850217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:10:29.850474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 08:10:29.850528: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 08:10:29.850555: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 08:10:29.850583: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 08:10:29.850594: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 08:10:29.850606: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 08:10:29.850635: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 08:10:29.850665: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 08:10:29.850719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:10:29.850986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:10:29.851244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 08:10:29.851294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 08:10:29.851307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 08:10:29.851313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 08:10:29.851394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:10:29.851693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:10:29.851928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
W0901 08:10:29.852277 140646288807744 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
I0901 08:10:29.852993 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-1000
I0901 08:10:30.467599 140646288807744 session_manager.py:500] Running local_init_op.
I0901 08:10:30.548311 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 08:10:33.564228 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 08:10:40.450709 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 08:10:41.644563 140646288807744 evaluation.py:167] Evaluation [30/100]
2019-09-01 08:10:42.871829: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:42.872386: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:42.964535: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:42.965013: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:10:43.045542 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 08:10:44.919541 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 08:10:46.373547 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 08:10:47.671566 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 08:10:49.025171 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 08:10:50.213254 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 08:10:51.442362 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 08:10:51.557443 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-08:10:51
I0901 08:10:51.557624 140646288807744 estimator.py:2039] Saving dict for global step 1000: global_step = 1000, loss = 7.125624, metrics-translate_ende_wmt8k/targets/accuracy = 0.08613202, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.17673208, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.0056664557, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -7.125925, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.023583746, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.13287461
I0901 08:10:51.558019 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 1000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-1000
I0901 08:10:51.700107 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.36341
I0901 08:10:51.700741 140646288807744 basic_session_run_hooks.py:260] loss = 6.1306624, step = 1000 (42.312 sec)
2019-09-01 08:10:58.654062: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.654577: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.749895: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.750346: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.786173: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.786626: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.788422: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.788859: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.858924: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.859366: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.861078: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:10:58.861514: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:11:05.834631 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.07488
I0901 08:11:05.835406 140646288807744 basic_session_run_hooks.py:260] loss = 5.782243, step = 1100 (14.135 sec)
I0901 08:11:20.199226 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.96156
I0901 08:11:20.199925 140646288807744 basic_session_run_hooks.py:260] loss = 6.1792192, step = 1200 (14.365 sec)
I0901 08:11:33.822383 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34045
I0901 08:11:33.823184 140646288807744 basic_session_run_hooks.py:260] loss = 6.3443737, step = 1300 (13.623 sec)
I0901 08:11:47.661282 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22601
I0901 08:11:47.661886 140646288807744 basic_session_run_hooks.py:260] loss = 6.056165, step = 1400 (13.839 sec)
I0901 08:12:01.713347 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.11639
I0901 08:12:01.713946 140646288807744 basic_session_run_hooks.py:260] loss = 5.7719994, step = 1500 (14.052 sec)
I0901 08:12:15.877428 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.06011
I0901 08:12:15.878075 140646288807744 basic_session_run_hooks.py:260] loss = 5.7800536, step = 1600 (14.164 sec)
I0901 08:12:30.151689 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.00562
I0901 08:12:30.152378 140646288807744 basic_session_run_hooks.py:260] loss = 5.7369304, step = 1700 (14.274 sec)
I0901 08:12:44.146107 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.14571
I0901 08:12:44.146937 140646288807744 basic_session_run_hooks.py:260] loss = 5.935699, step = 1800 (13.995 sec)
2019-09-01 08:12:53.025979: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.026697: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.122638: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.123092: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.160370: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.160955: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.162871: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.163436: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.235166: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.235702: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.237523: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:12:53.237970: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:12:58.360790 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.03498
I0901 08:12:58.361643 140646288807744 basic_session_run_hooks.py:260] loss = 5.994168, step = 1900 (14.215 sec)
I0901 08:13:13.044445 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 2000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:13:14.208394 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:13:14.340822 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.25781
I0901 08:13:14.341650 140646288807744 basic_session_run_hooks.py:260] loss = 5.644465, step = 2000 (15.980 sec)
I0901 08:13:28.273276 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.17749
I0901 08:13:28.274018 140646288807744 basic_session_run_hooks.py:260] loss = 5.752209, step = 2100 (13.932 sec)
I0901 08:13:41.911373 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3324
I0901 08:13:41.912075 140646288807744 basic_session_run_hooks.py:260] loss = 5.414302, step = 2200 (13.638 sec)
I0901 08:13:55.709401 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24741
I0901 08:13:55.710006 140646288807744 basic_session_run_hooks.py:260] loss = 5.318773, step = 2300 (13.798 sec)
I0901 08:14:09.676745 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.15956
I0901 08:14:09.677394 140646288807744 basic_session_run_hooks.py:260] loss = 5.5569806, step = 2400 (13.967 sec)
I0901 08:14:23.580190 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.19246
I0901 08:14:23.580924 140646288807744 basic_session_run_hooks.py:260] loss = 5.256733, step = 2500 (13.904 sec)
I0901 08:14:37.580468 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.14272
I0901 08:14:37.581176 140646288807744 basic_session_run_hooks.py:260] loss = 5.1446724, step = 2600 (14.000 sec)
I0901 08:14:51.831552 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.01701
I0901 08:14:51.832401 140646288807744 basic_session_run_hooks.py:260] loss = 5.330634, step = 2700 (14.251 sec)
I0901 08:15:05.863804 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.12644
I0901 08:15:05.864621 140646288807744 basic_session_run_hooks.py:260] loss = 5.2353244, step = 2800 (14.032 sec)
I0901 08:15:19.677718 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23908
I0901 08:15:19.678406 140646288807744 basic_session_run_hooks.py:260] loss = 5.409894, step = 2900 (13.814 sec)
I0901 08:15:33.086632 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 3000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:15:34.263401 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:15:34.397426 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.79361
I0901 08:15:34.398245 140646288807744 basic_session_run_hooks.py:260] loss = 5.220609, step = 3000 (14.720 sec)
I0901 08:15:48.006954 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34779
I0901 08:15:48.007806 140646288807744 basic_session_run_hooks.py:260] loss = 5.069082, step = 3100 (13.610 sec)
I0901 08:16:02.013205 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.13967
I0901 08:16:02.013834 140646288807744 basic_session_run_hooks.py:260] loss = 5.1560297, step = 3200 (14.006 sec)
I0901 08:16:15.949246 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.17564
I0901 08:16:15.949970 140646288807744 basic_session_run_hooks.py:260] loss = 4.7954826, step = 3300 (13.936 sec)
I0901 08:16:29.583672 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33437
I0901 08:16:29.584540 140646288807744 basic_session_run_hooks.py:260] loss = 5.1055975, step = 3400 (13.635 sec)
I0901 08:16:43.743273 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.06235
I0901 08:16:43.743978 140646288807744 basic_session_run_hooks.py:260] loss = 4.7677546, step = 3500 (14.159 sec)
I0901 08:16:57.630802 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.2007
I0901 08:16:57.631493 140646288807744 basic_session_run_hooks.py:260] loss = 5.0099764, step = 3600 (13.888 sec)
I0901 08:17:11.773843 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.07061
I0901 08:17:11.774529 140646288807744 basic_session_run_hooks.py:260] loss = 4.658429, step = 3700 (14.143 sec)
I0901 08:17:25.684029 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.18898
I0901 08:17:25.684966 140646288807744 basic_session_run_hooks.py:260] loss = 4.6378813, step = 3800 (13.910 sec)
I0901 08:17:39.225663 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38463
I0901 08:17:39.226613 140646288807744 basic_session_run_hooks.py:260] loss = 5.2858505, step = 3900 (13.542 sec)
I0901 08:17:52.781206 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 4000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:17:53.942898 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:17:54.088113 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.72837
I0901 08:17:54.088973 140646288807744 basic_session_run_hooks.py:260] loss = 4.824595, step = 4000 (14.862 sec)
2019-09-01 08:18:06.524761: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:18:06.525258: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:18:06.527060: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:18:06.527592: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:18:06.597721: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:18:06.598169: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:18:06.599912: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:18:06.600355: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:18:07.769541 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30918
I0901 08:18:07.770298 140646288807744 basic_session_run_hooks.py:260] loss = 4.753538, step = 4100 (13.681 sec)
I0901 08:18:21.316687 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38163
I0901 08:18:21.317385 140646288807744 basic_session_run_hooks.py:260] loss = 4.6943426, step = 4200 (13.547 sec)
I0901 08:18:35.186266 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.21002
I0901 08:18:35.187253 140646288807744 basic_session_run_hooks.py:260] loss = 4.4532714, step = 4300 (13.870 sec)
I0901 08:18:48.941915 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26974
I0901 08:18:48.942675 140646288807744 basic_session_run_hooks.py:260] loss = 4.9068522, step = 4400 (13.755 sec)
I0901 08:19:02.909223 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.15958
I0901 08:19:02.909883 140646288807744 basic_session_run_hooks.py:260] loss = 4.6415277, step = 4500 (13.967 sec)
I0901 08:19:16.489423 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36366
I0901 08:19:16.490260 140646288807744 basic_session_run_hooks.py:260] loss = 4.5063143, step = 4600 (13.580 sec)
2019-09-01 08:19:29.185108: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:19:29.185628: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:19:30.334365 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22286
I0901 08:19:30.335118 140646288807744 basic_session_run_hooks.py:260] loss = 4.2072554, step = 4700 (13.845 sec)
I0901 08:19:44.384025 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.11761
I0901 08:19:44.384828 140646288807744 basic_session_run_hooks.py:260] loss = 4.1447935, step = 4800 (14.050 sec)
I0901 08:19:58.187956 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24431
I0901 08:19:58.188795 140646288807744 basic_session_run_hooks.py:260] loss = 4.2695045, step = 4900 (13.804 sec)
I0901 08:20:11.648174 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 5000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:20:12.813475 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:20:12.953385 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.77258
I0901 08:20:12.954043 140646288807744 basic_session_run_hooks.py:260] loss = 4.306001, step = 5000 (14.765 sec)
I0901 08:20:26.705075 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27183
I0901 08:20:26.705760 140646288807744 basic_session_run_hooks.py:260] loss = 4.5180182, step = 5100 (13.752 sec)
I0901 08:20:40.909251 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.04018
I0901 08:20:40.909980 140646288807744 basic_session_run_hooks.py:260] loss = 3.710175, step = 5200 (14.204 sec)
I0901 08:20:54.502563 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35656
I0901 08:20:54.503357 140646288807744 basic_session_run_hooks.py:260] loss = 4.3543816, step = 5300 (13.593 sec)
2019-09-01 08:20:59.636438: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.636898: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.735479: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.735930: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.772563: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.773048: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.774936: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.775390: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.850172: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.850619: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.852567: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:20:59.853010: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:21:08.266687 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26526
I0901 08:21:08.267672 140646288807744 basic_session_run_hooks.py:260] loss = 4.334164, step = 5400 (13.764 sec)
I0901 08:21:22.439131 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.05595
I0901 08:21:22.440016 140646288807744 basic_session_run_hooks.py:260] loss = 4.3013754, step = 5500 (14.172 sec)
I0901 08:21:36.286449 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22162
I0901 08:21:36.287227 140646288807744 basic_session_run_hooks.py:260] loss = 3.9897394, step = 5600 (13.847 sec)
I0901 08:21:50.141785 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.21743
I0901 08:21:50.142681 140646288807744 basic_session_run_hooks.py:260] loss = 4.219001, step = 5700 (13.855 sec)
I0901 08:22:03.872506 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28294
I0901 08:22:03.873136 140646288807744 basic_session_run_hooks.py:260] loss = 4.164128, step = 5800 (13.730 sec)
I0901 08:22:17.671529 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24689
I0901 08:22:17.672204 140646288807744 basic_session_run_hooks.py:260] loss = 4.3202686, step = 5900 (13.799 sec)
I0901 08:22:31.335731 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 6000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:22:32.498714 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 08:22:32.499893 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 08:22:32.610838 140646288807744 estimator.py:1145] Calling model_fn.
I0901 08:22:32.611439 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 08:22:32.611740 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 08:22:32.611806 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 08:22:32.611881 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 08:22:32.611949 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 08:22:32.611997 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 08:22:32.612040 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 08:22:32.665510 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 08:22:32.713287 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 08:22:32.795802 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 08:22:32.803321 140646288807744 t2t_model.py:2248] Building model body
I0901 08:22:35.918081 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 08:22:36.335179 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 08:22:36.346472 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T08:22:36Z
I0901 08:22:36.699611 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 08:22:36.700135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:22:36.700445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 08:22:36.700514: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 08:22:36.700527: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 08:22:36.700539: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 08:22:36.700550: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 08:22:36.700562: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 08:22:36.700587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 08:22:36.700613: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 08:22:36.700657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:22:36.700877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:22:36.701063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 08:22:36.701087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 08:22:36.701094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 08:22:36.701107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 08:22:36.701161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:22:36.701404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:22:36.701613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 08:22:36.702294 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-6000
I0901 08:22:37.307085 140646288807744 session_manager.py:500] Running local_init_op.
I0901 08:22:37.382334 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 08:22:40.249790 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 08:22:41.608070 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 08:22:42.973238 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 08:22:44.203866 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 08:22:45.454361 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 08:22:46.845101 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 08:22:48.101887 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 08:22:49.373535 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 08:22:50.648469 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 08:22:51.879631 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 08:22:52.032589 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-08:22:52
I0901 08:22:52.032771 140646288807744 estimator.py:2039] Saving dict for global step 6000: global_step = 6000, loss = 4.7742567, metrics-translate_ende_wmt8k/targets/accuracy = 0.25835058, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.4367198, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.030504882, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -4.774927, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.07967395, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.27879998
I0901 08:22:52.033167 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 6000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-6000
I0901 08:22:52.167719 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.89887
I0901 08:22:52.168466 140646288807744 basic_session_run_hooks.py:260] loss = 4.168566, step = 6000 (34.496 sec)
I0901 08:23:05.979027 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24045
I0901 08:23:05.979768 140646288807744 basic_session_run_hooks.py:260] loss = 4.1061635, step = 6100 (13.811 sec)
I0901 08:23:19.640593 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3198
I0901 08:23:19.641231 140646288807744 basic_session_run_hooks.py:260] loss = 3.9754136, step = 6200 (13.661 sec)
I0901 08:23:33.198097 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37599
I0901 08:23:33.198822 140646288807744 basic_session_run_hooks.py:260] loss = 3.9572446, step = 6300 (13.558 sec)
I0901 08:23:46.915057 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29025
I0901 08:23:46.915746 140646288807744 basic_session_run_hooks.py:260] loss = 3.6621735, step = 6400 (13.717 sec)
I0901 08:24:00.986172 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.10676
I0901 08:24:00.987025 140646288807744 basic_session_run_hooks.py:260] loss = 3.6912065, step = 6500 (14.071 sec)
2019-09-01 08:24:10.710989: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.711484: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.805489: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.805943: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.839682: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.840136: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.841766: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.842202: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.908257: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.908712: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.910448: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:24:10.910883: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:24:14.868330 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.20349
I0901 08:24:14.869001 140646288807744 basic_session_run_hooks.py:260] loss = 3.424152, step = 6600 (13.882 sec)
I0901 08:24:28.732722 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.21272
I0901 08:24:28.733573 140646288807744 basic_session_run_hooks.py:260] loss = 3.5765157, step = 6700 (13.865 sec)
I0901 08:24:42.427979 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3018
I0901 08:24:42.428653 140646288807744 basic_session_run_hooks.py:260] loss = 3.2278595, step = 6800 (13.695 sec)
I0901 08:24:56.204803 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25857
I0901 08:24:56.205508 140646288807744 basic_session_run_hooks.py:260] loss = 3.7922678, step = 6900 (13.777 sec)
I0901 08:25:09.803899 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 7000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:25:10.966647 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:25:11.094485 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.71606
I0901 08:25:11.095270 140646288807744 basic_session_run_hooks.py:260] loss = 3.5398505, step = 7000 (14.890 sec)
I0901 08:25:24.977029 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.20329
I0901 08:25:24.977736 140646288807744 basic_session_run_hooks.py:260] loss = 3.9092386, step = 7100 (13.882 sec)
I0901 08:25:38.515248 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3865
I0901 08:25:38.515872 140646288807744 basic_session_run_hooks.py:260] loss = 3.9357862, step = 7200 (13.538 sec)
I0901 08:25:52.323039 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24229
I0901 08:25:52.323725 140646288807744 basic_session_run_hooks.py:260] loss = 3.9090502, step = 7300 (13.808 sec)
I0901 08:26:05.961194 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33237
I0901 08:26:05.961949 140646288807744 basic_session_run_hooks.py:260] loss = 3.1670113, step = 7400 (13.638 sec)
I0901 08:26:19.583203 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34106
I0901 08:26:19.583938 140646288807744 basic_session_run_hooks.py:260] loss = 4.0021105, step = 7500 (13.622 sec)
I0901 08:26:33.179517 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35493
I0901 08:26:33.180311 140646288807744 basic_session_run_hooks.py:260] loss = 4.069044, step = 7600 (13.596 sec)
I0901 08:26:46.807514 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33784
I0901 08:26:46.808156 140646288807744 basic_session_run_hooks.py:260] loss = 3.303373, step = 7700 (13.628 sec)
I0901 08:27:00.353280 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38238
I0901 08:27:00.354159 140646288807744 basic_session_run_hooks.py:260] loss = 3.5980077, step = 7800 (13.546 sec)
I0901 08:27:14.108013 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27023
I0901 08:27:14.108851 140646288807744 basic_session_run_hooks.py:260] loss = 3.0175133, step = 7900 (13.755 sec)
I0901 08:27:27.816235 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 8000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:27:29.009917 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:27:29.150229 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.64795
I0901 08:27:29.150921 140646288807744 basic_session_run_hooks.py:260] loss = 3.2400136, step = 8000 (15.042 sec)
I0901 08:27:42.801683 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32523
I0901 08:27:42.802407 140646288807744 basic_session_run_hooks.py:260] loss = 3.6801853, step = 8100 (13.651 sec)
I0901 08:27:56.486789 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30721
I0901 08:27:56.487573 140646288807744 basic_session_run_hooks.py:260] loss = 3.6237729, step = 8200 (13.685 sec)
I0901 08:28:10.136577 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32612
I0901 08:28:10.137363 140646288807744 basic_session_run_hooks.py:260] loss = 3.8644948, step = 8300 (13.650 sec)
I0901 08:28:23.727799 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35769
I0901 08:28:23.728585 140646288807744 basic_session_run_hooks.py:260] loss = 3.1329985, step = 8400 (13.591 sec)
2019-09-01 08:28:36.185371: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.185842: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.277195: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.277713: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.312299: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.312746: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.314263: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.314704: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.378862: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.379304: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.380863: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:28:36.381296: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:28:37.642535 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.18663
I0901 08:28:37.643451 140646288807744 basic_session_run_hooks.py:260] loss = 3.4158797, step = 8500 (13.915 sec)
I0901 08:28:51.246161 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35098
I0901 08:28:51.247025 140646288807744 basic_session_run_hooks.py:260] loss = 3.4020758, step = 8600 (13.604 sec)
I0901 08:29:04.884309 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33237
I0901 08:29:04.885156 140646288807744 basic_session_run_hooks.py:260] loss = 3.2820332, step = 8700 (13.638 sec)
I0901 08:29:18.412638 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3919
I0901 08:29:18.413285 140646288807744 basic_session_run_hooks.py:260] loss = 3.5301855, step = 8800 (13.528 sec)
I0901 08:29:32.758328 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.97073
I0901 08:29:32.759169 140646288807744 basic_session_run_hooks.py:260] loss = 3.5364516, step = 8900 (14.346 sec)
I0901 08:29:46.525351 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 9000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:29:47.686555 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:29:47.815766 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.64123
I0901 08:29:47.816486 140646288807744 basic_session_run_hooks.py:260] loss = 3.699634, step = 9000 (15.057 sec)
I0901 08:30:01.657767 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22439
I0901 08:30:01.659047 140646288807744 basic_session_run_hooks.py:260] loss = 4.171739, step = 9100 (13.843 sec)
I0901 08:30:15.315389 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32192
I0901 08:30:15.316086 140646288807744 basic_session_run_hooks.py:260] loss = 3.2883859, step = 9200 (13.657 sec)
I0901 08:30:29.313723 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.14371
I0901 08:30:29.314643 140646288807744 basic_session_run_hooks.py:260] loss = 3.036046, step = 9300 (13.999 sec)
I0901 08:30:43.103621 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25168
I0901 08:30:43.104334 140646288807744 basic_session_run_hooks.py:260] loss = 3.193429, step = 9400 (13.790 sec)
I0901 08:30:56.917623 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23905
I0901 08:30:56.918698 140646288807744 basic_session_run_hooks.py:260] loss = 4.067578, step = 9500 (13.814 sec)
2019-09-01 08:31:04.152852: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.153569: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.244857: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.245548: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.280242: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.280698: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.282229: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.282673: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.347599: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.348045: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.349607: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:31:04.350039: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:31:10.838438 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.18347
I0901 08:31:10.839137 140646288807744 basic_session_run_hooks.py:260] loss = 2.8977442, step = 9600 (13.920 sec)
I0901 08:31:24.565344 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28496
I0901 08:31:24.566167 140646288807744 basic_session_run_hooks.py:260] loss = 3.4190793, step = 9700 (13.727 sec)
I0901 08:31:38.458845 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.19761
I0901 08:31:38.459537 140646288807744 basic_session_run_hooks.py:260] loss = 3.265183, step = 9800 (13.893 sec)
I0901 08:31:52.163258 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29692
I0901 08:31:52.164109 140646288807744 basic_session_run_hooks.py:260] loss = 4.0089808, step = 9900 (13.705 sec)
I0901 08:32:05.769418 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 10000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:32:06.921266 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:32:07.066126 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.71012
I0901 08:32:07.066761 140646288807744 basic_session_run_hooks.py:260] loss = 3.1136646, step = 10000 (14.903 sec)
2019-09-01 08:32:09.241278: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:32:09.241739: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:32:09.332374: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:32:09.332844: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:32:09.333930: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:32:09.334381: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:32:20.787971 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28766
I0901 08:32:20.788815 140646288807744 basic_session_run_hooks.py:260] loss = 2.9980295, step = 10100 (13.722 sec)
I0901 08:32:34.333939 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38227
I0901 08:32:34.334609 140646288807744 basic_session_run_hooks.py:260] loss = 3.237672, step = 10200 (13.546 sec)
I0901 08:32:47.920549 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36019
I0901 08:32:47.921346 140646288807744 basic_session_run_hooks.py:260] loss = 3.2162864, step = 10300 (13.587 sec)
I0901 08:33:01.488207 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37047
I0901 08:33:01.488923 140646288807744 basic_session_run_hooks.py:260] loss = 4.0117106, step = 10400 (13.568 sec)
I0901 08:33:15.245676 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26878
I0901 08:33:15.246374 140646288807744 basic_session_run_hooks.py:260] loss = 2.5613272, step = 10500 (13.757 sec)
I0901 08:33:29.023926 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25782
I0901 08:33:29.024685 140646288807744 basic_session_run_hooks.py:260] loss = 3.4413064, step = 10600 (13.778 sec)
I0901 08:33:42.682777 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32126
I0901 08:33:42.683602 140646288807744 basic_session_run_hooks.py:260] loss = 3.0617955, step = 10700 (13.659 sec)
I0901 08:33:56.275391 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35693
I0901 08:33:56.276072 140646288807744 basic_session_run_hooks.py:260] loss = 2.7925422, step = 10800 (13.592 sec)
I0901 08:34:10.157743 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.20339
I0901 08:34:10.158570 140646288807744 basic_session_run_hooks.py:260] loss = 2.8329756, step = 10900 (13.882 sec)
I0901 08:34:23.790902 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 11000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:34:24.976702 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 08:34:24.977850 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 08:34:25.092796 140646288807744 estimator.py:1145] Calling model_fn.
I0901 08:34:25.093368 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 08:34:25.093646 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 08:34:25.093699 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 08:34:25.093746 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 08:34:25.093785 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 08:34:25.093846 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 08:34:25.093884 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 08:34:25.148749 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 08:34:25.197841 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 08:34:25.283455 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 08:34:25.291251 140646288807744 t2t_model.py:2248] Building model body
I0901 08:34:28.672119 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 08:34:29.103310 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 08:34:29.115316 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T08:34:29Z
I0901 08:34:29.478382 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 08:34:29.478840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:34:29.479106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 08:34:29.479162: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 08:34:29.479211: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 08:34:29.479223: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 08:34:29.479235: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 08:34:29.479259: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 08:34:29.479286: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 08:34:29.479299: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 08:34:29.479375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:34:29.479629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:34:29.479874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 08:34:29.479912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 08:34:29.479920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 08:34:29.479926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 08:34:29.480024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:34:29.480311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:34:29.480540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 08:34:29.481257 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-11000
I0901 08:34:30.092778 140646288807744 session_manager.py:500] Running local_init_op.
I0901 08:34:30.169911 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 08:34:32.987355 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 08:34:34.141625 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 08:34:35.419330 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 08:34:36.654364 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 08:34:37.909045 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 08:34:39.230949 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 08:34:40.475719 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 08:34:41.602167 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 08:34:42.948279 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 08:34:44.196239 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 08:34:44.342363 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-08:34:44
I0901 08:34:44.342535 140646288807744 estimator.py:2039] Saving dict for global step 11000: global_step = 11000, loss = 3.4996488, metrics-translate_ende_wmt8k/targets/accuracy = 0.42116693, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.6132359, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.14344718, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -3.4992115, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.20378037, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.41635993
I0901 08:34:44.342943 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 11000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-11000
I0901 08:34:44.469620 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.91444
I0901 08:34:44.470418 140646288807744 basic_session_run_hooks.py:260] loss = 2.938969, step = 11000 (34.312 sec)
I0901 08:34:58.097508 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33789
I0901 08:34:58.098282 140646288807744 basic_session_run_hooks.py:260] loss = 3.2175956, step = 11100 (13.628 sec)
I0901 08:35:11.679663 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3626
I0901 08:35:11.680302 140646288807744 basic_session_run_hooks.py:260] loss = 2.6169806, step = 11200 (13.582 sec)
2019-09-01 08:35:20.770638: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:35:20.771127: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:35:20.847263: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:35:20.847776: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:35:20.881810: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:35:20.882378: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:35:20.950093: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:35:20.950542: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:35:20.952049: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:35:20.952485: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:35:25.410012 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28314
I0901 08:35:25.410881 140646288807744 basic_session_run_hooks.py:260] loss = 3.1432042, step = 11300 (13.731 sec)
I0901 08:35:39.112898 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29773
I0901 08:35:39.113587 140646288807744 basic_session_run_hooks.py:260] loss = 2.911541, step = 11400 (13.703 sec)
2019-09-01 08:35:52.603452: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:35:52.604128: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:35:52.962371 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22049
I0901 08:35:52.963106 140646288807744 basic_session_run_hooks.py:260] loss = 3.0817065, step = 11500 (13.850 sec)
I0901 08:36:06.744330 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25587
I0901 08:36:06.745022 140646288807744 basic_session_run_hooks.py:260] loss = 2.708549, step = 11600 (13.782 sec)
I0901 08:36:20.334295 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35837
I0901 08:36:20.334984 140646288807744 basic_session_run_hooks.py:260] loss = 3.0215192, step = 11700 (13.590 sec)
I0901 08:36:33.833992 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40757
I0901 08:36:33.834645 140646288807744 basic_session_run_hooks.py:260] loss = 2.800022, step = 11800 (13.500 sec)
I0901 08:36:47.375039 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38495
I0901 08:36:47.375989 140646288807744 basic_session_run_hooks.py:260] loss = 2.933078, step = 11900 (13.541 sec)
I0901 08:37:01.240260 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 12000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:37:02.397587 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:37:02.541782 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.59337
I0901 08:37:02.542667 140646288807744 basic_session_run_hooks.py:260] loss = 3.313217, step = 12000 (15.167 sec)
I0901 08:37:16.091484 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38024
I0901 08:37:16.092246 140646288807744 basic_session_run_hooks.py:260] loss = 3.2415369, step = 12100 (13.550 sec)
I0901 08:37:29.793136 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29839
I0901 08:37:29.793851 140646288807744 basic_session_run_hooks.py:260] loss = 2.5688083, step = 12200 (13.702 sec)
I0901 08:37:43.489956 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30097
I0901 08:37:43.490790 140646288807744 basic_session_run_hooks.py:260] loss = 2.9475803, step = 12300 (13.697 sec)
I0901 08:37:57.004864 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39924
I0901 08:37:57.005835 140646288807744 basic_session_run_hooks.py:260] loss = 2.571438, step = 12400 (13.515 sec)
I0901 08:38:10.753893 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27324
I0901 08:38:10.754647 140646288807744 basic_session_run_hooks.py:260] loss = 3.5938516, step = 12500 (13.749 sec)
I0901 08:38:24.679982 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.18077
I0901 08:38:24.680688 140646288807744 basic_session_run_hooks.py:260] loss = 3.5793405, step = 12600 (13.926 sec)
I0901 08:38:38.205993 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39316
I0901 08:38:38.206661 140646288807744 basic_session_run_hooks.py:260] loss = 3.0009458, step = 12700 (13.526 sec)
I0901 08:38:52.416493 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.03706
I0901 08:38:52.418345 140646288807744 basic_session_run_hooks.py:260] loss = 4.293484, step = 12800 (14.212 sec)
I0901 08:39:06.318923 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.19297
I0901 08:39:06.319611 140646288807744 basic_session_run_hooks.py:260] loss = 2.7127116, step = 12900 (13.901 sec)
I0901 08:39:19.579682 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 13000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:39:20.752230 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:39:20.887839 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.86393
I0901 08:39:20.888610 140646288807744 basic_session_run_hooks.py:260] loss = 3.0293875, step = 13000 (14.569 sec)
I0901 08:39:34.415551 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39223
I0901 08:39:34.416331 140646288807744 basic_session_run_hooks.py:260] loss = 3.3610692, step = 13100 (13.528 sec)
I0901 08:39:47.977490 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37358
I0901 08:39:47.978256 140646288807744 basic_session_run_hooks.py:260] loss = 3.5644455, step = 13200 (13.562 sec)
I0901 08:40:01.421703 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43815
I0901 08:40:01.422572 140646288807744 basic_session_run_hooks.py:260] loss = 3.2131696, step = 13300 (13.444 sec)
I0901 08:40:14.960698 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38606
I0901 08:40:14.961454 140646288807744 basic_session_run_hooks.py:260] loss = 2.8793514, step = 13400 (13.539 sec)
I0901 08:40:28.716250 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26979
I0901 08:40:28.717084 140646288807744 basic_session_run_hooks.py:260] loss = 3.5302222, step = 13500 (13.756 sec)
I0901 08:40:42.276569 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37446
I0901 08:40:42.277472 140646288807744 basic_session_run_hooks.py:260] loss = 2.8428402, step = 13600 (13.560 sec)
I0901 08:40:55.949749 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31359
I0901 08:40:55.950563 140646288807744 basic_session_run_hooks.py:260] loss = 3.6703243, step = 13700 (13.673 sec)
I0901 08:41:09.541150 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35759
I0901 08:41:09.541821 140646288807744 basic_session_run_hooks.py:260] loss = 3.3677874, step = 13800 (13.591 sec)
I0901 08:41:23.347904 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24283
I0901 08:41:23.348674 140646288807744 basic_session_run_hooks.py:260] loss = 2.9825437, step = 13900 (13.807 sec)
I0901 08:41:36.747214 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 14000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:41:37.901957 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:41:38.044849 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.80414
I0901 08:41:38.045638 140646288807744 basic_session_run_hooks.py:260] loss = 2.7743354, step = 14000 (14.697 sec)
2019-09-01 08:41:45.191563: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.192091: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.282635: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.283119: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.318305: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.318871: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.320549: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.321100: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.385512: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.385956: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.387483: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:41:45.387917: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:41:51.655049 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34743
I0901 08:41:51.655838 140646288807744 basic_session_run_hooks.py:260] loss = 2.7199862, step = 14100 (13.610 sec)
I0901 08:42:05.293167 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33239
I0901 08:42:05.293865 140646288807744 basic_session_run_hooks.py:260] loss = 3.1542828, step = 14200 (13.638 sec)
I0901 08:42:19.056043 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26593
I0901 08:42:19.056768 140646288807744 basic_session_run_hooks.py:260] loss = 2.9855063, step = 14300 (13.763 sec)
I0901 08:42:32.581854 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39327
I0901 08:42:32.582521 140646288807744 basic_session_run_hooks.py:260] loss = 2.818051, step = 14400 (13.526 sec)
I0901 08:42:46.356544 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25969
I0901 08:42:46.357347 140646288807744 basic_session_run_hooks.py:260] loss = 3.7314973, step = 14500 (13.775 sec)
I0901 08:42:59.887929 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39022
I0901 08:42:59.888593 140646288807744 basic_session_run_hooks.py:260] loss = 2.3496547, step = 14600 (13.531 sec)
I0901 08:43:13.492247 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35061
I0901 08:43:13.493103 140646288807744 basic_session_run_hooks.py:260] loss = 2.8037527, step = 14700 (13.605 sec)
I0901 08:43:26.875134 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.47223
I0901 08:43:26.875792 140646288807744 basic_session_run_hooks.py:260] loss = 3.3088558, step = 14800 (13.383 sec)
I0901 08:43:40.767920 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.19798
I0901 08:43:40.768692 140646288807744 basic_session_run_hooks.py:260] loss = 3.101592, step = 14900 (13.893 sec)
I0901 08:43:54.448572 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 15000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:43:55.617663 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:43:55.764128 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.66835
I0901 08:43:55.764912 140646288807744 basic_session_run_hooks.py:260] loss = 3.0565574, step = 15000 (14.996 sec)
I0901 08:44:09.274250 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40186
I0901 08:44:09.274922 140646288807744 basic_session_run_hooks.py:260] loss = 2.6647217, step = 15100 (13.510 sec)
I0901 08:44:23.132353 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.216
I0901 08:44:23.133293 140646288807744 basic_session_run_hooks.py:260] loss = 2.7676647, step = 15200 (13.858 sec)
I0901 08:44:36.764681 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3355
I0901 08:44:36.765485 140646288807744 basic_session_run_hooks.py:260] loss = 3.3538725, step = 15300 (13.632 sec)
I0901 08:44:50.371878 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34905
I0901 08:44:50.372668 140646288807744 basic_session_run_hooks.py:260] loss = 2.6136427, step = 15400 (13.607 sec)
I0901 08:45:04.156982 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25421
I0901 08:45:04.157665 140646288807744 basic_session_run_hooks.py:260] loss = 2.7346334, step = 15500 (13.785 sec)
I0901 08:45:17.827279 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31513
I0901 08:45:17.828035 140646288807744 basic_session_run_hooks.py:260] loss = 2.5741923, step = 15600 (13.670 sec)
I0901 08:45:31.328939 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4065
I0901 08:45:31.329882 140646288807744 basic_session_run_hooks.py:260] loss = 2.829536, step = 15700 (13.502 sec)
I0901 08:45:44.986139 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32214
I0901 08:45:44.986789 140646288807744 basic_session_run_hooks.py:260] loss = 2.6796966, step = 15800 (13.657 sec)
I0901 08:45:58.556399 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36906
I0901 08:45:58.557110 140646288807744 basic_session_run_hooks.py:260] loss = 2.8220346, step = 15900 (13.570 sec)
I0901 08:46:12.170273 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 16000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:46:13.329740 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 08:46:13.330884 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 08:46:13.441848 140646288807744 estimator.py:1145] Calling model_fn.
I0901 08:46:13.442440 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 08:46:13.442676 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 08:46:13.442749 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 08:46:13.442801 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 08:46:13.442859 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 08:46:13.442955 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 08:46:13.443011 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 08:46:13.497612 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 08:46:13.546568 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 08:46:13.634184 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 08:46:13.643516 140646288807744 t2t_model.py:2248] Building model body
I0901 08:46:16.803359 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 08:46:17.216275 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 08:46:17.227250 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T08:46:17Z
I0901 08:46:17.582146 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 08:46:17.582591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:46:17.582852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 08:46:17.582905: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 08:46:17.582919: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 08:46:17.582931: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 08:46:17.582960: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 08:46:17.582987: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 08:46:17.583006: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 08:46:17.583017: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 08:46:17.583058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:46:17.583285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:46:17.583526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 08:46:17.583565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 08:46:17.583572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 08:46:17.583577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 08:46:17.583670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:46:17.583902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:46:17.584119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 08:46:17.584917 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-16000
I0901 08:46:18.206534 140646288807744 session_manager.py:500] Running local_init_op.
I0901 08:46:18.284855 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 08:46:21.255650 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 08:46:26.216550 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 08:46:27.647781 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 08:46:28.812047 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 08:46:29.956901 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 08:46:31.232136 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 08:46:32.444040 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 08:46:33.662801 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 08:46:34.974483 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 08:46:36.176620 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 08:46:36.328243 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-08:46:36
I0901 08:46:36.328418 140646288807744 estimator.py:2039] Saving dict for global step 16000: global_step = 16000, loss = 3.1508248, metrics-translate_ende_wmt8k/targets/accuracy = 0.46179995, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.6590077, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.17482989, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -3.1512113, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.23965588, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.45316887
I0901 08:46:36.328909 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 16000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-16000
I0901 08:46:36.467986 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.63772
I0901 08:46:36.468744 140646288807744 basic_session_run_hooks.py:260] loss = 2.1205757, step = 16000 (37.912 sec)
I0901 08:46:50.137440 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31558
I0901 08:46:50.138112 140646288807744 basic_session_run_hooks.py:260] loss = 3.0136907, step = 16100 (13.669 sec)
2019-09-01 08:46:51.401774: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.402238: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.496948: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.497404: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.534337: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.535180: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.537214: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.538022: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.608366: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.609025: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.610796: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:46:51.611455: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:47:04.075861 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.17441
I0901 08:47:04.076787 140646288807744 basic_session_run_hooks.py:260] loss = 3.1291041, step = 16200 (13.939 sec)
I0901 08:47:17.847610 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26124
I0901 08:47:17.848392 140646288807744 basic_session_run_hooks.py:260] loss = 2.9260268, step = 16300 (13.772 sec)
I0901 08:47:31.500514 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32445
I0901 08:47:31.501412 140646288807744 basic_session_run_hooks.py:260] loss = 3.2267098, step = 16400 (13.653 sec)
I0901 08:47:45.034856 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38861
I0901 08:47:45.035490 140646288807744 basic_session_run_hooks.py:260] loss = 2.7523472, step = 16500 (13.534 sec)
I0901 08:47:58.845997 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24053
I0901 08:47:58.846777 140646288807744 basic_session_run_hooks.py:260] loss = 3.2360666, step = 16600 (13.811 sec)
I0901 08:48:12.350438 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40497
I0901 08:48:12.351178 140646288807744 basic_session_run_hooks.py:260] loss = 1.7076572, step = 16700 (13.504 sec)
I0901 08:48:25.973626 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34043
I0901 08:48:25.974322 140646288807744 basic_session_run_hooks.py:260] loss = 3.0231845, step = 16800 (13.623 sec)
I0901 08:48:39.664165 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30432
I0901 08:48:39.664843 140646288807744 basic_session_run_hooks.py:260] loss = 2.7523057, step = 16900 (13.691 sec)
I0901 08:48:53.143628 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 17000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:48:54.314302 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:48:54.445663 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.76521
I0901 08:48:54.446357 140646288807744 basic_session_run_hooks.py:260] loss = 3.0274208, step = 17000 (14.782 sec)
I0901 08:49:08.027842 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36259
I0901 08:49:08.028770 140646288807744 basic_session_run_hooks.py:260] loss = 2.8638031, step = 17100 (13.582 sec)
I0901 08:49:21.439037 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.45645
I0901 08:49:21.439698 140646288807744 basic_session_run_hooks.py:260] loss = 2.8852072, step = 17200 (13.411 sec)
I0901 08:49:35.481529 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.12125
I0901 08:49:35.482279 140646288807744 basic_session_run_hooks.py:260] loss = 2.7506309, step = 17300 (14.043 sec)
I0901 08:49:49.161896 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30974
I0901 08:49:49.162637 140646288807744 basic_session_run_hooks.py:260] loss = 1.8314065, step = 17400 (13.680 sec)
2019-09-01 08:49:52.569816: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.570279: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.661206: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.661659: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.696708: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.697200: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.698765: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.699240: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.763866: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.764331: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.765939: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:49:52.766382: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:50:03.015935 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.21811
I0901 08:50:03.016655 140646288807744 basic_session_run_hooks.py:260] loss = 3.0617886, step = 17500 (13.854 sec)
I0901 08:50:16.783333 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26354
I0901 08:50:16.783966 140646288807744 basic_session_run_hooks.py:260] loss = 2.8073468, step = 17600 (13.767 sec)
I0901 08:50:30.565454 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25578
I0901 08:50:30.566161 140646288807744 basic_session_run_hooks.py:260] loss = 2.8643272, step = 17700 (13.782 sec)
I0901 08:50:44.295794 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28314
I0901 08:50:44.296655 140646288807744 basic_session_run_hooks.py:260] loss = 2.6212246, step = 17800 (13.730 sec)
I0901 08:50:58.141841 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22228
I0901 08:50:58.142471 140646288807744 basic_session_run_hooks.py:260] loss = 2.301012, step = 17900 (13.846 sec)
I0901 08:51:11.596498 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 18000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:51:12.802204 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:51:12.929333 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.76247
I0901 08:51:12.930167 140646288807744 basic_session_run_hooks.py:260] loss = 3.0331268, step = 18000 (14.788 sec)
I0901 08:51:26.457920 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39176
I0901 08:51:26.458591 140646288807744 basic_session_run_hooks.py:260] loss = 2.6693962, step = 18100 (13.528 sec)
I0901 08:51:40.000978 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38386
I0901 08:51:40.001837 140646288807744 basic_session_run_hooks.py:260] loss = 2.9276733, step = 18200 (13.543 sec)
I0901 08:51:53.528214 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39249
I0901 08:51:53.529018 140646288807744 basic_session_run_hooks.py:260] loss = 2.9928503, step = 18300 (13.527 sec)
I0901 08:52:07.084485 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37666
I0901 08:52:07.085193 140646288807744 basic_session_run_hooks.py:260] loss = 2.2583895, step = 18400 (13.556 sec)
I0901 08:52:20.917329 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22917
I0901 08:52:20.918167 140646288807744 basic_session_run_hooks.py:260] loss = 2.6273215, step = 18500 (13.833 sec)
I0901 08:52:34.683553 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26416
I0901 08:52:34.684354 140646288807744 basic_session_run_hooks.py:260] loss = 2.3248537, step = 18600 (13.766 sec)
I0901 08:52:48.318907 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33388
I0901 08:52:48.319633 140646288807744 basic_session_run_hooks.py:260] loss = 2.6215434, step = 18700 (13.635 sec)
I0901 08:53:01.866184 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38155
I0901 08:53:01.866971 140646288807744 basic_session_run_hooks.py:260] loss = 2.3540692, step = 18800 (13.547 sec)
I0901 08:53:15.359391 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41114
I0901 08:53:15.360016 140646288807744 basic_session_run_hooks.py:260] loss = 2.374964, step = 18900 (13.493 sec)
I0901 08:53:28.840217 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 19000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:53:29.989813 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:53:30.136866 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.76705
I0901 08:53:30.137601 140646288807744 basic_session_run_hooks.py:260] loss = 2.8049908, step = 19000 (14.778 sec)
I0901 08:53:43.799119 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31944
I0901 08:53:44.153971 140646288807744 basic_session_run_hooks.py:260] loss = 2.488716, step = 19100 (14.016 sec)
I0901 08:53:57.585929 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25331
I0901 08:53:57.586593 140646288807744 basic_session_run_hooks.py:260] loss = 2.7556071, step = 19200 (13.433 sec)
I0901 08:54:11.352951 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26373
I0901 08:54:11.353632 140646288807744 basic_session_run_hooks.py:260] loss = 3.165564, step = 19300 (13.767 sec)
I0901 08:54:25.069466 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29048
I0901 08:54:25.070086 140646288807744 basic_session_run_hooks.py:260] loss = 2.6356723, step = 19400 (13.716 sec)
I0901 08:54:38.904262 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22815
I0901 08:54:38.904865 140646288807744 basic_session_run_hooks.py:260] loss = 2.1828897, step = 19500 (13.835 sec)
I0901 08:54:52.704144 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24644
I0901 08:54:52.704981 140646288807744 basic_session_run_hooks.py:260] loss = 4.3549953, step = 19600 (13.800 sec)
I0901 08:55:06.224550 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39623
I0901 08:55:06.225284 140646288807744 basic_session_run_hooks.py:260] loss = 2.611418, step = 19700 (13.520 sec)
I0901 08:55:19.745824 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39575
I0901 08:55:19.746529 140646288807744 basic_session_run_hooks.py:260] loss = 3.0461667, step = 19800 (13.521 sec)
I0901 08:55:33.388678 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32984
I0901 08:55:33.389315 140646288807744 basic_session_run_hooks.py:260] loss = 2.7193127, step = 19900 (13.643 sec)
I0901 08:55:46.987352 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 20000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
W0901 08:55:47.443901 140646288807744 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
I0901 08:55:48.168394 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 08:55:48.313246 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.70036
I0901 08:55:48.313946 140646288807744 basic_session_run_hooks.py:260] loss = 2.8812313, step = 20000 (14.925 sec)
I0901 08:56:01.897300 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36157
I0901 08:56:01.897935 140646288807744 basic_session_run_hooks.py:260] loss = 3.007787, step = 20100 (13.584 sec)
I0901 08:56:15.486830 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35861
I0901 08:56:15.487569 140646288807744 basic_session_run_hooks.py:260] loss = 2.7630534, step = 20200 (13.590 sec)
I0901 08:56:29.088015 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35231
I0901 08:56:29.088866 140646288807744 basic_session_run_hooks.py:260] loss = 2.9423127, step = 20300 (13.601 sec)
I0901 08:56:42.867780 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25701
I0901 08:56:42.868498 140646288807744 basic_session_run_hooks.py:260] loss = 2.5148218, step = 20400 (13.780 sec)
I0901 08:56:56.460299 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.357
I0901 08:56:56.461005 140646288807744 basic_session_run_hooks.py:260] loss = 1.5964069, step = 20500 (13.593 sec)
I0901 08:57:09.919254 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42998
I0901 08:57:09.919949 140646288807744 basic_session_run_hooks.py:260] loss = 2.8614123, step = 20600 (13.459 sec)
I0901 08:57:23.704341 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25422
I0901 08:57:23.705029 140646288807744 basic_session_run_hooks.py:260] loss = 2.7547872, step = 20700 (13.785 sec)
I0901 08:57:37.582871 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.20537
I0901 08:57:37.583669 140646288807744 basic_session_run_hooks.py:260] loss = 1.9724486, step = 20800 (13.879 sec)
I0901 08:57:51.234802 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32497
I0901 08:57:51.235480 140646288807744 basic_session_run_hooks.py:260] loss = 3.5791543, step = 20900 (13.652 sec)
I0901 08:58:04.677107 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 21000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 08:58:05.863006 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 08:58:05.863965 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 08:58:05.973557 140646288807744 estimator.py:1145] Calling model_fn.
I0901 08:58:05.974470 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 08:58:05.974871 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 08:58:05.974986 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 08:58:05.975089 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 08:58:05.975186 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 08:58:05.975309 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 08:58:05.975419 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 08:58:06.028972 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 08:58:06.077083 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 08:58:06.161058 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 08:58:06.168748 140646288807744 t2t_model.py:2248] Building model body
I0901 08:58:09.485473 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 08:58:09.905407 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 08:58:09.916526 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T08:58:09Z
I0901 08:58:10.271858 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 08:58:10.272328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:58:10.272771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 08:58:10.272864: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 08:58:10.272917: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 08:58:10.272936: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 08:58:10.272969: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 08:58:10.273009: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 08:58:10.273040: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 08:58:10.273082: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 08:58:10.273206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:58:10.273596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:58:10.273942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 08:58:10.274000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 08:58:10.274012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 08:58:10.274021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 08:58:10.274125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:58:10.274468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 08:58:10.274875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 08:58:10.275820 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-21000
I0901 08:58:10.886239 140646288807744 session_manager.py:500] Running local_init_op.
I0901 08:58:10.961435 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 08:58:13.786750 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 08:58:15.136296 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 08:58:16.577901 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 08:58:17.736863 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 08:58:18.986975 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 08:58:20.237910 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 08:58:21.422631 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 08:58:22.785017 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 08:58:23.970602 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 08:58:25.261375 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 08:58:25.407540 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-08:58:25
I0901 08:58:25.407732 140646288807744 estimator.py:2039] Saving dict for global step 21000: global_step = 21000, loss = 2.84541, metrics-translate_ende_wmt8k/targets/accuracy = 0.50191003, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.6946497, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.20688733, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.8456385, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.2755043, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.48899567
I0901 08:58:25.408135 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 21000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-21000
I0901 08:58:25.541436 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.91489
I0901 08:58:25.542171 140646288807744 basic_session_run_hooks.py:260] loss = 2.4060297, step = 21000 (34.307 sec)
2019-09-01 08:58:38.136538: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:58:38.137141: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:58:38.223375: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:58:38.223864: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:58:38.224848: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 08:58:38.225318: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 08:58:39.139503 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35399
I0901 08:58:39.140214 140646288807744 basic_session_run_hooks.py:260] loss = 2.6719472, step = 21100 (13.598 sec)
I0901 08:58:52.657607 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39749
I0901 08:58:52.658418 140646288807744 basic_session_run_hooks.py:260] loss = 2.8376458, step = 21200 (13.518 sec)
I0901 08:59:06.276516 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34273
I0901 08:59:06.277269 140646288807744 basic_session_run_hooks.py:260] loss = 2.788001, step = 21300 (13.619 sec)
I0901 08:59:19.897244 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34175
I0901 08:59:19.898366 140646288807744 basic_session_run_hooks.py:260] loss = 2.9393864, step = 21400 (13.621 sec)
I0901 08:59:33.410960 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39989
I0901 08:59:33.411694 140646288807744 basic_session_run_hooks.py:260] loss = 2.4301617, step = 21500 (13.513 sec)
I0901 08:59:46.962097 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37945
I0901 08:59:46.962879 140646288807744 basic_session_run_hooks.py:260] loss = 2.7444518, step = 21600 (13.551 sec)
I0901 09:00:00.700707 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27876
I0901 09:00:00.701681 140646288807744 basic_session_run_hooks.py:260] loss = 2.411569, step = 21700 (13.739 sec)
I0901 09:00:14.143240 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43907
I0901 09:00:14.144020 140646288807744 basic_session_run_hooks.py:260] loss = 2.5411777, step = 21800 (13.442 sec)
I0901 09:00:27.700546 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3761
I0901 09:00:27.701178 140646288807744 basic_session_run_hooks.py:260] loss = 2.3953402, step = 21900 (13.557 sec)
I0901 09:00:41.201680 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 22000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:00:42.395905 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:00:42.544698 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.73666
I0901 09:00:42.545512 140646288807744 basic_session_run_hooks.py:260] loss = 2.9611702, step = 22000 (14.844 sec)
2019-09-01 09:00:44.703461: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:00:44.704439: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:00:44.812061: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:00:44.812732: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:00:56.079106 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38858
I0901 09:00:56.079943 140646288807744 basic_session_run_hooks.py:260] loss = 2.7389345, step = 22100 (13.534 sec)
I0901 09:01:09.978576 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.19452
I0901 09:01:09.979429 140646288807744 basic_session_run_hooks.py:260] loss = 2.8365972, step = 22200 (13.899 sec)
I0901 09:01:23.561731 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36206
I0901 09:01:23.562481 140646288807744 basic_session_run_hooks.py:260] loss = 2.4644766, step = 22300 (13.583 sec)
I0901 09:01:37.132720 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36866
I0901 09:01:37.133386 140646288807744 basic_session_run_hooks.py:260] loss = 2.4201639, step = 22400 (13.571 sec)
I0901 09:01:50.799235 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31715
I0901 09:01:50.800174 140646288807744 basic_session_run_hooks.py:260] loss = 2.5416327, step = 22500 (13.667 sec)
I0901 09:02:04.363618 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37225
I0901 09:02:04.364340 140646288807744 basic_session_run_hooks.py:260] loss = 2.42113, step = 22600 (13.564 sec)
I0901 09:02:18.041216 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31122
I0901 09:02:18.042042 140646288807744 basic_session_run_hooks.py:260] loss = 2.6154983, step = 22700 (13.678 sec)
I0901 09:02:31.565399 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39416
I0901 09:02:31.566011 140646288807744 basic_session_run_hooks.py:260] loss = 2.5699148, step = 22800 (13.524 sec)
I0901 09:02:45.018627 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43316
I0901 09:02:45.019246 140646288807744 basic_session_run_hooks.py:260] loss = 2.3306906, step = 22900 (13.453 sec)
I0901 09:02:58.509261 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 23000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:02:59.677450 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:02:59.815840 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.75803
I0901 09:02:59.816565 140646288807744 basic_session_run_hooks.py:260] loss = 2.1528478, step = 23000 (14.797 sec)
2019-09-01 09:03:10.960835: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:03:10.991720: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:03:11.111222: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:03:11.111677: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:03:11.112660: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:03:11.113099: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:03:13.442192 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33872
I0901 09:03:13.442934 140646288807744 basic_session_run_hooks.py:260] loss = 2.7270002, step = 23100 (13.626 sec)
I0901 09:03:27.050208 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34861
I0901 09:03:27.050857 140646288807744 basic_session_run_hooks.py:260] loss = 2.6703799, step = 23200 (13.608 sec)
I0901 09:03:40.651401 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3523
I0901 09:03:40.652098 140646288807744 basic_session_run_hooks.py:260] loss = 2.4948552, step = 23300 (13.601 sec)
I0901 09:03:54.280912 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33702
I0901 09:03:54.281561 140646288807744 basic_session_run_hooks.py:260] loss = 2.4189034, step = 23400 (13.629 sec)
I0901 09:04:08.250679 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.15832
I0901 09:04:08.251352 140646288807744 basic_session_run_hooks.py:260] loss = 2.490998, step = 23500 (13.970 sec)
I0901 09:04:21.776417 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39331
I0901 09:04:21.777122 140646288807744 basic_session_run_hooks.py:260] loss = 2.8505752, step = 23600 (13.526 sec)
I0901 09:04:35.480034 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29735
I0901 09:04:35.480775 140646288807744 basic_session_run_hooks.py:260] loss = 3.2637067, step = 23700 (13.704 sec)
I0901 09:04:48.997666 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39774
I0901 09:04:48.998307 140646288807744 basic_session_run_hooks.py:260] loss = 2.4923635, step = 23800 (13.518 sec)
I0901 09:05:02.696156 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30007
I0901 09:05:02.696820 140646288807744 basic_session_run_hooks.py:260] loss = 2.0823717, step = 23900 (13.699 sec)
I0901 09:05:16.148668 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 24000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:05:17.312186 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:05:17.444504 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.78042
I0901 09:05:17.445273 140646288807744 basic_session_run_hooks.py:260] loss = 2.8608475, step = 24000 (14.748 sec)
I0901 09:05:31.125681 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30931
I0901 09:05:31.126350 140646288807744 basic_session_run_hooks.py:260] loss = 2.3577, step = 24100 (13.681 sec)
I0901 09:05:44.819173 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30274
I0901 09:05:44.819854 140646288807744 basic_session_run_hooks.py:260] loss = 2.1249368, step = 24200 (13.694 sec)
I0901 09:05:58.384841 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37155
I0901 09:05:58.385570 140646288807744 basic_session_run_hooks.py:260] loss = 2.358629, step = 24300 (13.566 sec)
I0901 09:06:11.918011 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38925
I0901 09:06:11.918748 140646288807744 basic_session_run_hooks.py:260] loss = 2.500483, step = 24400 (13.533 sec)
I0901 09:06:25.567262 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32641
I0901 09:06:25.567924 140646288807744 basic_session_run_hooks.py:260] loss = 2.4631877, step = 24500 (13.649 sec)
I0901 09:06:39.215206 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32711
I0901 09:06:39.215888 140646288807744 basic_session_run_hooks.py:260] loss = 2.5015798, step = 24600 (13.648 sec)
I0901 09:06:52.870065 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3234
I0901 09:06:52.870730 140646288807744 basic_session_run_hooks.py:260] loss = 2.5305767, step = 24700 (13.655 sec)
I0901 09:07:06.316848 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43672
I0901 09:07:06.317603 140646288807744 basic_session_run_hooks.py:260] loss = 2.287806, step = 24800 (13.447 sec)
I0901 09:07:19.820680 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40531
I0901 09:07:19.821417 140646288807744 basic_session_run_hooks.py:260] loss = 2.4865959, step = 24900 (13.504 sec)
I0901 09:07:33.158620 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 25000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:07:34.330467 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:07:34.473467 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.82464
I0901 09:07:34.474173 140646288807744 basic_session_run_hooks.py:260] loss = 2.943873, step = 25000 (14.653 sec)
I0901 09:07:48.045051 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36834
I0901 09:07:48.045839 140646288807744 basic_session_run_hooks.py:260] loss = 3.0938215, step = 25100 (13.572 sec)
I0901 09:08:01.676065 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33622
I0901 09:08:01.676759 140646288807744 basic_session_run_hooks.py:260] loss = 3.2862186, step = 25200 (13.631 sec)
I0901 09:08:15.406494 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28309
I0901 09:08:15.407166 140646288807744 basic_session_run_hooks.py:260] loss = 2.4295096, step = 25300 (13.730 sec)
I0901 09:08:28.800364 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4661
I0901 09:08:28.801303 140646288807744 basic_session_run_hooks.py:260] loss = 2.8085515, step = 25400 (13.394 sec)
I0901 09:08:42.417965 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34344
I0901 09:08:42.418623 140646288807744 basic_session_run_hooks.py:260] loss = 2.2345986, step = 25500 (13.617 sec)
I0901 09:08:56.297226 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.20499
I0901 09:08:56.298172 140646288807744 basic_session_run_hooks.py:260] loss = 3.0859964, step = 25600 (13.880 sec)
I0901 09:09:09.849618 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37877
I0901 09:09:09.850333 140646288807744 basic_session_run_hooks.py:260] loss = 2.1862378, step = 25700 (13.552 sec)
I0901 09:09:23.609152 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26769
I0901 09:09:23.609818 140646288807744 basic_session_run_hooks.py:260] loss = 2.1758597, step = 25800 (13.759 sec)
I0901 09:09:37.306040 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30093
I0901 09:09:37.306773 140646288807744 basic_session_run_hooks.py:260] loss = 1.932735, step = 25900 (13.697 sec)
I0901 09:09:50.887502 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 26000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:09:52.071068 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 09:09:52.071974 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 09:09:52.179758 140646288807744 estimator.py:1145] Calling model_fn.
I0901 09:09:52.180326 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 09:09:52.180560 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 09:09:52.180625 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 09:09:52.180704 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 09:09:52.180775 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 09:09:52.180832 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 09:09:52.180890 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 09:09:52.233088 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 09:09:52.280255 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 09:09:52.364716 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 09:09:52.372613 140646288807744 t2t_model.py:2248] Building model body
I0901 09:09:55.510237 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 09:09:55.920581 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 09:09:55.931975 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T09:09:55Z
I0901 09:09:56.279253 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 09:09:56.279695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:09:56.279954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 09:09:56.280019: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 09:09:56.280071: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 09:09:56.280084: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 09:09:56.280095: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 09:09:56.280106: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 09:09:56.280117: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 09:09:56.280128: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 09:09:56.280186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:09:56.280428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:09:56.280697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 09:09:56.280736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 09:09:56.280743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 09:09:56.280749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 09:09:56.280847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:09:56.281142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:09:56.281384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 09:09:56.282231 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-26000
I0901 09:09:56.883500 140646288807744 session_manager.py:500] Running local_init_op.
I0901 09:09:56.958973 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 09:09:59.860575 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 09:10:01.057867 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 09:10:02.240660 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 09:10:03.646564 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 09:10:04.703613 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 09:10:05.938222 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 09:10:07.133693 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 09:10:08.332698 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 09:10:09.515538 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 09:10:10.711544 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 09:10:10.858264 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-09:10:10
I0901 09:10:10.858426 140646288807744 estimator.py:2039] Saving dict for global step 26000: global_step = 26000, loss = 2.6717942, metrics-translate_ende_wmt8k/targets/accuracy = 0.52463675, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.7159895, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.22838087, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.6722105, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.2981859, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.5097337
I0901 09:10:10.858886 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 26000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-26000
I0901 09:10:10.991818 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.96861
I0901 09:10:10.992603 140646288807744 basic_session_run_hooks.py:260] loss = 2.5050397, step = 26000 (33.686 sec)
I0901 09:10:24.627528 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33369
I0901 09:10:24.628213 140646288807744 basic_session_run_hooks.py:260] loss = 2.8685505, step = 26100 (13.636 sec)
I0901 09:10:38.452682 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23319
I0901 09:10:38.453543 140646288807744 basic_session_run_hooks.py:260] loss = 2.5641627, step = 26200 (13.825 sec)
I0901 09:10:51.889806 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.44207
I0901 09:10:51.890625 140646288807744 basic_session_run_hooks.py:260] loss = 2.3894022, step = 26300 (13.437 sec)
I0901 09:11:05.471717 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36273
I0901 09:11:05.472478 140646288807744 basic_session_run_hooks.py:260] loss = 2.2417705, step = 26400 (13.582 sec)
I0901 09:11:19.262049 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25146
I0901 09:11:19.262892 140646288807744 basic_session_run_hooks.py:260] loss = 2.4367013, step = 26500 (13.790 sec)
I0901 09:11:33.039753 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25811
I0901 09:11:33.040720 140646288807744 basic_session_run_hooks.py:260] loss = 2.12523, step = 26600 (13.778 sec)
I0901 09:11:46.619886 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3637
I0901 09:11:46.620709 140646288807744 basic_session_run_hooks.py:260] loss = 2.8203435, step = 26700 (13.580 sec)
I0901 09:12:00.113164 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4111
I0901 09:12:00.113810 140646288807744 basic_session_run_hooks.py:260] loss = 2.7557075, step = 26800 (13.493 sec)
I0901 09:12:13.764224 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32544
I0901 09:12:13.764889 140646288807744 basic_session_run_hooks.py:260] loss = 2.561472, step = 26900 (13.651 sec)
I0901 09:12:27.500969 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 27000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:12:28.682374 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:12:28.826980 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.63889
I0901 09:12:28.827773 140646288807744 basic_session_run_hooks.py:260] loss = 2.48953, step = 27000 (15.063 sec)
I0901 09:12:42.438150 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34691
I0901 09:12:42.438862 140646288807744 basic_session_run_hooks.py:260] loss = 2.4437678, step = 27100 (13.611 sec)
2019-09-01 09:12:52.880058: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:12:52.880538: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:12:52.957685: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:12:52.958387: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:12:52.991940: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:12:52.992767: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:12:53.060177: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:12:53.060848: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:12:53.062438: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:12:53.063094: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:12:56.084237 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32811
I0901 09:12:56.084899 140646288807744 basic_session_run_hooks.py:260] loss = 2.5196881, step = 27200 (13.646 sec)
2019-09-01 09:13:02.568106: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.568569: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.606529: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.606992: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.690599: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.691048: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.691951: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.692387: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.740933: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.741373: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.742291: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:02.742724: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:13:09.932182 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22129
I0901 09:13:09.932862 140646288807744 basic_session_run_hooks.py:260] loss = 2.2203083, step = 27300 (13.848 sec)
2019-09-01 09:13:11.002972: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.003845: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.041822: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.042284: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.130663: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.131119: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.132039: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.132475: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.183057: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.183502: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.184423: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:11.184858: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.349986: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.350505: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.443148: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.443630: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.480724: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.481393: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.483087: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.483724: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.552783: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.553435: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.555187: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:13:23.555832: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:13:24.046107 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.0852
I0901 09:13:24.046857 140646288807744 basic_session_run_hooks.py:260] loss = 2.333578, step = 27400 (14.114 sec)
I0901 09:13:37.515675 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42414
I0901 09:13:37.516395 140646288807744 basic_session_run_hooks.py:260] loss = 3.0313234, step = 27500 (13.470 sec)
I0901 09:13:51.123143 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34891
I0901 09:13:51.123810 140646288807744 basic_session_run_hooks.py:260] loss = 2.2473905, step = 27600 (13.607 sec)
I0901 09:14:04.727382 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35065
I0901 09:14:04.728031 140646288807744 basic_session_run_hooks.py:260] loss = 2.5022032, step = 27700 (13.604 sec)
I0901 09:14:18.260706 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38917
I0901 09:14:18.261415 140646288807744 basic_session_run_hooks.py:260] loss = 2.0715578, step = 27800 (13.533 sec)
I0901 09:14:31.743404 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41691
I0901 09:14:31.744210 140646288807744 basic_session_run_hooks.py:260] loss = 2.177754, step = 27900 (13.483 sec)
2019-09-01 09:14:32.957450: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:14:32.957914: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:14:33.078426: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:14:33.078886: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:14:33.079932: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:14:33.080380: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:14:45.394308 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 28000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:14:46.568937 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:14:46.712874 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.68026
I0901 09:14:46.713611 140646288807744 basic_session_run_hooks.py:260] loss = 2.1532261, step = 28000 (14.969 sec)
I0901 09:15:00.026061 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.51135
I0901 09:15:00.026755 140646288807744 basic_session_run_hooks.py:260] loss = 2.456235, step = 28100 (13.313 sec)
I0901 09:15:13.592037 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37139
I0901 09:15:13.592698 140646288807744 basic_session_run_hooks.py:260] loss = 1.902301, step = 28200 (13.566 sec)
I0901 09:15:27.367281 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.2594
I0901 09:15:27.367949 140646288807744 basic_session_run_hooks.py:260] loss = 2.2259789, step = 28300 (13.775 sec)
I0901 09:15:40.856973 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41307
I0901 09:15:40.857609 140646288807744 basic_session_run_hooks.py:260] loss = 2.032474, step = 28400 (13.490 sec)
I0901 09:15:54.956915 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.09223
I0901 09:15:54.957550 140646288807744 basic_session_run_hooks.py:260] loss = 2.1776118, step = 28500 (14.100 sec)
I0901 09:16:08.585927 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33729
I0901 09:16:08.586644 140646288807744 basic_session_run_hooks.py:260] loss = 1.8261895, step = 28600 (13.629 sec)
I0901 09:16:22.245620 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32081
I0901 09:16:22.246483 140646288807744 basic_session_run_hooks.py:260] loss = 2.1147945, step = 28700 (13.660 sec)
I0901 09:16:35.837806 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35717
I0901 09:16:35.838463 140646288807744 basic_session_run_hooks.py:260] loss = 2.8296337, step = 28800 (13.592 sec)
I0901 09:16:49.383911 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38219
I0901 09:16:49.384782 140646288807744 basic_session_run_hooks.py:260] loss = 2.8882062, step = 28900 (13.546 sec)
I0901 09:17:02.820565 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 29000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:17:04.052064 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:17:04.183400 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.75699
I0901 09:17:04.184184 140646288807744 basic_session_run_hooks.py:260] loss = 2.4008296, step = 29000 (14.799 sec)
I0901 09:17:17.596889 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.45518
I0901 09:17:17.597625 140646288807744 basic_session_run_hooks.py:260] loss = 2.4265347, step = 29100 (13.413 sec)
I0901 09:17:31.311328 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29158
I0901 09:17:31.312232 140646288807744 basic_session_run_hooks.py:260] loss = 2.4545836, step = 29200 (13.715 sec)
I0901 09:17:44.901180 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35843
I0901 09:17:44.901871 140646288807744 basic_session_run_hooks.py:260] loss = 2.2557507, step = 29300 (13.590 sec)
I0901 09:17:58.315118 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.45494
I0901 09:17:58.315874 140646288807744 basic_session_run_hooks.py:260] loss = 2.825142, step = 29400 (13.414 sec)
I0901 09:18:12.090579 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25929
I0901 09:18:12.091247 140646288807744 basic_session_run_hooks.py:260] loss = 2.3135371, step = 29500 (13.775 sec)
I0901 09:18:25.699746 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34799
I0901 09:18:25.700607 140646288807744 basic_session_run_hooks.py:260] loss = 2.867065, step = 29600 (13.609 sec)
I0901 09:18:39.174661 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4212
I0901 09:18:39.175433 140646288807744 basic_session_run_hooks.py:260] loss = 2.288539, step = 29700 (13.475 sec)
I0901 09:18:52.708720 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38877
I0901 09:18:52.709575 140646288807744 basic_session_run_hooks.py:260] loss = 3.0549922, step = 29800 (13.534 sec)
I0901 09:19:06.295350 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36018
I0901 09:19:06.296282 140646288807744 basic_session_run_hooks.py:260] loss = 2.1052883, step = 29900 (13.587 sec)
I0901 09:19:19.844381 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 30000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:19:20.999681 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:19:21.144724 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.73429
I0901 09:19:21.145477 140646288807744 basic_session_run_hooks.py:260] loss = 2.7701335, step = 30000 (14.849 sec)
I0901 09:19:34.774532 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33686
I0901 09:19:34.775329 140646288807744 basic_session_run_hooks.py:260] loss = 2.3152769, step = 30100 (13.630 sec)
I0901 09:19:48.518678 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27583
I0901 09:19:48.519546 140646288807744 basic_session_run_hooks.py:260] loss = 2.470072, step = 30200 (13.744 sec)
2019-09-01 09:19:58.873319: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:19:58.873783: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:19:58.972927: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:19:58.973383: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:19:58.974289: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:19:58.974721: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:20:02.209357 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30424
I0901 09:20:02.209979 140646288807744 basic_session_run_hooks.py:260] loss = 2.1363683, step = 30300 (13.690 sec)
2019-09-01 09:20:08.861981: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:20:08.862645: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:20:08.967544: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:20:08.968062: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:20:08.969223: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:20:08.969704: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:20:16.001294 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25062
I0901 09:20:16.002149 140646288807744 basic_session_run_hooks.py:260] loss = 2.4109643, step = 30400 (13.792 sec)
I0901 09:20:30.204550 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.04064
I0901 09:20:30.205351 140646288807744 basic_session_run_hooks.py:260] loss = 2.031798, step = 30500 (14.203 sec)
I0901 09:20:43.750778 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38213
I0901 09:20:43.751457 140646288807744 basic_session_run_hooks.py:260] loss = 2.2614353, step = 30600 (13.546 sec)
I0901 09:20:57.353350 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35155
I0901 09:20:57.354051 140646288807744 basic_session_run_hooks.py:260] loss = 2.1980517, step = 30700 (13.603 sec)
I0901 09:21:10.909688 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37662
I0901 09:21:10.910402 140646288807744 basic_session_run_hooks.py:260] loss = 2.3079016, step = 30800 (13.556 sec)
I0901 09:21:24.460567 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37959
I0901 09:21:24.461189 140646288807744 basic_session_run_hooks.py:260] loss = 2.4363759, step = 30900 (13.551 sec)
I0901 09:21:38.068180 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 31000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:21:39.362511 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 09:21:39.363449 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 09:21:39.473328 140646288807744 estimator.py:1145] Calling model_fn.
I0901 09:21:39.473932 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 09:21:39.474227 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 09:21:39.474293 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 09:21:39.474368 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 09:21:39.474436 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 09:21:39.474508 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 09:21:39.474578 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 09:21:39.526540 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 09:21:39.573112 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 09:21:39.656113 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 09:21:39.663683 140646288807744 t2t_model.py:2248] Building model body
I0901 09:21:42.763725 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 09:21:43.404802 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 09:21:43.415812 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T09:21:43Z
I0901 09:21:43.772066 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 09:21:43.772571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:21:43.772830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 09:21:43.772885: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 09:21:43.772928: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 09:21:43.772940: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 09:21:43.772952: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 09:21:43.772964: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 09:21:43.772975: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 09:21:43.772986: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 09:21:43.773031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:21:43.773309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:21:43.773543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 09:21:43.773580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 09:21:43.773594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 09:21:43.773600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 09:21:43.773697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:21:43.773983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:21:43.774218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 09:21:43.775177 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-31000
I0901 09:21:44.396849 140646288807744 session_manager.py:500] Running local_init_op.
I0901 09:21:44.474715 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 09:21:47.547361 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 09:21:48.843734 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 09:21:49.959387 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 09:21:51.310805 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 09:21:52.545377 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 09:21:53.905749 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 09:21:55.190610 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 09:21:56.445393 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 09:21:57.783450 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 09:21:59.038923 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 09:21:59.166356 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-09:21:59
I0901 09:21:59.166506 140646288807744 estimator.py:2039] Saving dict for global step 31000: global_step = 31000, loss = 2.5484939, metrics-translate_ende_wmt8k/targets/accuracy = 0.54430526, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.7310762, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.24622324, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.5486012, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.31807414, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.5281686
I0901 09:21:59.166916 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 31000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-31000
I0901 09:21:59.299604 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.87034
I0901 09:21:59.300315 140646288807744 basic_session_run_hooks.py:260] loss = 2.2851026, step = 31000 (34.839 sec)
I0901 09:22:12.871788 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36801
I0901 09:22:12.872744 140646288807744 basic_session_run_hooks.py:260] loss = 2.366789, step = 31100 (13.572 sec)
I0901 09:22:26.412672 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38504
I0901 09:22:26.413342 140646288807744 basic_session_run_hooks.py:260] loss = 2.8994083, step = 31200 (13.541 sec)
I0901 09:22:39.854611 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4394
I0901 09:22:39.855354 140646288807744 basic_session_run_hooks.py:260] loss = 2.562094, step = 31300 (13.442 sec)
I0901 09:22:53.311234 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43129
I0901 09:22:53.312115 140646288807744 basic_session_run_hooks.py:260] loss = 2.7411335, step = 31400 (13.457 sec)
I0901 09:23:06.866431 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37724
I0901 09:23:06.867153 140646288807744 basic_session_run_hooks.py:260] loss = 2.7955492, step = 31500 (13.555 sec)
I0901 09:23:20.304809 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.44138
I0901 09:23:20.305639 140646288807744 basic_session_run_hooks.py:260] loss = 2.0362918, step = 31600 (13.438 sec)
I0901 09:23:34.496073 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.04659
I0901 09:23:34.496778 140646288807744 basic_session_run_hooks.py:260] loss = 2.1994762, step = 31700 (14.191 sec)
I0901 09:23:48.192868 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30097
I0901 09:23:48.193791 140646288807744 basic_session_run_hooks.py:260] loss = 2.300966, step = 31800 (13.697 sec)
I0901 09:24:02.006824 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23906
I0901 09:24:02.007640 140646288807744 basic_session_run_hooks.py:260] loss = 2.3323982, step = 31900 (13.814 sec)
I0901 09:24:15.462641 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 32000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:24:16.633343 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:24:16.777247 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.77028
I0901 09:24:16.778084 140646288807744 basic_session_run_hooks.py:260] loss = 2.1815302, step = 32000 (14.770 sec)
I0901 09:24:30.374139 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35462
I0901 09:24:30.374843 140646288807744 basic_session_run_hooks.py:260] loss = 2.9925907, step = 32100 (13.597 sec)
I0901 09:24:43.901267 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39255
I0901 09:24:43.902095 140646288807744 basic_session_run_hooks.py:260] loss = 2.2031379, step = 32200 (13.527 sec)
I0901 09:24:57.394232 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41127
I0901 09:24:57.394874 140646288807744 basic_session_run_hooks.py:260] loss = 2.4594305, step = 32300 (13.493 sec)
I0901 09:25:11.006933 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34608
I0901 09:25:11.007930 140646288807744 basic_session_run_hooks.py:260] loss = 2.711527, step = 32400 (13.613 sec)
I0901 09:25:24.438315 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.44525
I0901 09:25:24.438999 140646288807744 basic_session_run_hooks.py:260] loss = 2.660448, step = 32500 (13.431 sec)
I0901 09:25:38.022197 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36166
I0901 09:25:38.022839 140646288807744 basic_session_run_hooks.py:260] loss = 2.3664377, step = 32600 (13.584 sec)
I0901 09:25:51.600223 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36484
I0901 09:25:51.600993 140646288807744 basic_session_run_hooks.py:260] loss = 2.4789944, step = 32700 (13.578 sec)
I0901 09:26:05.174835 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36669
I0901 09:26:05.175573 140646288807744 basic_session_run_hooks.py:260] loss = 2.1835034, step = 32800 (13.575 sec)
I0901 09:26:18.537864 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.48333
I0901 09:26:18.538559 140646288807744 basic_session_run_hooks.py:260] loss = 2.3782873, step = 32900 (13.363 sec)
I0901 09:26:31.985525 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 33000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:26:33.153335 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:26:33.293323 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.77715
I0901 09:26:33.294169 140646288807744 basic_session_run_hooks.py:260] loss = 2.4184887, step = 33000 (14.756 sec)
I0901 09:26:46.953431 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32059
I0901 09:26:46.954174 140646288807744 basic_session_run_hooks.py:260] loss = 2.1139023, step = 33100 (13.660 sec)
I0901 09:27:00.408027 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43241
I0901 09:27:00.408711 140646288807744 basic_session_run_hooks.py:260] loss = 2.507537, step = 33200 (13.455 sec)
I0901 09:27:14.075157 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31682
I0901 09:27:14.075841 140646288807744 basic_session_run_hooks.py:260] loss = 2.7199237, step = 33300 (13.667 sec)
I0901 09:27:27.699190 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33997
I0901 09:27:27.700112 140646288807744 basic_session_run_hooks.py:260] loss = 2.382792, step = 33400 (13.624 sec)
I0901 09:27:41.212577 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40007
I0901 09:27:41.213272 140646288807744 basic_session_run_hooks.py:260] loss = 2.837981, step = 33500 (13.513 sec)
I0901 09:27:54.791048 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3646
I0901 09:27:54.791798 140646288807744 basic_session_run_hooks.py:260] loss = 2.8200417, step = 33600 (13.579 sec)
I0901 09:28:08.306496 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39894
I0901 09:28:08.307334 140646288807744 basic_session_run_hooks.py:260] loss = 2.8910222, step = 33700 (13.516 sec)
I0901 09:28:21.824296 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39765
I0901 09:28:21.825020 140646288807744 basic_session_run_hooks.py:260] loss = 3.1544635, step = 33800 (13.518 sec)
I0901 09:28:35.342772 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39728
I0901 09:28:35.343479 140646288807744 basic_session_run_hooks.py:260] loss = 2.2709055, step = 33900 (13.518 sec)
I0901 09:28:48.889976 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 34000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:28:50.069400 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:28:50.214852 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.72401
I0901 09:28:50.215526 140646288807744 basic_session_run_hooks.py:260] loss = 2.5768912, step = 34000 (14.872 sec)
I0901 09:29:03.717663 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40588
I0901 09:29:03.718315 140646288807744 basic_session_run_hooks.py:260] loss = 1.7217057, step = 34100 (13.503 sec)
I0901 09:29:17.285596 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37031
I0901 09:29:17.286351 140646288807744 basic_session_run_hooks.py:260] loss = 2.1011047, step = 34200 (13.568 sec)
I0901 09:29:30.728848 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43868
I0901 09:29:30.729675 140646288807744 basic_session_run_hooks.py:260] loss = 2.25373, step = 34300 (13.443 sec)
I0901 09:29:44.311456 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36236
I0901 09:29:44.312390 140646288807744 basic_session_run_hooks.py:260] loss = 1.9432937, step = 34400 (13.583 sec)
2019-09-01 09:29:51.918935: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:51.919661: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:51.963020: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:51.963509: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:52.078297: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:52.078982: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:52.080047: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:52.080701: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:52.140667: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:52.141120: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:52.142159: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:29:52.142594: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:29:58.403746 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.09608
I0901 09:29:58.404397 140646288807744 basic_session_run_hooks.py:260] loss = 2.0708501, step = 34500 (14.092 sec)
I0901 09:30:12.307449 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.19233
I0901 09:30:12.308176 140646288807744 basic_session_run_hooks.py:260] loss = 2.3171377, step = 34600 (13.904 sec)
I0901 09:30:25.991282 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3079
I0901 09:30:25.992018 140646288807744 basic_session_run_hooks.py:260] loss = 2.3131905, step = 34700 (13.684 sec)
I0901 09:30:39.542506 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37941
I0901 09:30:39.543292 140646288807744 basic_session_run_hooks.py:260] loss = 2.279466, step = 34800 (13.551 sec)
I0901 09:30:53.081262 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3862
I0901 09:30:53.081974 140646288807744 basic_session_run_hooks.py:260] loss = 2.3969123, step = 34900 (13.539 sec)
I0901 09:31:06.370771 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 35000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:31:07.565182 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:31:07.705618 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.83791
I0901 09:31:07.706318 140646288807744 basic_session_run_hooks.py:260] loss = 2.0584242, step = 35000 (14.624 sec)
I0901 09:31:21.242273 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38735
I0901 09:31:21.243210 140646288807744 basic_session_run_hooks.py:260] loss = 2.1908464, step = 35100 (13.537 sec)
I0901 09:31:35.113009 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.20942
I0901 09:31:35.113790 140646288807744 basic_session_run_hooks.py:260] loss = 2.6181617, step = 35200 (13.871 sec)
I0901 09:31:48.696209 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36203
I0901 09:31:48.696891 140646288807744 basic_session_run_hooks.py:260] loss = 1.8682332, step = 35300 (13.583 sec)
I0901 09:32:02.274909 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36448
I0901 09:32:02.275704 140646288807744 basic_session_run_hooks.py:260] loss = 2.1132438, step = 35400 (13.579 sec)
I0901 09:32:15.885244 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34736
I0901 09:32:15.885887 140646288807744 basic_session_run_hooks.py:260] loss = 3.158954, step = 35500 (13.610 sec)
I0901 09:32:29.441122 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37687
I0901 09:32:29.441909 140646288807744 basic_session_run_hooks.py:260] loss = 2.301238, step = 35600 (13.556 sec)
I0901 09:32:42.869657 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.44684
I0901 09:32:42.870576 140646288807744 basic_session_run_hooks.py:260] loss = 2.5039818, step = 35700 (13.429 sec)
I0901 09:32:56.482619 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34593
I0901 09:32:56.483292 140646288807744 basic_session_run_hooks.py:260] loss = 1.8357959, step = 35800 (13.613 sec)
I0901 09:33:09.970182 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41424
I0901 09:33:09.971151 140646288807744 basic_session_run_hooks.py:260] loss = 2.31563, step = 35900 (13.488 sec)
I0901 09:33:23.474645 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 36000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:33:24.660786 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 09:33:24.662159 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 09:33:24.777210 140646288807744 estimator.py:1145] Calling model_fn.
I0901 09:33:24.777849 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 09:33:24.778100 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 09:33:24.778169 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 09:33:24.778244 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 09:33:24.778313 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 09:33:24.778388 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 09:33:24.778454 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 09:33:24.831578 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 09:33:24.880206 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 09:33:24.966245 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 09:33:24.973905 140646288807744 t2t_model.py:2248] Building model body
I0901 09:33:28.186318 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 09:33:28.606932 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 09:33:28.618191 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T09:33:28Z
I0901 09:33:29.037827 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 09:33:29.038283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:33:29.038557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 09:33:29.038603: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 09:33:29.038618: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 09:33:29.038644: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 09:33:29.038672: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 09:33:29.038684: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 09:33:29.038704: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 09:33:29.038729: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 09:33:29.038801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:33:29.039081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:33:29.039413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 09:33:29.039446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 09:33:29.039477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 09:33:29.039484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 09:33:29.039608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:33:29.039847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:33:29.040082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 09:33:29.041109 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-36000
I0901 09:33:29.656985 140646288807744 session_manager.py:500] Running local_init_op.
I0901 09:33:29.735271 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 09:33:32.590350 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 09:33:33.884880 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 09:33:35.170443 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 09:33:36.576268 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 09:33:37.819905 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 09:33:39.136537 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 09:33:40.323199 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 09:33:41.776379 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 09:33:43.128091 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 09:33:44.424713 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 09:33:44.540998 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-09:33:44
I0901 09:33:44.541153 140646288807744 estimator.py:2039] Saving dict for global step 36000: global_step = 36000, loss = 2.4829056, metrics-translate_ende_wmt8k/targets/accuracy = 0.5528207, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.7390686, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.2535968, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.4832282, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.32591006, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.535616
I0901 09:33:44.541477 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 36000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-36000
I0901 09:33:44.684817 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.88063
I0901 09:33:44.685489 140646288807744 basic_session_run_hooks.py:260] loss = 2.3616781, step = 36000 (34.714 sec)
I0901 09:33:58.259891 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36644
I0901 09:33:58.260750 140646288807744 basic_session_run_hooks.py:260] loss = 2.2648444, step = 36100 (13.575 sec)
I0901 09:34:11.934044 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31307
I0901 09:34:11.934696 140646288807744 basic_session_run_hooks.py:260] loss = 2.366524, step = 36200 (13.674 sec)
I0901 09:34:25.669419 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28047
I0901 09:34:25.670051 140646288807744 basic_session_run_hooks.py:260] loss = 2.5162244, step = 36300 (13.735 sec)
I0901 09:34:39.113204 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43838
I0901 09:34:39.113990 140646288807744 basic_session_run_hooks.py:260] loss = 1.9674431, step = 36400 (13.444 sec)
I0901 09:34:52.672596 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37496
I0901 09:34:52.673370 140646288807744 basic_session_run_hooks.py:260] loss = 2.5097215, step = 36500 (13.559 sec)
I0901 09:35:06.171749 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40787
I0901 09:35:06.172437 140646288807744 basic_session_run_hooks.py:260] loss = 2.3938773, step = 36600 (13.499 sec)
I0901 09:35:19.773500 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.352
I0901 09:35:19.774405 140646288807744 basic_session_run_hooks.py:260] loss = 2.547354, step = 36700 (13.602 sec)
I0901 09:35:33.272977 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40769
I0901 09:35:33.273622 140646288807744 basic_session_run_hooks.py:260] loss = 1.863082, step = 36800 (13.499 sec)
I0901 09:35:46.775918 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4058
I0901 09:35:46.776769 140646288807744 basic_session_run_hooks.py:260] loss = 2.565959, step = 36900 (13.503 sec)
I0901 09:36:00.303945 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 37000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:36:01.484487 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:36:01.623710 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.735
I0901 09:36:01.624427 140646288807744 basic_session_run_hooks.py:260] loss = 2.3159149, step = 37000 (14.848 sec)
I0901 09:36:15.128536 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40476
I0901 09:36:15.129232 140646288807744 basic_session_run_hooks.py:260] loss = 2.2321546, step = 37100 (13.505 sec)
I0901 09:36:28.968061 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22569
I0901 09:36:28.968723 140646288807744 basic_session_run_hooks.py:260] loss = 2.2362707, step = 37200 (13.839 sec)
I0901 09:36:42.591360 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34036
I0901 09:36:42.592108 140646288807744 basic_session_run_hooks.py:260] loss = 2.5315373, step = 37300 (13.623 sec)
I0901 09:36:56.236403 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32867
I0901 09:36:56.237126 140646288807744 basic_session_run_hooks.py:260] loss = 1.8992251, step = 37400 (13.645 sec)
I0901 09:37:09.693222 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43117
I0901 09:37:09.694035 140646288807744 basic_session_run_hooks.py:260] loss = 2.276155, step = 37500 (13.457 sec)
I0901 09:37:23.347055 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32395
I0901 09:37:23.347865 140646288807744 basic_session_run_hooks.py:260] loss = 2.9425912, step = 37600 (13.654 sec)
I0901 09:37:36.700220 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.48886
I0901 09:37:36.700861 140646288807744 basic_session_run_hooks.py:260] loss = 2.3840308, step = 37700 (13.353 sec)
I0901 09:37:50.314624 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34516
I0901 09:37:50.315351 140646288807744 basic_session_run_hooks.py:260] loss = 2.7549314, step = 37800 (13.614 sec)
I0901 09:38:04.086237 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26131
I0901 09:38:04.087057 140646288807744 basic_session_run_hooks.py:260] loss = 2.5369294, step = 37900 (13.772 sec)
I0901 09:38:17.409888 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 38000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:38:18.583237 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:38:18.723661 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.8318
I0901 09:38:18.724406 140646288807744 basic_session_run_hooks.py:260] loss = 2.2217429, step = 38000 (14.637 sec)
I0901 09:38:32.357408 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33474
I0901 09:38:32.358065 140646288807744 basic_session_run_hooks.py:260] loss = 2.0991113, step = 38100 (13.634 sec)
I0901 09:38:45.949218 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35737
I0901 09:38:45.949836 140646288807744 basic_session_run_hooks.py:260] loss = 2.2380457, step = 38200 (13.592 sec)
I0901 09:38:59.551465 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35173
I0901 09:38:59.552176 140646288807744 basic_session_run_hooks.py:260] loss = 2.3026729, step = 38300 (13.602 sec)
I0901 09:39:12.999171 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43621
I0901 09:39:12.999870 140646288807744 basic_session_run_hooks.py:260] loss = 2.788339, step = 38400 (13.448 sec)
I0901 09:39:26.684312 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3072
I0901 09:39:26.684936 140646288807744 basic_session_run_hooks.py:260] loss = 2.5142605, step = 38500 (13.685 sec)
I0901 09:39:40.090440 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.45927
I0901 09:39:40.091104 140646288807744 basic_session_run_hooks.py:260] loss = 1.8879462, step = 38600 (13.406 sec)
I0901 09:39:53.695363 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35028
I0901 09:39:53.696208 140646288807744 basic_session_run_hooks.py:260] loss = 2.244388, step = 38700 (13.605 sec)
I0901 09:40:07.399432 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.2971
I0901 09:40:07.400301 140646288807744 basic_session_run_hooks.py:260] loss = 2.359595, step = 38800 (13.704 sec)
I0901 09:40:21.058241 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32128
I0901 09:40:21.058919 140646288807744 basic_session_run_hooks.py:260] loss = 2.3933563, step = 38900 (13.659 sec)
I0901 09:40:34.467969 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 39000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:40:35.641725 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:40:35.783029 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.79127
I0901 09:40:35.783841 140646288807744 basic_session_run_hooks.py:260] loss = 2.5603085, step = 39000 (14.725 sec)
I0901 09:40:49.505741 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28719
I0901 09:40:49.506439 140646288807744 basic_session_run_hooks.py:260] loss = 2.1521454, step = 39100 (13.723 sec)
I0901 09:41:03.052298 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38195
I0901 09:41:03.053014 140646288807744 basic_session_run_hooks.py:260] loss = 2.3313766, step = 39200 (13.547 sec)
I0901 09:41:16.691657 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33173
I0901 09:41:16.692356 140646288807744 basic_session_run_hooks.py:260] loss = 2.4012816, step = 39300 (13.639 sec)
I0901 09:41:30.266513 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36656
I0901 09:41:30.267333 140646288807744 basic_session_run_hooks.py:260] loss = 2.1903374, step = 39400 (13.575 sec)
I0901 09:41:43.810544 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38333
I0901 09:41:43.811230 140646288807744 basic_session_run_hooks.py:260] loss = 1.9819996, step = 39500 (13.544 sec)
I0901 09:41:57.321918 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40117
I0901 09:41:57.322595 140646288807744 basic_session_run_hooks.py:260] loss = 2.2499852, step = 39600 (13.511 sec)
I0901 09:42:10.840560 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39719
I0901 09:42:10.841372 140646288807744 basic_session_run_hooks.py:260] loss = 2.3203044, step = 39700 (13.519 sec)
I0901 09:42:24.301032 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42916
I0901 09:42:24.301756 140646288807744 basic_session_run_hooks.py:260] loss = 2.1664402, step = 39800 (13.460 sec)
I0901 09:42:38.092586 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25081
I0901 09:42:38.093235 140646288807744 basic_session_run_hooks.py:260] loss = 2.5889213, step = 39900 (13.791 sec)
I0901 09:42:51.808998 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 40000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:42:52.986743 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:42:53.119118 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.6549
I0901 09:42:53.119819 140646288807744 basic_session_run_hooks.py:260] loss = 2.0131474, step = 40000 (15.027 sec)
I0901 09:43:06.733227 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34533
I0901 09:43:06.733884 140646288807744 basic_session_run_hooks.py:260] loss = 1.660566, step = 40100 (13.614 sec)
I0901 09:43:20.228438 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41004
I0901 09:43:20.229211 140646288807744 basic_session_run_hooks.py:260] loss = 2.3843293, step = 40200 (13.495 sec)
I0901 09:43:33.715728 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41439
I0901 09:43:33.716450 140646288807744 basic_session_run_hooks.py:260] loss = 2.0993922, step = 40300 (13.487 sec)
I0901 09:43:47.341130 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33923
I0901 09:43:47.341960 140646288807744 basic_session_run_hooks.py:260] loss = 2.295903, step = 40400 (13.626 sec)
I0901 09:44:00.762260 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.45094
I0901 09:44:00.763041 140646288807744 basic_session_run_hooks.py:260] loss = 2.3292377, step = 40500 (13.421 sec)
I0901 09:44:14.688308 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.18079
I0901 09:44:14.688980 140646288807744 basic_session_run_hooks.py:260] loss = 2.2106745, step = 40600 (13.926 sec)
I0901 09:44:28.432947 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27557
I0901 09:44:28.433759 140646288807744 basic_session_run_hooks.py:260] loss = 2.0681612, step = 40700 (13.745 sec)
I0901 09:44:42.250911 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23696
I0901 09:44:42.251702 140646288807744 basic_session_run_hooks.py:260] loss = 2.0588272, step = 40800 (13.818 sec)
I0901 09:44:55.980119 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28374
I0901 09:44:55.981009 140646288807744 basic_session_run_hooks.py:260] loss = 2.0779214, step = 40900 (13.729 sec)
I0901 09:45:09.538367 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 41000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:45:10.712618 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 09:45:10.713791 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 09:45:10.824792 140646288807744 estimator.py:1145] Calling model_fn.
I0901 09:45:10.825318 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 09:45:10.825532 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 09:45:10.825586 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 09:45:10.825632 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 09:45:10.825671 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 09:45:10.825715 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 09:45:10.825752 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 09:45:10.879189 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 09:45:10.926120 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 09:45:11.007841 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 09:45:11.015497 140646288807744 t2t_model.py:2248] Building model body
I0901 09:45:14.114888 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 09:45:14.542116 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 09:45:14.553179 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T09:45:14Z
I0901 09:45:14.904803 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 09:45:14.905260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:45:14.905539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 09:45:14.905579: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 09:45:14.905592: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 09:45:14.905603: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 09:45:14.905614: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 09:45:14.905626: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 09:45:14.905637: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 09:45:14.905648: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 09:45:14.905748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:45:14.906003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:45:14.906264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 09:45:14.906300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 09:45:14.906308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 09:45:14.906314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 09:45:14.906402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:45:14.906616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:45:14.906820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 09:45:14.907583 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-41000
I0901 09:45:15.523684 140646288807744 session_manager.py:500] Running local_init_op.
I0901 09:45:15.599023 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 09:45:18.443326 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 09:45:19.723506 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 09:45:21.019839 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 09:45:22.338661 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 09:45:23.630880 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 09:45:24.840809 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 09:45:26.240623 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 09:45:27.445191 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 09:45:28.871237 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 09:45:30.066368 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 09:45:30.217260 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-09:45:30
I0901 09:45:30.217455 140646288807744 estimator.py:2039] Saving dict for global step 41000: global_step = 41000, loss = 2.3946898, metrics-translate_ende_wmt8k/targets/accuracy = 0.56645215, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.75155187, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.26776826, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.3948505, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.3418846, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.54863137
I0901 09:45:30.217936 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 41000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-41000
I0901 09:45:30.349681 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.90955
I0901 09:45:30.350376 140646288807744 basic_session_run_hooks.py:260] loss = 2.1292038, step = 41000 (34.369 sec)
I0901 09:45:43.899384 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38024
I0901 09:45:43.900239 140646288807744 basic_session_run_hooks.py:260] loss = 2.0910013, step = 41100 (13.550 sec)
I0901 09:45:57.468153 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36986
I0901 09:45:57.468800 140646288807744 basic_session_run_hooks.py:260] loss = 2.6135685, step = 41200 (13.569 sec)
I0901 09:46:10.924635 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43136
I0901 09:46:10.925468 140646288807744 basic_session_run_hooks.py:260] loss = 2.216306, step = 41300 (13.457 sec)
I0901 09:46:24.443260 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3972
I0901 09:46:24.444152 140646288807744 basic_session_run_hooks.py:260] loss = 2.2287853, step = 41400 (13.519 sec)
I0901 09:46:38.216437 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26049
I0901 09:46:38.217126 140646288807744 basic_session_run_hooks.py:260] loss = 2.6361194, step = 41500 (13.773 sec)
2019-09-01 09:46:44.867870: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:44.868356: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:44.941419: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:44.941888: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:44.975361: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:44.976042: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:44.977585: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:44.978220: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:45.041236: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:45.041903: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:45.043751: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:46:45.044392: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:46:51.775777 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37499
I0901 09:46:51.776692 140646288807744 basic_session_run_hooks.py:260] loss = 2.2906775, step = 41600 (13.560 sec)
I0901 09:47:05.514514 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27869
I0901 09:47:05.515147 140646288807744 basic_session_run_hooks.py:260] loss = 2.131792, step = 41700 (13.738 sec)
I0901 09:47:18.968118 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43295
I0901 09:47:18.968981 140646288807744 basic_session_run_hooks.py:260] loss = 2.1906352, step = 41800 (13.454 sec)
2019-09-01 09:47:21.709076: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:47:21.709881: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:47:21.847823: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:47:21.848283: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:47:21.849262: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:47:21.849697: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:47:32.770063 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24536
I0901 09:47:32.771037 140646288807744 basic_session_run_hooks.py:260] loss = 2.5405743, step = 41900 (13.802 sec)
I0901 09:47:46.262853 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 42000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:47:47.450647 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:47:47.652517 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.71932
I0901 09:47:47.653367 140646288807744 basic_session_run_hooks.py:260] loss = 2.8822014, step = 42000 (14.882 sec)
I0901 09:48:01.149708 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40895
I0901 09:48:01.150608 140646288807744 basic_session_run_hooks.py:260] loss = 1.9751675, step = 42100 (13.497 sec)
I0901 09:48:14.940426 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25125
I0901 09:48:14.941210 140646288807744 basic_session_run_hooks.py:260] loss = 2.4273553, step = 42200 (13.791 sec)
I0901 09:48:28.613174 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31382
I0901 09:48:28.613935 140646288807744 basic_session_run_hooks.py:260] loss = 2.6776857, step = 42300 (13.673 sec)
I0901 09:48:42.063408 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43481
I0901 09:48:42.064203 140646288807744 basic_session_run_hooks.py:260] loss = 2.7951992, step = 42400 (13.450 sec)
I0901 09:48:55.644982 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36292
I0901 09:48:55.645642 140646288807744 basic_session_run_hooks.py:260] loss = 2.1872551, step = 42500 (13.581 sec)
I0901 09:49:09.276247 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33608
I0901 09:49:09.277057 140646288807744 basic_session_run_hooks.py:260] loss = 2.4015691, step = 42600 (13.631 sec)
I0901 09:49:22.953589 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31136
I0901 09:49:22.954318 140646288807744 basic_session_run_hooks.py:260] loss = 2.2127244, step = 42700 (13.677 sec)
I0901 09:49:36.533961 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36357
I0901 09:49:36.534798 140646288807744 basic_session_run_hooks.py:260] loss = 2.2207892, step = 42800 (13.580 sec)
I0901 09:49:50.089226 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37721
I0901 09:49:50.090007 140646288807744 basic_session_run_hooks.py:260] loss = 3.0639524, step = 42900 (13.555 sec)
I0901 09:50:03.597672 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 43000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:50:04.767333 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:50:04.911455 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.74662
I0901 09:50:04.912223 140646288807744 basic_session_run_hooks.py:260] loss = 2.3564248, step = 43000 (14.822 sec)
I0901 09:50:18.432783 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39572
I0901 09:50:18.433631 140646288807744 basic_session_run_hooks.py:260] loss = 2.1687484, step = 43100 (13.521 sec)
I0901 09:50:31.932300 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40767
I0901 09:50:31.933147 140646288807744 basic_session_run_hooks.py:260] loss = 2.4391093, step = 43200 (13.500 sec)
I0901 09:50:45.644036 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29303
I0901 09:50:45.644896 140646288807744 basic_session_run_hooks.py:260] loss = 1.8568219, step = 43300 (13.712 sec)
I0901 09:50:59.090442 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43693
I0901 09:50:59.091078 140646288807744 basic_session_run_hooks.py:260] loss = 2.0842416, step = 43400 (13.446 sec)
I0901 09:51:12.565261 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42125
I0901 09:51:12.566005 140646288807744 basic_session_run_hooks.py:260] loss = 1.8030833, step = 43500 (13.475 sec)
I0901 09:51:26.096658 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39022
I0901 09:51:26.097450 140646288807744 basic_session_run_hooks.py:260] loss = 2.2697532, step = 43600 (13.531 sec)
I0901 09:51:39.493050 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4647
I0901 09:51:39.493993 140646288807744 basic_session_run_hooks.py:260] loss = 2.4186668, step = 43700 (13.397 sec)
I0901 09:51:52.967005 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42173
I0901 09:51:52.967681 140646288807744 basic_session_run_hooks.py:260] loss = 2.3533614, step = 43800 (13.474 sec)
I0901 09:52:06.555448 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35919
I0901 09:52:06.556259 140646288807744 basic_session_run_hooks.py:260] loss = 2.0251963, step = 43900 (13.589 sec)
I0901 09:52:20.069418 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 44000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:52:21.235589 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:52:21.374763 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.74795
I0901 09:52:21.375421 140646288807744 basic_session_run_hooks.py:260] loss = 1.7584999, step = 44000 (14.819 sec)
I0901 09:52:35.017732 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32979
I0901 09:52:35.018558 140646288807744 basic_session_run_hooks.py:260] loss = 1.9688082, step = 44100 (13.643 sec)
I0901 09:52:48.547088 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39133
I0901 09:52:48.547924 140646288807744 basic_session_run_hooks.py:260] loss = 2.9623506, step = 44200 (13.529 sec)
I0901 09:53:01.914843 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.48069
I0901 09:53:01.915479 140646288807744 basic_session_run_hooks.py:260] loss = 1.8722781, step = 44300 (13.368 sec)
I0901 09:53:15.579249 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31828
I0901 09:53:15.579880 140646288807744 basic_session_run_hooks.py:260] loss = 2.2016284, step = 44400 (13.664 sec)
I0901 09:53:29.150304 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36862
I0901 09:53:29.151038 140646288807744 basic_session_run_hooks.py:260] loss = 2.1542165, step = 44500 (13.571 sec)
I0901 09:53:42.636260 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41512
I0901 09:53:42.637093 140646288807744 basic_session_run_hooks.py:260] loss = 2.15054, step = 44600 (13.486 sec)
I0901 09:53:56.197662 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37387
I0901 09:53:56.198518 140646288807744 basic_session_run_hooks.py:260] loss = 1.8642445, step = 44700 (13.561 sec)
I0901 09:54:09.837533 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33144
I0901 09:54:09.838232 140646288807744 basic_session_run_hooks.py:260] loss = 2.373455, step = 44800 (13.640 sec)
I0901 09:54:23.415495 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36488
I0901 09:54:23.416217 140646288807744 basic_session_run_hooks.py:260] loss = 2.8139954, step = 44900 (13.578 sec)
2019-09-01 09:54:31.632487: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:54:31.664205: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:54:31.789013: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:54:31.789467: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:54:31.790445: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:54:31.790879: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:54:37.284472 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 45000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:54:38.471060 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:54:38.603973 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.58394
I0901 09:54:38.604821 140646288807744 basic_session_run_hooks.py:260] loss = 2.3279686, step = 45000 (15.189 sec)
I0901 09:54:52.539899 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.1757
I0901 09:54:52.540853 140646288807744 basic_session_run_hooks.py:260] loss = 2.0612345, step = 45100 (13.936 sec)
I0901 09:55:06.286602 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27447
I0901 09:55:06.287384 140646288807744 basic_session_run_hooks.py:260] loss = 2.0675905, step = 45200 (13.747 sec)
I0901 09:55:19.797977 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40117
I0901 09:55:19.798635 140646288807744 basic_session_run_hooks.py:260] loss = 1.9507701, step = 45300 (13.511 sec)
I0901 09:55:33.361402 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37277
I0901 09:55:33.362036 140646288807744 basic_session_run_hooks.py:260] loss = 1.9437206, step = 45400 (13.563 sec)
I0901 09:55:47.030320 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31587
I0901 09:55:47.031073 140646288807744 basic_session_run_hooks.py:260] loss = 2.4267235, step = 45500 (13.669 sec)
I0901 09:56:00.418645 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.46919
I0901 09:56:00.419311 140646288807744 basic_session_run_hooks.py:260] loss = 2.1467588, step = 45600 (13.388 sec)
I0901 09:56:14.047490 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33738
I0901 09:56:14.048140 140646288807744 basic_session_run_hooks.py:260] loss = 2.7014458, step = 45700 (13.629 sec)
I0901 09:56:27.833639 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25366
I0901 09:56:27.834497 140646288807744 basic_session_run_hooks.py:260] loss = 2.3503287, step = 45800 (13.786 sec)
I0901 09:56:41.360463 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39272
I0901 09:56:41.361165 140646288807744 basic_session_run_hooks.py:260] loss = 2.6186821, step = 45900 (13.527 sec)
I0901 09:56:54.805113 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 46000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:56:55.971281 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 09:56:55.972456 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 09:56:56.083349 140646288807744 estimator.py:1145] Calling model_fn.
I0901 09:56:56.083907 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 09:56:56.084153 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 09:56:56.084221 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 09:56:56.084296 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 09:56:56.084363 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 09:56:56.084436 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 09:56:56.084519 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 09:56:56.138109 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 09:56:56.186029 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 09:56:56.270695 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 09:56:56.278398 140646288807744 t2t_model.py:2248] Building model body
I0901 09:56:59.578245 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 09:56:59.994470 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 09:57:00.006161 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T09:57:00Z
I0901 09:57:00.361889 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 09:57:00.362327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:57:00.362606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 09:57:00.362674: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 09:57:00.362701: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 09:57:00.362728: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 09:57:00.362740: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 09:57:00.362752: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 09:57:00.362776: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 09:57:00.362806: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 09:57:00.362850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:57:00.363077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:57:00.363289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 09:57:00.363326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 09:57:00.363333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 09:57:00.363338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 09:57:00.363429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:57:00.363646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 09:57:00.363890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 09:57:00.364612 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-46000
I0901 09:57:00.978785 140646288807744 session_manager.py:500] Running local_init_op.
I0901 09:57:01.055065 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 09:57:03.917862 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 09:57:05.118931 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 09:57:06.373854 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 09:57:07.616553 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 09:57:08.966154 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 09:57:10.161833 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 09:57:11.241219 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 09:57:12.586392 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 09:57:13.874747 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 09:57:15.131128 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 09:57:15.285528 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-09:57:15
I0901 09:57:15.285728 140646288807744 estimator.py:2039] Saving dict for global step 46000: global_step = 46000, loss = 2.3400424, metrics-translate_ende_wmt8k/targets/accuracy = 0.5742059, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.7588281, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.27597722, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.3402953, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.35041, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.5557291
I0901 09:57:15.286128 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 46000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-46000
I0901 09:57:15.422913 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.93578
I0901 09:57:15.423904 140646288807744 basic_session_run_hooks.py:260] loss = 2.6802065, step = 46000 (34.063 sec)
I0901 09:57:29.206816 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25484
I0901 09:57:29.207484 140646288807744 basic_session_run_hooks.py:260] loss = 2.3622925, step = 46100 (13.784 sec)
I0901 09:57:42.784314 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36513
I0901 09:57:42.785103 140646288807744 basic_session_run_hooks.py:260] loss = 2.5119216, step = 46200 (13.578 sec)
I0901 09:57:56.209358 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.44876
I0901 09:57:56.210186 140646288807744 basic_session_run_hooks.py:260] loss = 2.7726698, step = 46300 (13.425 sec)
I0901 09:58:09.708156 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40807
I0901 09:58:09.708926 140646288807744 basic_session_run_hooks.py:260] loss = 2.5798123, step = 46400 (13.499 sec)
I0901 09:58:23.215771 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40323
I0901 09:58:23.216506 140646288807744 basic_session_run_hooks.py:260] loss = 2.2602787, step = 46500 (13.508 sec)
I0901 09:58:36.751202 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38802
I0901 09:58:36.751926 140646288807744 basic_session_run_hooks.py:260] loss = 3.142383, step = 46600 (13.535 sec)
I0901 09:58:50.242673 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41209
I0901 09:58:50.243471 140646288807744 basic_session_run_hooks.py:260] loss = 2.0530446, step = 46700 (13.492 sec)
2019-09-01 09:58:52.377703: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.378180: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.431907: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.432402: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.460855: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.461303: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.462284: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.462717: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.509013: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.509508: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.510492: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 09:58:52.510924: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 09:59:04.006011 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26568
I0901 09:59:04.006683 140646288807744 basic_session_run_hooks.py:260] loss = 2.1881173, step = 46800 (13.763 sec)
I0901 09:59:17.498453 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41156
I0901 09:59:17.499219 140646288807744 basic_session_run_hooks.py:260] loss = 2.415028, step = 46900 (13.493 sec)
I0901 09:59:31.028380 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 47000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 09:59:32.197836 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 09:59:32.331082 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.74189
I0901 09:59:32.331711 140646288807744 basic_session_run_hooks.py:260] loss = 1.9513978, step = 47000 (14.832 sec)
I0901 09:59:45.885154 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37786
I0901 09:59:45.886059 140646288807744 basic_session_run_hooks.py:260] loss = 2.6831803, step = 47100 (13.554 sec)
I0901 09:59:59.687988 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24489
I0901 09:59:59.688819 140646288807744 basic_session_run_hooks.py:260] loss = 2.177754, step = 47200 (13.803 sec)
I0901 10:00:13.298176 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34744
I0901 10:00:13.299279 140646288807744 basic_session_run_hooks.py:260] loss = 3.0057688, step = 47300 (13.610 sec)
I0901 10:00:26.771361 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42215
I0901 10:00:26.772156 140646288807744 basic_session_run_hooks.py:260] loss = 1.9477398, step = 47400 (13.473 sec)
I0901 10:00:40.532449 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26687
I0901 10:00:40.533350 140646288807744 basic_session_run_hooks.py:260] loss = 2.5731266, step = 47500 (13.761 sec)
I0901 10:00:54.050801 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39735
I0901 10:00:54.051549 140646288807744 basic_session_run_hooks.py:260] loss = 1.9593877, step = 47600 (13.518 sec)
I0901 10:01:07.530598 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41851
I0901 10:01:07.531322 140646288807744 basic_session_run_hooks.py:260] loss = 2.0402014, step = 47700 (13.480 sec)
I0901 10:01:21.045197 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3994
I0901 10:01:21.045846 140646288807744 basic_session_run_hooks.py:260] loss = 2.092904, step = 47800 (13.515 sec)
I0901 10:01:34.448343 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.46094
I0901 10:01:34.449145 140646288807744 basic_session_run_hooks.py:260] loss = 1.7430996, step = 47900 (13.403 sec)
I0901 10:01:47.853759 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 48000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:01:49.025743 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:01:49.157851 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.79832
I0901 10:01:49.158574 140646288807744 basic_session_run_hooks.py:260] loss = 2.5422838, step = 48000 (14.709 sec)
I0901 10:02:02.667839 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40193
I0901 10:02:02.668860 140646288807744 basic_session_run_hooks.py:260] loss = 2.4169705, step = 48100 (13.510 sec)
I0901 10:02:16.654446 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.1497
I0901 10:02:16.655402 140646288807744 basic_session_run_hooks.py:260] loss = 2.2020571, step = 48200 (13.987 sec)
I0901 10:02:30.144468 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41288
I0901 10:02:30.145119 140646288807744 basic_session_run_hooks.py:260] loss = 1.9169501, step = 48300 (13.490 sec)
2019-09-01 10:02:38.752762: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.753230: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.846744: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.847245: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.884210: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.884781: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.886665: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.887215: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.957032: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.957480: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.959155: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:02:38.959589: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 10:02:44.209842 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.10966
I0901 10:02:44.210670 140646288807744 basic_session_run_hooks.py:260] loss = 2.7889585, step = 48400 (14.066 sec)
I0901 10:02:57.771785 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37358
I0901 10:02:57.772762 140646288807744 basic_session_run_hooks.py:260] loss = 1.8127902, step = 48500 (13.562 sec)
I0901 10:03:11.400750 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33731
I0901 10:03:11.401449 140646288807744 basic_session_run_hooks.py:260] loss = 2.546931, step = 48600 (13.629 sec)
I0901 10:03:24.888733 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41401
I0901 10:03:24.889437 140646288807744 basic_session_run_hooks.py:260] loss = 3.3098166, step = 48700 (13.488 sec)
I0901 10:03:38.600745 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29288
I0901 10:03:38.601456 140646288807744 basic_session_run_hooks.py:260] loss = 2.1930306, step = 48800 (13.712 sec)
I0901 10:03:52.232550 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33578
I0901 10:03:52.233359 140646288807744 basic_session_run_hooks.py:260] loss = 2.0055094, step = 48900 (13.632 sec)
I0901 10:04:05.567711 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 49000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:04:06.765588 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:04:06.893184 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.82099
I0901 10:04:06.893890 140646288807744 basic_session_run_hooks.py:260] loss = 2.193916, step = 49000 (14.661 sec)
I0901 10:04:20.464220 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36863
I0901 10:04:20.464904 140646288807744 basic_session_run_hooks.py:260] loss = 1.7787594, step = 49100 (13.571 sec)
I0901 10:04:34.166964 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29781
I0901 10:04:34.167684 140646288807744 basic_session_run_hooks.py:260] loss = 2.107129, step = 49200 (13.703 sec)
I0901 10:04:47.740266 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3674
I0901 10:04:47.741077 140646288807744 basic_session_run_hooks.py:260] loss = 1.8909389, step = 49300 (13.573 sec)
I0901 10:05:01.224597 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41602
I0901 10:05:01.225343 140646288807744 basic_session_run_hooks.py:260] loss = 2.3943598, step = 49400 (13.484 sec)
I0901 10:05:14.690778 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42601
I0901 10:05:14.691424 140646288807744 basic_session_run_hooks.py:260] loss = 2.514889, step = 49500 (13.466 sec)
I0901 10:05:28.256629 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37145
I0901 10:05:28.257409 140646288807744 basic_session_run_hooks.py:260] loss = 1.9704697, step = 49600 (13.566 sec)
I0901 10:05:41.774258 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39774
I0901 10:05:41.775075 140646288807744 basic_session_run_hooks.py:260] loss = 2.709554, step = 49700 (13.518 sec)
I0901 10:05:55.236836 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.428
I0901 10:05:55.237487 140646288807744 basic_session_run_hooks.py:260] loss = 1.9711432, step = 49800 (13.462 sec)
I0901 10:06:08.722776 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41513
I0901 10:06:08.723481 140646288807744 basic_session_run_hooks.py:260] loss = 2.2554576, step = 49900 (13.486 sec)
I0901 10:06:22.249523 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 50000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:06:23.436300 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:06:23.579380 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.73101
I0901 10:06:23.580267 140646288807744 basic_session_run_hooks.py:260] loss = 2.5699122, step = 50000 (14.857 sec)
I0901 10:06:37.145547 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37128
I0901 10:06:37.146210 140646288807744 basic_session_run_hooks.py:260] loss = 2.008385, step = 50100 (13.566 sec)
I0901 10:06:50.765511 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34217
I0901 10:06:50.766266 140646288807744 basic_session_run_hooks.py:260] loss = 2.8004005, step = 50200 (13.620 sec)
I0901 10:07:04.347162 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36287
I0901 10:07:04.347933 140646288807744 basic_session_run_hooks.py:260] loss = 2.0117402, step = 50300 (13.582 sec)
I0901 10:07:17.864275 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39803
I0901 10:07:17.865132 140646288807744 basic_session_run_hooks.py:260] loss = 2.1129239, step = 50400 (13.517 sec)
I0901 10:07:31.397587 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38917
I0901 10:07:31.398298 140646288807744 basic_session_run_hooks.py:260] loss = 3.514113, step = 50500 (13.533 sec)
I0901 10:07:45.244832 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22165
I0901 10:07:45.245633 140646288807744 basic_session_run_hooks.py:260] loss = 2.1284983, step = 50600 (13.847 sec)
I0901 10:07:58.907258 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31934
I0901 10:07:58.907900 140646288807744 basic_session_run_hooks.py:260] loss = 2.4439545, step = 50700 (13.662 sec)
I0901 10:08:12.478878 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36832
I0901 10:08:12.479585 140646288807744 basic_session_run_hooks.py:260] loss = 2.2225454, step = 50800 (13.572 sec)
I0901 10:08:26.009337 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39073
I0901 10:08:26.010008 140646288807744 basic_session_run_hooks.py:260] loss = 2.3130846, step = 50900 (13.530 sec)
I0901 10:08:39.473063 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 51000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:08:40.675630 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 10:08:40.676685 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 10:08:40.789460 140646288807744 estimator.py:1145] Calling model_fn.
I0901 10:08:40.790060 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 10:08:40.790405 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 10:08:40.790459 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 10:08:40.790520 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 10:08:40.790591 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 10:08:40.790687 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 10:08:40.790760 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 10:08:40.845527 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 10:08:40.894887 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 10:08:40.989193 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 10:08:40.997073 140646288807744 t2t_model.py:2248] Building model body
I0901 10:08:44.178153 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 10:08:44.615975 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 10:08:44.627843 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T10:08:44Z
I0901 10:08:44.983566 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 10:08:44.984049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:08:44.984304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 10:08:44.984360: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 10:08:44.984388: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 10:08:44.984414: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 10:08:44.984426: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 10:08:44.984438: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 10:08:44.984462: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 10:08:44.984491: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 10:08:44.984534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:08:44.984826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:08:44.985038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 10:08:44.985076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 10:08:44.985083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 10:08:44.985088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 10:08:44.985172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:08:44.985389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:08:44.985642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 10:08:44.986381 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-51000
I0901 10:08:45.606474 140646288807744 session_manager.py:500] Running local_init_op.
I0901 10:08:45.682300 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 10:08:48.741006 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 10:08:50.022355 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 10:08:51.304571 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 10:08:52.504040 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 10:08:53.776706 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 10:08:55.005366 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 10:08:56.278320 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 10:08:57.677298 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 10:08:58.996749 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 10:09:00.271825 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 10:09:00.421607 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-10:09:00
I0901 10:09:00.421774 140646288807744 estimator.py:2039] Saving dict for global step 51000: global_step = 51000, loss = 2.2941391, metrics-translate_ende_wmt8k/targets/accuracy = 0.57991314, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.76326203, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.28214273, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.2943249, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.35630295, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.56114024
I0901 10:09:00.422101 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 51000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-51000
I0901 10:09:00.563652 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.89399
I0901 10:09:00.564345 140646288807744 basic_session_run_hooks.py:260] loss = 2.0069642, step = 51000 (34.554 sec)
I0901 10:09:14.145339 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36286
I0901 10:09:14.146100 140646288807744 basic_session_run_hooks.py:260] loss = 1.9361624, step = 51100 (13.582 sec)
I0901 10:09:27.612301 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42558
I0901 10:09:27.612948 140646288807744 basic_session_run_hooks.py:260] loss = 2.2344186, step = 51200 (13.467 sec)
I0901 10:09:41.514216 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.19325
I0901 10:09:41.514852 140646288807744 basic_session_run_hooks.py:260] loss = 2.087424, step = 51300 (13.902 sec)
I0901 10:09:55.002474 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41386
I0901 10:09:55.003141 140646288807744 basic_session_run_hooks.py:260] loss = 2.1889825, step = 51400 (13.488 sec)
I0901 10:10:08.706158 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29731
I0901 10:10:08.707121 140646288807744 basic_session_run_hooks.py:260] loss = 2.857669, step = 51500 (13.704 sec)
I0901 10:10:22.257861 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37914
I0901 10:10:22.258553 140646288807744 basic_session_run_hooks.py:260] loss = 2.0994265, step = 51600 (13.551 sec)
I0901 10:10:35.694808 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.44217
I0901 10:10:35.695536 140646288807744 basic_session_run_hooks.py:260] loss = 2.4029565, step = 51700 (13.437 sec)
I0901 10:10:49.207222 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4006
I0901 10:10:49.207865 140646288807744 basic_session_run_hooks.py:260] loss = 2.1682148, step = 51800 (13.512 sec)
I0901 10:11:02.819641 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34624
I0901 10:11:02.820264 140646288807744 basic_session_run_hooks.py:260] loss = 1.9902146, step = 51900 (13.612 sec)
I0901 10:11:16.328038 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 52000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:11:17.509099 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:11:17.638080 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.74835
I0901 10:11:17.638786 140646288807744 basic_session_run_hooks.py:260] loss = 2.7121038, step = 52000 (14.819 sec)
I0901 10:11:31.301765 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31867
I0901 10:11:31.302595 140646288807744 basic_session_run_hooks.py:260] loss = 1.9737283, step = 52100 (13.664 sec)
I0901 10:11:44.897162 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35543
I0901 10:11:44.897952 140646288807744 basic_session_run_hooks.py:260] loss = 2.4688768, step = 52200 (13.595 sec)
I0901 10:11:58.287530 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.46805
I0901 10:11:58.288356 140646288807744 basic_session_run_hooks.py:260] loss = 3.2529924, step = 52300 (13.390 sec)
I0901 10:12:11.908403 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34168
I0901 10:12:11.909096 140646288807744 basic_session_run_hooks.py:260] loss = 2.222983, step = 52400 (13.621 sec)
I0901 10:12:25.734542 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23267
I0901 10:12:25.735189 140646288807744 basic_session_run_hooks.py:260] loss = 2.1602087, step = 52500 (13.826 sec)
I0901 10:12:39.174411 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.44055
I0901 10:12:39.175064 140646288807744 basic_session_run_hooks.py:260] loss = 2.2308927, step = 52600 (13.440 sec)
I0901 10:12:52.671822 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40883
I0901 10:12:52.672662 140646288807744 basic_session_run_hooks.py:260] loss = 2.096041, step = 52700 (13.498 sec)
I0901 10:13:06.232367 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37433
I0901 10:13:06.233158 140646288807744 basic_session_run_hooks.py:260] loss = 2.4453351, step = 52800 (13.560 sec)
I0901 10:13:19.954352 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28758
I0901 10:13:19.955189 140646288807744 basic_session_run_hooks.py:260] loss = 2.9385507, step = 52900 (13.722 sec)
I0901 10:13:33.558727 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 53000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:13:34.736208 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:13:34.882441 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.69878
I0901 10:13:34.883156 140646288807744 basic_session_run_hooks.py:260] loss = 2.463691, step = 53000 (14.928 sec)
I0901 10:13:48.435722 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37829
I0901 10:13:48.436621 140646288807744 basic_session_run_hooks.py:260] loss = 1.9012612, step = 53100 (13.553 sec)
I0901 10:14:02.043978 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34848
I0901 10:14:02.044643 140646288807744 basic_session_run_hooks.py:260] loss = 1.8943211, step = 53200 (13.608 sec)
I0901 10:14:15.951332 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.19044
I0901 10:14:15.952030 140646288807744 basic_session_run_hooks.py:260] loss = 2.112903, step = 53300 (13.907 sec)
I0901 10:14:29.686329 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28067
I0901 10:14:29.687165 140646288807744 basic_session_run_hooks.py:260] loss = 1.791447, step = 53400 (13.735 sec)
I0901 10:14:43.205708 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39679
I0901 10:14:43.206391 140646288807744 basic_session_run_hooks.py:260] loss = 2.574541, step = 53500 (13.519 sec)
I0901 10:14:57.030223 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23353
I0901 10:14:57.031122 140646288807744 basic_session_run_hooks.py:260] loss = 2.6992047, step = 53600 (13.825 sec)
I0901 10:15:10.592873 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37319
I0901 10:15:10.593733 140646288807744 basic_session_run_hooks.py:260] loss = 2.6513102, step = 53700 (13.563 sec)
I0901 10:15:24.207333 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34513
I0901 10:15:24.208204 140646288807744 basic_session_run_hooks.py:260] loss = 2.160219, step = 53800 (13.614 sec)
I0901 10:15:37.665156 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43062
I0901 10:15:37.665948 140646288807744 basic_session_run_hooks.py:260] loss = 2.7704499, step = 53900 (13.458 sec)
I0901 10:15:51.035094 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 54000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:15:52.196422 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:15:52.336036 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.81622
I0901 10:15:52.336781 140646288807744 basic_session_run_hooks.py:260] loss = 2.0589094, step = 54000 (14.671 sec)
I0901 10:16:06.028836 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30311
I0901 10:16:06.029572 140646288807744 basic_session_run_hooks.py:260] loss = 1.6822014, step = 54100 (13.693 sec)
I0901 10:16:19.533279 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40497
I0901 10:16:19.534121 140646288807744 basic_session_run_hooks.py:260] loss = 2.2336664, step = 54200 (13.505 sec)
I0901 10:16:33.161771 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33757
I0901 10:16:33.162608 140646288807744 basic_session_run_hooks.py:260] loss = 2.4408822, step = 54300 (13.628 sec)
I0901 10:16:46.688930 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39253
I0901 10:16:46.689840 140646288807744 basic_session_run_hooks.py:260] loss = 2.3142104, step = 54400 (13.527 sec)
I0901 10:17:00.189004 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40737
I0901 10:17:00.189842 140646288807744 basic_session_run_hooks.py:260] loss = 3.0634007, step = 54500 (13.500 sec)
I0901 10:17:13.703982 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3992
I0901 10:17:13.704796 140646288807744 basic_session_run_hooks.py:260] loss = 1.9559611, step = 54600 (13.515 sec)
I0901 10:17:27.273181 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36963
I0901 10:17:27.273954 140646288807744 basic_session_run_hooks.py:260] loss = 1.648272, step = 54700 (13.569 sec)
I0901 10:17:40.906036 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33522
I0901 10:17:40.906894 140646288807744 basic_session_run_hooks.py:260] loss = 2.6207874, step = 54800 (13.633 sec)
I0901 10:17:54.734765 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23132
I0901 10:17:54.735635 140646288807744 basic_session_run_hooks.py:260] loss = 2.1992543, step = 54900 (13.829 sec)
I0901 10:18:08.196458 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 55000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:18:09.378170 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:18:09.525066 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.76119
I0901 10:18:09.525866 140646288807744 basic_session_run_hooks.py:260] loss = 2.2651384, step = 55000 (14.790 sec)
I0901 10:18:23.186381 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31994
I0901 10:18:23.187123 140646288807744 basic_session_run_hooks.py:260] loss = 2.2342508, step = 55100 (13.661 sec)
I0901 10:18:36.893575 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29544
I0901 10:18:36.894449 140646288807744 basic_session_run_hooks.py:260] loss = 2.219135, step = 55200 (13.707 sec)
I0901 10:18:50.397629 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40519
I0901 10:18:50.398278 140646288807744 basic_session_run_hooks.py:260] loss = 2.0110765, step = 55300 (13.504 sec)
I0901 10:19:03.821168 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4496
I0901 10:19:03.821805 140646288807744 basic_session_run_hooks.py:260] loss = 1.79217, step = 55400 (13.424 sec)
I0901 10:19:17.366117 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38283
I0901 10:19:17.366921 140646288807744 basic_session_run_hooks.py:260] loss = 2.4494848, step = 55500 (13.545 sec)
I0901 10:19:30.917721 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3792
I0901 10:19:30.918407 140646288807744 basic_session_run_hooks.py:260] loss = 2.3399909, step = 55600 (13.551 sec)
I0901 10:19:44.427681 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40195
I0901 10:19:44.428565 140646288807744 basic_session_run_hooks.py:260] loss = 2.157771, step = 55700 (13.510 sec)
I0901 10:19:58.002660 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36649
I0901 10:19:58.003430 140646288807744 basic_session_run_hooks.py:260] loss = 1.8680123, step = 55800 (13.575 sec)
I0901 10:20:11.724259 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28778
I0901 10:20:11.725048 140646288807744 basic_session_run_hooks.py:260] loss = 2.1351073, step = 55900 (13.722 sec)
I0901 10:20:25.100224 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 56000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:20:26.279802 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 10:20:26.280742 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 10:20:26.392263 140646288807744 estimator.py:1145] Calling model_fn.
I0901 10:20:26.392819 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 10:20:26.393089 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 10:20:26.393159 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 10:20:26.393234 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 10:20:26.393288 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 10:20:26.393378 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 10:20:26.393442 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 10:20:26.446948 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 10:20:26.495042 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 10:20:26.579065 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 10:20:26.586687 140646288807744 t2t_model.py:2248] Building model body
I0901 10:20:29.923414 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 10:20:30.344730 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 10:20:30.356082 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T10:20:30Z
I0901 10:20:30.716828 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 10:20:30.717306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:20:30.717584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 10:20:30.717661: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 10:20:30.717677: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 10:20:30.717689: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 10:20:30.717700: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 10:20:30.717712: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 10:20:30.717724: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 10:20:30.717736: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 10:20:30.717789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:20:30.718044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:20:30.718231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 10:20:30.718254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 10:20:30.718261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 10:20:30.718267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 10:20:30.718319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:20:30.718545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:20:30.718817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 10:20:30.719590 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-56000
I0901 10:20:31.334276 140646288807744 session_manager.py:500] Running local_init_op.
I0901 10:20:31.409229 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 10:20:34.297109 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 10:20:35.559406 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 10:20:36.885785 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 10:20:38.656748 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 10:20:39.832934 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 10:20:41.112690 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 10:20:42.313358 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 10:20:43.465695 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 10:20:44.742743 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 10:20:46.043050 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 10:20:46.192370 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-10:20:46
I0901 10:20:46.192540 140646288807744 estimator.py:2039] Saving dict for global step 56000: global_step = 56000, loss = 2.2491605, metrics-translate_ende_wmt8k/targets/accuracy = 0.5867573, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.76886696, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.29016694, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.2492929, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.36507946, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.5674409
I0901 10:20:46.192892 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 56000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-56000
I0901 10:20:46.330730 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.88963
I0901 10:20:46.331535 140646288807744 basic_session_run_hooks.py:260] loss = 2.4287722, step = 56000 (34.606 sec)
I0901 10:20:59.800223 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42418
I0901 10:20:59.800978 140646288807744 basic_session_run_hooks.py:260] loss = 2.2318583, step = 56100 (13.469 sec)
I0901 10:21:13.335328 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3882
I0901 10:21:13.335957 140646288807744 basic_session_run_hooks.py:260] loss = 1.977562, step = 56200 (13.535 sec)
I0901 10:21:26.894971 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37483
I0901 10:21:26.895725 140646288807744 basic_session_run_hooks.py:260] loss = 2.2369905, step = 56300 (13.560 sec)
I0901 10:21:40.361448 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42585
I0901 10:21:40.362268 140646288807744 basic_session_run_hooks.py:260] loss = 2.072766, step = 56400 (13.467 sec)
I0901 10:21:54.074179 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29249
I0901 10:21:54.074811 140646288807744 basic_session_run_hooks.py:260] loss = 2.178814, step = 56500 (13.713 sec)
I0901 10:22:07.518177 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43826
I0901 10:22:07.519006 140646288807744 basic_session_run_hooks.py:260] loss = 2.0562835, step = 56600 (13.444 sec)
I0901 10:22:21.052709 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38851
I0901 10:22:21.053653 140646288807744 basic_session_run_hooks.py:260] loss = 2.265708, step = 56700 (13.535 sec)
I0901 10:22:34.717316 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31817
I0901 10:22:34.718018 140646288807744 basic_session_run_hooks.py:260] loss = 3.0570679, step = 56800 (13.664 sec)
I0901 10:22:48.434443 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29016
I0901 10:22:48.435158 140646288807744 basic_session_run_hooks.py:260] loss = 1.5823146, step = 56900 (13.717 sec)
I0901 10:23:01.778753 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 57000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:23:02.950122 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:23:03.095079 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.82099
I0901 10:23:03.095777 140646288807744 basic_session_run_hooks.py:260] loss = 2.222446, step = 57000 (14.661 sec)
I0901 10:23:16.694267 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35338
I0901 10:23:16.694935 140646288807744 basic_session_run_hooks.py:260] loss = 2.5590396, step = 57100 (13.599 sec)
I0901 10:23:30.262276 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37028
I0901 10:23:30.262989 140646288807744 basic_session_run_hooks.py:260] loss = 2.3256874, step = 57200 (13.568 sec)
I0901 10:23:44.030155 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26328
I0901 10:23:44.030891 140646288807744 basic_session_run_hooks.py:260] loss = 2.3600943, step = 57300 (13.768 sec)
I0901 10:23:57.579933 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3802
I0901 10:23:57.580926 140646288807744 basic_session_run_hooks.py:260] loss = 2.2718303, step = 57400 (13.550 sec)
I0901 10:24:11.133409 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37818
I0901 10:24:11.134063 140646288807744 basic_session_run_hooks.py:260] loss = 1.9243509, step = 57500 (13.553 sec)
I0901 10:24:24.797976 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3182
I0901 10:24:24.798885 140646288807744 basic_session_run_hooks.py:260] loss = 2.2275312, step = 57600 (13.665 sec)
I0901 10:24:38.375886 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3649
I0901 10:24:38.376705 140646288807744 basic_session_run_hooks.py:260] loss = 2.2662718, step = 57700 (13.578 sec)
I0901 10:24:51.958796 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36219
I0901 10:24:51.959511 140646288807744 basic_session_run_hooks.py:260] loss = 1.9194757, step = 57800 (13.583 sec)
I0901 10:25:05.774679 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23804
I0901 10:25:05.775317 140646288807744 basic_session_run_hooks.py:260] loss = 1.8194548, step = 57900 (13.816 sec)
I0901 10:25:19.144528 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 58000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:25:20.399675 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:25:20.537573 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.77374
I0901 10:25:20.538348 140646288807744 basic_session_run_hooks.py:260] loss = 1.8985338, step = 58000 (14.763 sec)
I0901 10:25:34.070721 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38927
I0901 10:25:34.071515 140646288807744 basic_session_run_hooks.py:260] loss = 2.4061065, step = 58100 (13.533 sec)
I0901 10:25:47.720210 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32628
I0901 10:25:47.721061 140646288807744 basic_session_run_hooks.py:260] loss = 2.269766, step = 58200 (13.650 sec)
I0901 10:26:01.203527 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41657
I0901 10:26:01.204290 140646288807744 basic_session_run_hooks.py:260] loss = 2.0834603, step = 58300 (13.483 sec)
I0901 10:26:14.737273 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38894
I0901 10:26:14.738039 140646288807744 basic_session_run_hooks.py:260] loss = 2.6423755, step = 58400 (13.534 sec)
I0901 10:26:28.359012 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34121
I0901 10:26:28.359721 140646288807744 basic_session_run_hooks.py:260] loss = 2.508699, step = 58500 (13.622 sec)
I0901 10:26:41.923295 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3723
I0901 10:26:41.924026 140646288807744 basic_session_run_hooks.py:260] loss = 2.3323479, step = 58600 (13.564 sec)
I0901 10:26:55.526654 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35113
I0901 10:26:55.527308 140646288807744 basic_session_run_hooks.py:260] loss = 1.9958525, step = 58700 (13.603 sec)
I0901 10:27:09.037065 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4017
I0901 10:27:09.037756 140646288807744 basic_session_run_hooks.py:260] loss = 2.226936, step = 58800 (13.510 sec)
I0901 10:27:22.579878 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38399
I0901 10:27:22.580892 140646288807744 basic_session_run_hooks.py:260] loss = 2.2674923, step = 58900 (13.543 sec)
I0901 10:27:35.958175 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 59000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:27:37.135708 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:27:37.279299 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.80299
I0901 10:27:37.280068 140646288807744 basic_session_run_hooks.py:260] loss = 2.1044664, step = 59000 (14.699 sec)
I0901 10:27:51.084617 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24359
I0901 10:27:51.085332 140646288807744 basic_session_run_hooks.py:260] loss = 1.9649495, step = 59100 (13.805 sec)
I0901 10:28:04.726547 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33034
I0901 10:28:04.727392 140646288807744 basic_session_run_hooks.py:260] loss = 2.3989613, step = 59200 (13.642 sec)
I0901 10:28:18.345851 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34252
I0901 10:28:18.346513 140646288807744 basic_session_run_hooks.py:260] loss = 2.0189822, step = 59300 (13.619 sec)
I0901 10:28:31.807745 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42838
I0901 10:28:31.808478 140646288807744 basic_session_run_hooks.py:260] loss = 1.7871945, step = 59400 (13.462 sec)
I0901 10:28:45.359501 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37912
I0901 10:28:45.360325 140646288807744 basic_session_run_hooks.py:260] loss = 1.6664555, step = 59500 (13.552 sec)
I0901 10:28:58.828084 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42469
I0901 10:28:58.828776 140646288807744 basic_session_run_hooks.py:260] loss = 1.8419236, step = 59600 (13.468 sec)
I0901 10:29:12.572127 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27587
I0901 10:29:12.572867 140646288807744 basic_session_run_hooks.py:260] loss = 2.2386892, step = 59700 (13.744 sec)
I0901 10:29:26.235960 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31859
I0901 10:29:26.236568 140646288807744 basic_session_run_hooks.py:260] loss = 2.3223395, step = 59800 (13.664 sec)
I0901 10:29:40.010902 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25956
I0901 10:29:40.011620 140646288807744 basic_session_run_hooks.py:260] loss = 2.8427844, step = 59900 (13.775 sec)
I0901 10:29:53.535409 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 60000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:29:54.709936 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:29:54.851053 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.73848
I0901 10:29:54.851731 140646288807744 basic_session_run_hooks.py:260] loss = 1.8381838, step = 60000 (14.840 sec)
I0901 10:30:08.714420 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.21325
I0901 10:30:08.715180 140646288807744 basic_session_run_hooks.py:260] loss = 2.5633473, step = 60100 (13.863 sec)
I0901 10:30:24.439976 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.35908
I0901 10:30:24.440673 140646288807744 basic_session_run_hooks.py:260] loss = 1.9110307, step = 60200 (15.725 sec)
I0901 10:30:38.193305 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27097
I0901 10:30:38.194063 140646288807744 basic_session_run_hooks.py:260] loss = 1.735921, step = 60300 (13.753 sec)
I0901 10:30:51.742083 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38074
I0901 10:30:51.742922 140646288807744 basic_session_run_hooks.py:260] loss = 2.6302752, step = 60400 (13.549 sec)
I0901 10:31:05.328919 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36006
I0901 10:31:05.329682 140646288807744 basic_session_run_hooks.py:260] loss = 2.146425, step = 60500 (13.587 sec)
I0901 10:31:19.071302 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27676
I0901 10:31:19.071940 140646288807744 basic_session_run_hooks.py:260] loss = 2.042731, step = 60600 (13.742 sec)
I0901 10:31:32.694243 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34056
I0901 10:31:32.695121 140646288807744 basic_session_run_hooks.py:260] loss = 1.7331078, step = 60700 (13.623 sec)
I0901 10:31:46.533411 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22587
I0901 10:31:46.534140 140646288807744 basic_session_run_hooks.py:260] loss = 2.347972, step = 60800 (13.839 sec)
I0901 10:32:00.266915 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28147
I0901 10:32:00.267751 140646288807744 basic_session_run_hooks.py:260] loss = 1.5864687, step = 60900 (13.734 sec)
I0901 10:32:13.727151 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 61000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:32:14.940198 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 10:32:14.941299 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 10:32:15.053849 140646288807744 estimator.py:1145] Calling model_fn.
I0901 10:32:15.054653 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 10:32:15.054985 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 10:32:15.055066 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 10:32:15.055142 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 10:32:15.055235 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 10:32:15.055310 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 10:32:15.055392 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 10:32:15.109564 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 10:32:15.157792 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 10:32:15.241236 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 10:32:15.249057 140646288807744 t2t_model.py:2248] Building model body
I0901 10:32:18.372266 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 10:32:18.787388 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 10:32:18.798931 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T10:32:18Z
I0901 10:32:19.150912 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 10:32:19.151401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:32:19.151755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 10:32:19.151832: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 10:32:19.151862: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 10:32:19.151879: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 10:32:19.151894: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 10:32:19.151910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 10:32:19.151927: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 10:32:19.151943: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 10:32:19.152024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:32:19.152392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:32:19.152737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 10:32:19.152783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 10:32:19.152809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 10:32:19.152830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 10:32:19.152931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:32:19.153348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:32:19.153753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 10:32:19.154746 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-61000
I0901 10:32:19.770920 140646288807744 session_manager.py:500] Running local_init_op.
I0901 10:32:19.846045 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 10:32:22.883526 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 10:32:24.225017 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 10:32:25.557910 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 10:32:26.737823 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 10:32:27.891239 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 10:32:29.240656 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 10:32:30.553661 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 10:32:31.928184 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 10:32:33.227739 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 10:32:34.463632 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 10:32:34.594453 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-10:32:34
I0901 10:32:34.594649 140646288807744 estimator.py:2039] Saving dict for global step 61000: global_step = 61000, loss = 2.21732, metrics-translate_ende_wmt8k/targets/accuracy = 0.5925783, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.77364194, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.29504526, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.217481, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.37009966, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.57236487
I0901 10:32:34.594996 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 61000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-61000
I0901 10:32:34.734140 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.90131
I0901 10:32:34.734996 140646288807744 basic_session_run_hooks.py:260] loss = 1.5376521, step = 61000 (34.467 sec)
I0901 10:32:48.293524 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37496
I0901 10:32:48.294150 140646288807744 basic_session_run_hooks.py:260] loss = 2.005343, step = 61100 (13.559 sec)
I0901 10:33:01.961622 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31631
I0901 10:33:01.962481 140646288807744 basic_session_run_hooks.py:260] loss = 2.527356, step = 61200 (13.668 sec)
I0901 10:33:15.419600 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43054
I0901 10:33:15.420282 140646288807744 basic_session_run_hooks.py:260] loss = 1.940363, step = 61300 (13.458 sec)
I0901 10:33:29.025756 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34961
I0901 10:33:29.026567 140646288807744 basic_session_run_hooks.py:260] loss = 2.8313012, step = 61400 (13.606 sec)
2019-09-01 10:33:42.249773: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.250239: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.305352: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.305821: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.334833: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.335580: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.336948: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.337584: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.385500: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.386146: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.387272: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:33:42.387905: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 10:33:42.586937 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37399
I0901 10:33:42.587560 140646288807744 basic_session_run_hooks.py:260] loss = 2.2371557, step = 61500 (13.561 sec)
I0901 10:33:56.293505 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29577
I0901 10:33:56.294116 140646288807744 basic_session_run_hooks.py:260] loss = 2.0117378, step = 61600 (13.707 sec)
I0901 10:34:09.913446 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34218
I0901 10:34:09.914189 140646288807744 basic_session_run_hooks.py:260] loss = 2.3028305, step = 61700 (13.620 sec)
I0901 10:34:23.350156 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4423
I0901 10:34:23.351072 140646288807744 basic_session_run_hooks.py:260] loss = 1.8767174, step = 61800 (13.437 sec)
I0901 10:34:37.025190 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3126
I0901 10:34:37.026169 140646288807744 basic_session_run_hooks.py:260] loss = 2.3131475, step = 61900 (13.675 sec)
I0901 10:34:50.486299 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 62000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:34:51.670555 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:34:51.809803 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.76379
I0901 10:34:51.810503 140646288807744 basic_session_run_hooks.py:260] loss = 1.8510686, step = 62000 (14.784 sec)
I0901 10:35:05.305180 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40995
I0901 10:35:05.305798 140646288807744 basic_session_run_hooks.py:260] loss = 2.1889932, step = 62100 (13.495 sec)
I0901 10:35:18.880306 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36641
I0901 10:35:18.881014 140646288807744 basic_session_run_hooks.py:260] loss = 2.4239597, step = 62200 (13.575 sec)
I0901 10:35:32.602921 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28724
I0901 10:35:32.603698 140646288807744 basic_session_run_hooks.py:260] loss = 1.8670148, step = 62300 (13.723 sec)
I0901 10:35:46.174445 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36837
I0901 10:35:46.175169 140646288807744 basic_session_run_hooks.py:260] loss = 1.99316, step = 62400 (13.571 sec)
I0901 10:35:59.723075 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38082
I0901 10:35:59.723888 140646288807744 basic_session_run_hooks.py:260] loss = 2.0888677, step = 62500 (13.549 sec)
I0901 10:36:13.355263 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33558
I0901 10:36:13.355952 140646288807744 basic_session_run_hooks.py:260] loss = 1.8403509, step = 62600 (13.632 sec)
I0901 10:36:27.015344 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3206
I0901 10:36:27.016085 140646288807744 basic_session_run_hooks.py:260] loss = 2.3545952, step = 62700 (13.660 sec)
I0901 10:36:40.620643 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35008
I0901 10:36:40.621521 140646288807744 basic_session_run_hooks.py:260] loss = 2.248765, step = 62800 (13.605 sec)
I0901 10:36:54.228296 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3488
I0901 10:36:54.229014 140646288807744 basic_session_run_hooks.py:260] loss = 2.2715163, step = 62900 (13.607 sec)
I0901 10:37:07.651090 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 63000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:37:08.827833 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:37:08.976056 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.78069
I0901 10:37:08.976935 140646288807744 basic_session_run_hooks.py:260] loss = 2.6173427, step = 63000 (14.748 sec)
I0901 10:37:22.584751 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34824
I0901 10:37:22.585427 140646288807744 basic_session_run_hooks.py:260] loss = 2.1963298, step = 63100 (13.608 sec)
I0901 10:37:36.201519 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34389
I0901 10:37:36.202219 140646288807744 basic_session_run_hooks.py:260] loss = 2.3296542, step = 63200 (13.617 sec)
I0901 10:37:49.922260 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28824
I0901 10:37:49.923129 140646288807744 basic_session_run_hooks.py:260] loss = 1.9240967, step = 63300 (13.721 sec)
I0901 10:38:03.463015 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38511
I0901 10:38:03.463699 140646288807744 basic_session_run_hooks.py:260] loss = 1.8127114, step = 63400 (13.541 sec)
I0901 10:38:17.046252 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36201
I0901 10:38:17.046957 140646288807744 basic_session_run_hooks.py:260] loss = 1.8719672, step = 63500 (13.583 sec)
I0901 10:38:30.556391 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40185
I0901 10:38:30.557186 140646288807744 basic_session_run_hooks.py:260] loss = 1.6188023, step = 63600 (13.510 sec)
I0901 10:38:44.092360 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38774
I0901 10:38:44.093364 140646288807744 basic_session_run_hooks.py:260] loss = 1.9609969, step = 63700 (13.536 sec)
I0901 10:38:57.740739 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32686
I0901 10:38:57.741399 140646288807744 basic_session_run_hooks.py:260] loss = 2.2412026, step = 63800 (13.648 sec)
I0901 10:39:11.349430 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34825
I0901 10:39:11.350036 140646288807744 basic_session_run_hooks.py:260] loss = 1.7940167, step = 63900 (13.609 sec)
I0901 10:39:24.719937 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 64000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:39:25.902008 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:39:26.035228 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.8093
I0901 10:39:26.035896 140646288807744 basic_session_run_hooks.py:260] loss = 2.0477064, step = 64000 (14.686 sec)
I0901 10:39:39.725669 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30437
I0901 10:39:39.726594 140646288807744 basic_session_run_hooks.py:260] loss = 1.5945185, step = 64100 (13.691 sec)
I0901 10:39:53.426707 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29872
I0901 10:39:53.427446 140646288807744 basic_session_run_hooks.py:260] loss = 2.6009655, step = 64200 (13.701 sec)
I0901 10:40:07.014512 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35954
I0901 10:40:07.015230 140646288807744 basic_session_run_hooks.py:260] loss = 1.9348923, step = 64300 (13.588 sec)
I0901 10:40:20.474002 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4297
I0901 10:40:20.474719 140646288807744 basic_session_run_hooks.py:260] loss = 1.9505254, step = 64400 (13.459 sec)
I0901 10:40:34.234815 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26701
I0901 10:40:34.235522 140646288807744 basic_session_run_hooks.py:260] loss = 1.8646897, step = 64500 (13.761 sec)
I0901 10:40:47.737496 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40594
I0901 10:40:47.738214 140646288807744 basic_session_run_hooks.py:260] loss = 1.892639, step = 64600 (13.503 sec)
I0901 10:41:01.268887 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39022
I0901 10:41:01.269699 140646288807744 basic_session_run_hooks.py:260] loss = 2.495974, step = 64700 (13.531 sec)
I0901 10:41:14.925215 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32261
I0901 10:41:14.925936 140646288807744 basic_session_run_hooks.py:260] loss = 1.9836982, step = 64800 (13.656 sec)
I0901 10:41:28.521252 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35508
I0901 10:41:28.522122 140646288807744 basic_session_run_hooks.py:260] loss = 2.1494865, step = 64900 (13.596 sec)
I0901 10:41:41.923276 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 65000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:41:43.095866 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:41:43.239230 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.79441
I0901 10:41:43.239981 140646288807744 basic_session_run_hooks.py:260] loss = 2.1197805, step = 65000 (14.718 sec)
I0901 10:41:56.821483 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36255
I0901 10:41:56.822308 140646288807744 basic_session_run_hooks.py:260] loss = 2.2194448, step = 65100 (13.582 sec)
I0901 10:42:10.263235 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4395
I0901 10:42:10.263905 140646288807744 basic_session_run_hooks.py:260] loss = 1.9561535, step = 65200 (13.442 sec)
I0901 10:42:23.814708 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37927
I0901 10:42:23.815377 140646288807744 basic_session_run_hooks.py:260] loss = 2.295888, step = 65300 (13.551 sec)
I0901 10:42:37.430483 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34442
I0901 10:42:37.431344 140646288807744 basic_session_run_hooks.py:260] loss = 2.1241233, step = 65400 (13.616 sec)
I0901 10:42:51.021365 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35787
I0901 10:42:51.022097 140646288807744 basic_session_run_hooks.py:260] loss = 2.1792886, step = 65500 (13.591 sec)
I0901 10:43:04.923948 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.19291
I0901 10:43:04.924719 140646288807744 basic_session_run_hooks.py:260] loss = 2.001871, step = 65600 (13.903 sec)
I0901 10:43:18.557841 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33466
I0901 10:43:18.558748 140646288807744 basic_session_run_hooks.py:260] loss = 2.762066, step = 65700 (13.634 sec)
I0901 10:43:32.025714 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42508
I0901 10:43:32.026384 140646288807744 basic_session_run_hooks.py:260] loss = 2.116056, step = 65800 (13.468 sec)
I0901 10:43:45.551248 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39342
I0901 10:43:45.551942 140646288807744 basic_session_run_hooks.py:260] loss = 2.1593153, step = 65900 (13.526 sec)
I0901 10:43:59.173483 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 66000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:44:00.353341 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 10:44:00.354249 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 10:44:00.467145 140646288807744 estimator.py:1145] Calling model_fn.
I0901 10:44:00.467664 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 10:44:00.467862 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 10:44:00.467913 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 10:44:00.467958 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 10:44:00.468002 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 10:44:00.468054 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 10:44:00.468093 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 10:44:00.521340 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 10:44:00.568832 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 10:44:00.650865 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 10:44:00.658292 140646288807744 t2t_model.py:2248] Building model body
I0901 10:44:03.745855 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 10:44:04.393338 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 10:44:04.404675 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T10:44:04Z
I0901 10:44:04.759660 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 10:44:04.760159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:44:04.760417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 10:44:04.760477: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 10:44:04.760505: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 10:44:04.760516: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 10:44:04.760547: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 10:44:04.760560: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 10:44:04.760586: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 10:44:04.760615: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 10:44:04.760671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:44:04.760909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:44:04.761115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 10:44:04.761152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 10:44:04.761159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 10:44:04.761165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 10:44:04.761237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:44:04.761467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:44:04.761679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 10:44:04.762755 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-66000
I0901 10:44:05.384616 140646288807744 session_manager.py:500] Running local_init_op.
I0901 10:44:05.461011 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 10:44:08.231034 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 10:44:09.513869 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 10:44:10.915272 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 10:44:12.251895 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 10:44:13.674670 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 10:44:15.005190 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 10:44:16.260941 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 10:44:17.545873 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 10:44:18.712474 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 10:44:19.965739 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 10:44:20.111536 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-10:44:20
I0901 10:44:20.111719 140646288807744 estimator.py:2039] Saving dict for global step 66000: global_step = 66000, loss = 2.1808856, metrics-translate_ende_wmt8k/targets/accuracy = 0.5993088, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.77702993, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.3027151, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.1812017, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.37769353, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.57876664
I0901 10:44:20.112073 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 66000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-66000
I0901 10:44:20.254674 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.88156
I0901 10:44:20.255399 140646288807744 basic_session_run_hooks.py:260] loss = 2.057502, step = 66000 (34.703 sec)
I0901 10:44:33.842310 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35963
I0901 10:44:33.843114 140646288807744 basic_session_run_hooks.py:260] loss = 2.1385183, step = 66100 (13.588 sec)
I0901 10:44:47.555197 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29241
I0901 10:44:47.556045 140646288807744 basic_session_run_hooks.py:260] loss = 1.6304607, step = 66200 (13.713 sec)
I0901 10:45:01.064960 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40205
I0901 10:45:01.065652 140646288807744 basic_session_run_hooks.py:260] loss = 2.0572898, step = 66300 (13.510 sec)
I0901 10:45:14.624850 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37469
I0901 10:45:14.625706 140646288807744 basic_session_run_hooks.py:260] loss = 1.9814743, step = 66400 (13.560 sec)
I0901 10:45:28.244928 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3421
I0901 10:45:28.245569 140646288807744 basic_session_run_hooks.py:260] loss = 2.055027, step = 66500 (13.620 sec)
I0901 10:45:41.812618 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37045
I0901 10:45:41.813650 140646288807744 basic_session_run_hooks.py:260] loss = 2.7799678, step = 66600 (13.568 sec)
I0901 10:45:55.475713 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31899
I0901 10:45:55.476625 140646288807744 basic_session_run_hooks.py:260] loss = 1.7215334, step = 66700 (13.663 sec)
I0901 10:46:09.386483 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.18868
I0901 10:46:09.387274 140646288807744 basic_session_run_hooks.py:260] loss = 1.5227758, step = 66800 (13.911 sec)
I0901 10:46:22.978578 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35722
I0901 10:46:22.979317 140646288807744 basic_session_run_hooks.py:260] loss = 1.8924865, step = 66900 (13.592 sec)
I0901 10:46:36.438129 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 67000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:46:37.617788 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:46:37.761679 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.76448
I0901 10:46:37.762372 140646288807744 basic_session_run_hooks.py:260] loss = 2.0666077, step = 67000 (14.783 sec)
I0901 10:46:51.429616 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31639
I0901 10:46:51.430360 140646288807744 basic_session_run_hooks.py:260] loss = 1.8911777, step = 67100 (13.668 sec)
I0901 10:47:05.027969 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35383
I0901 10:47:05.028793 140646288807744 basic_session_run_hooks.py:260] loss = 2.278935, step = 67200 (13.598 sec)
I0901 10:47:18.984083 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.16532
I0901 10:47:18.984760 140646288807744 basic_session_run_hooks.py:260] loss = 2.334965, step = 67300 (13.956 sec)
I0901 10:47:32.537159 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37839
I0901 10:47:32.537886 140646288807744 basic_session_run_hooks.py:260] loss = 2.9357035, step = 67400 (13.553 sec)
I0901 10:47:46.117133 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36378
I0901 10:47:46.118237 140646288807744 basic_session_run_hooks.py:260] loss = 1.9490857, step = 67500 (13.580 sec)
I0901 10:47:59.654900 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38675
I0901 10:47:59.655776 140646288807744 basic_session_run_hooks.py:260] loss = 2.039667, step = 67600 (13.538 sec)
I0901 10:48:13.314493 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32086
I0901 10:48:13.315295 140646288807744 basic_session_run_hooks.py:260] loss = 1.7943087, step = 67700 (13.660 sec)
I0901 10:48:26.882405 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37033
I0901 10:48:26.883298 140646288807744 basic_session_run_hooks.py:260] loss = 2.097197, step = 67800 (13.568 sec)
I0901 10:48:40.431138 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38077
I0901 10:48:40.431880 140646288807744 basic_session_run_hooks.py:260] loss = 1.9742101, step = 67900 (13.549 sec)
2019-09-01 10:48:40.439671: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:48:40.440158: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:48:40.534446: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:48:40.535123: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:48:40.536178: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:48:40.536812: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 10:48:53.830474 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 68000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:48:55.021134 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:48:55.171709 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.784
I0901 10:48:55.172518 140646288807744 basic_session_run_hooks.py:260] loss = 2.6984453, step = 68000 (14.741 sec)
I0901 10:49:09.021189 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22049
I0901 10:49:09.021941 140646288807744 basic_session_run_hooks.py:260] loss = 2.5141437, step = 68100 (13.849 sec)
I0901 10:49:22.562795 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38465
I0901 10:49:22.563580 140646288807744 basic_session_run_hooks.py:260] loss = 1.9938608, step = 68200 (13.542 sec)
I0901 10:49:36.162584 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35305
I0901 10:49:36.163347 140646288807744 basic_session_run_hooks.py:260] loss = 2.292654, step = 68300 (13.600 sec)
I0901 10:49:49.667488 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40472
I0901 10:49:49.668309 140646288807744 basic_session_run_hooks.py:260] loss = 2.0948093, step = 68400 (13.505 sec)
I0901 10:50:03.252507 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36105
I0901 10:50:03.253154 140646288807744 basic_session_run_hooks.py:260] loss = 1.9894282, step = 68500 (13.585 sec)
I0901 10:50:16.947984 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30168
I0901 10:50:16.948654 140646288807744 basic_session_run_hooks.py:260] loss = 2.7522395, step = 68600 (13.695 sec)
I0901 10:50:30.556553 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34831
I0901 10:50:30.557162 140646288807744 basic_session_run_hooks.py:260] loss = 1.8773594, step = 68700 (13.609 sec)
I0901 10:50:44.182746 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33881
I0901 10:50:44.183575 140646288807744 basic_session_run_hooks.py:260] loss = 2.988643, step = 68800 (13.626 sec)
I0901 10:50:57.765726 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36215
I0901 10:50:57.766339 140646288807744 basic_session_run_hooks.py:260] loss = 0.66130584, step = 68900 (13.583 sec)
2019-09-01 10:51:09.709049: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:51:09.725062: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:51:09.831253: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:51:09.831715: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:51:09.832775: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 10:51:09.833218: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 10:51:11.432174 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 69000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:51:12.603347 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:51:12.733584 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.68098
I0901 10:51:12.734349 140646288807744 basic_session_run_hooks.py:260] loss = 2.628049, step = 69000 (14.968 sec)
I0901 10:51:26.318772 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36096
I0901 10:51:26.319537 140646288807744 basic_session_run_hooks.py:260] loss = 2.1267624, step = 69100 (13.585 sec)
I0901 10:51:39.858530 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38566
I0901 10:51:39.859404 140646288807744 basic_session_run_hooks.py:260] loss = 2.63933, step = 69200 (13.540 sec)
I0901 10:51:53.409665 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37945
I0901 10:51:53.410325 140646288807744 basic_session_run_hooks.py:260] loss = 1.813359, step = 69300 (13.551 sec)
I0901 10:52:06.956047 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38205
I0901 10:52:06.956679 140646288807744 basic_session_run_hooks.py:260] loss = 2.3006659, step = 69400 (13.546 sec)
I0901 10:52:20.812721 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.21673
I0901 10:52:20.813353 140646288807744 basic_session_run_hooks.py:260] loss = 2.078088, step = 69500 (13.857 sec)
I0901 10:52:34.415030 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35169
I0901 10:52:34.415755 140646288807744 basic_session_run_hooks.py:260] loss = 1.7717198, step = 69600 (13.602 sec)
I0901 10:52:48.076139 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32005
I0901 10:52:48.076843 140646288807744 basic_session_run_hooks.py:260] loss = 2.4687016, step = 69700 (13.661 sec)
I0901 10:53:01.592097 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39866
I0901 10:53:01.592841 140646288807744 basic_session_run_hooks.py:260] loss = 2.502128, step = 69800 (13.516 sec)
I0901 10:53:15.348518 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26933
I0901 10:53:15.349172 140646288807744 basic_session_run_hooks.py:260] loss = 1.8568397, step = 69900 (13.756 sec)
I0901 10:53:28.650676 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 70000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:53:29.841516 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:53:29.980069 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.83455
I0901 10:53:29.980678 140646288807744 basic_session_run_hooks.py:260] loss = 1.706457, step = 70000 (14.632 sec)
I0901 10:53:43.658229 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31092
I0901 10:53:43.658987 140646288807744 basic_session_run_hooks.py:260] loss = 2.0896692, step = 70100 (13.678 sec)
I0901 10:53:57.285410 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33827
I0901 10:53:57.286067 140646288807744 basic_session_run_hooks.py:260] loss = 1.8420801, step = 70200 (13.627 sec)
I0901 10:54:11.127721 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22422
I0901 10:54:11.128491 140646288807744 basic_session_run_hooks.py:260] loss = 2.014721, step = 70300 (13.842 sec)
I0901 10:54:24.801096 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31349
I0901 10:54:24.801780 140646288807744 basic_session_run_hooks.py:260] loss = 2.7156022, step = 70400 (13.673 sec)
I0901 10:54:38.429349 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3377
I0901 10:54:38.430055 140646288807744 basic_session_run_hooks.py:260] loss = 1.7200682, step = 70500 (13.628 sec)
I0901 10:54:52.040381 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34698
I0901 10:54:52.041157 140646288807744 basic_session_run_hooks.py:260] loss = 2.2844484, step = 70600 (13.611 sec)
I0901 10:55:05.529321 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41348
I0901 10:55:05.530070 140646288807744 basic_session_run_hooks.py:260] loss = 2.162601, step = 70700 (13.489 sec)
I0901 10:55:19.023374 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41067
I0901 10:55:19.024218 140646288807744 basic_session_run_hooks.py:260] loss = 1.5799577, step = 70800 (13.494 sec)
I0901 10:55:32.639153 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34442
I0901 10:55:32.639790 140646288807744 basic_session_run_hooks.py:260] loss = 1.8057473, step = 70900 (13.616 sec)
I0901 10:55:46.219681 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 71000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:55:47.434627 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 10:55:47.435586 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 10:55:47.547090 140646288807744 estimator.py:1145] Calling model_fn.
I0901 10:55:47.547657 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 10:55:47.547857 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 10:55:47.547911 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 10:55:47.547961 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 10:55:47.548013 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 10:55:47.548108 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 10:55:47.548164 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 10:55:47.602257 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 10:55:47.650715 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 10:55:47.734793 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 10:55:47.742530 140646288807744 t2t_model.py:2248] Building model body
I0901 10:55:50.917160 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 10:55:51.335705 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 10:55:51.347587 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T10:55:51Z
I0901 10:55:51.702305 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 10:55:51.702817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:55:51.703094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 10:55:51.703156: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 10:55:51.703188: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 10:55:51.703205: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 10:55:51.703240: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 10:55:51.703272: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 10:55:51.703287: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 10:55:51.703302: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 10:55:51.703378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:55:51.703663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:55:51.703851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 10:55:51.703873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 10:55:51.703880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 10:55:51.703886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 10:55:51.703938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:55:51.704181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 10:55:51.704448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 10:55:51.705403 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-71000
I0901 10:55:52.318041 140646288807744 session_manager.py:500] Running local_init_op.
I0901 10:55:52.393980 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 10:55:55.307900 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 10:55:56.570577 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 10:56:01.953891 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 10:56:03.248938 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 10:56:04.648646 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 10:56:05.842586 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 10:56:07.155121 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 10:56:08.526830 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 10:56:09.764720 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 10:56:11.040794 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 10:56:11.157518 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-10:56:11
I0901 10:56:11.157687 140646288807744 estimator.py:2039] Saving dict for global step 71000: global_step = 71000, loss = 2.1551492, metrics-translate_ende_wmt8k/targets/accuracy = 0.6024921, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.7801678, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.3056522, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.1555567, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.38030788, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.58194005
I0901 10:56:11.158053 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 71000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-71000
I0901 10:56:11.288716 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.58735
I0901 10:56:11.289473 140646288807744 basic_session_run_hooks.py:260] loss = 1.9951683, step = 71000 (38.650 sec)
I0901 10:56:24.775812 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4145
I0901 10:56:24.776568 140646288807744 basic_session_run_hooks.py:260] loss = 2.2015893, step = 71100 (13.487 sec)
I0901 10:56:38.228613 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.4334
I0901 10:56:38.229391 140646288807744 basic_session_run_hooks.py:260] loss = 2.0784447, step = 71200 (13.453 sec)
I0901 10:56:51.772244 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38354
I0901 10:56:51.772973 140646288807744 basic_session_run_hooks.py:260] loss = 2.0629723, step = 71300 (13.544 sec)
I0901 10:57:05.279358 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40351
I0901 10:57:05.280208 140646288807744 basic_session_run_hooks.py:260] loss = 2.2952583, step = 71400 (13.507 sec)
I0901 10:57:18.778474 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40789
I0901 10:57:18.779196 140646288807744 basic_session_run_hooks.py:260] loss = 2.1411762, step = 71500 (13.499 sec)
I0901 10:57:32.410899 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33545
I0901 10:57:32.411576 140646288807744 basic_session_run_hooks.py:260] loss = 2.0349352, step = 71600 (13.632 sec)
I0901 10:57:45.995049 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36152
I0901 10:57:45.995814 140646288807744 basic_session_run_hooks.py:260] loss = 2.3107615, step = 71700 (13.584 sec)
I0901 10:57:59.792279 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24783
I0901 10:57:59.792921 140646288807744 basic_session_run_hooks.py:260] loss = 1.9867611, step = 71800 (13.797 sec)
I0901 10:58:13.618803 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23247
I0901 10:58:13.619458 140646288807744 basic_session_run_hooks.py:260] loss = 2.0089505, step = 71900 (13.827 sec)
I0901 10:58:27.118035 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 72000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 10:58:28.317988 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 10:58:28.457711 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.73904
I0901 10:58:28.458432 140646288807744 basic_session_run_hooks.py:260] loss = 1.8809098, step = 72000 (14.839 sec)
I0901 10:58:41.940873 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41666
I0901 10:58:41.941699 140646288807744 basic_session_run_hooks.py:260] loss = 1.6019878, step = 72100 (13.483 sec)
I0901 10:58:55.512066 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36856
I0901 10:58:55.512689 140646288807744 basic_session_run_hooks.py:260] loss = 2.0401773, step = 72200 (13.571 sec)
I0901 10:59:09.038703 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39281
I0901 10:59:09.039359 140646288807744 basic_session_run_hooks.py:260] loss = 2.0978038, step = 72300 (13.527 sec)
I0901 10:59:22.700108 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31989
I0901 10:59:22.700966 140646288807744 basic_session_run_hooks.py:260] loss = 2.2888112, step = 72400 (13.662 sec)
I0901 10:59:36.444271 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27582
I0901 10:59:36.444982 140646288807744 basic_session_run_hooks.py:260] loss = 2.1169834, step = 72500 (13.744 sec)
I0901 10:59:49.957112 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40037
I0901 10:59:49.957791 140646288807744 basic_session_run_hooks.py:260] loss = 2.5600073, step = 72600 (13.513 sec)
I0901 11:00:03.580556 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34029
I0901 11:00:03.581198 140646288807744 basic_session_run_hooks.py:260] loss = 1.517426, step = 72700 (13.623 sec)
I0901 11:00:17.116484 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38774
I0901 11:00:17.117177 140646288807744 basic_session_run_hooks.py:260] loss = 2.008347, step = 72800 (13.536 sec)
I0901 11:00:30.756947 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33113
I0901 11:00:30.757606 140646288807744 basic_session_run_hooks.py:260] loss = 2.4600644, step = 72900 (13.640 sec)
I0901 11:00:44.197910 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 73000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:00:45.373699 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:00:45.507585 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.77937
I0901 11:00:45.508561 140646288807744 basic_session_run_hooks.py:260] loss = 2.036525, step = 73000 (14.751 sec)
I0901 11:00:59.103883 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35494
I0901 11:00:59.104882 140646288807744 basic_session_run_hooks.py:260] loss = 2.48416, step = 73100 (13.596 sec)
I0901 11:01:12.672572 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36991
I0901 11:01:12.673411 140646288807744 basic_session_run_hooks.py:260] loss = 3.073067, step = 73200 (13.569 sec)
I0901 11:01:26.289642 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34372
I0901 11:01:26.290395 140646288807744 basic_session_run_hooks.py:260] loss = 2.2194195, step = 73300 (13.617 sec)
I0901 11:01:39.925166 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33378
I0901 11:01:39.925935 140646288807744 basic_session_run_hooks.py:260] loss = 1.7271131, step = 73400 (13.636 sec)
I0901 11:01:53.506882 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36284
I0901 11:01:53.507606 140646288807744 basic_session_run_hooks.py:260] loss = 2.142063, step = 73500 (13.582 sec)
I0901 11:02:07.128049 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34152
I0901 11:02:07.128648 140646288807744 basic_session_run_hooks.py:260] loss = 1.6811922, step = 73600 (13.621 sec)
I0901 11:02:20.707368 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36414
I0901 11:02:20.708028 140646288807744 basic_session_run_hooks.py:260] loss = 2.3197005, step = 73700 (13.579 sec)
I0901 11:02:34.318404 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34698
I0901 11:02:34.319083 140646288807744 basic_session_run_hooks.py:260] loss = 1.6787966, step = 73800 (13.611 sec)
I0901 11:02:47.872992 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37758
I0901 11:02:47.873734 140646288807744 basic_session_run_hooks.py:260] loss = 1.8003843, step = 73900 (13.555 sec)
I0901 11:03:01.486604 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 74000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:03:02.660289 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:03:02.804658 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.69718
I0901 11:03:02.805459 140646288807744 basic_session_run_hooks.py:260] loss = 1.9844925, step = 74000 (14.932 sec)
I0901 11:03:16.303454 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40807
I0901 11:03:16.304139 140646288807744 basic_session_run_hooks.py:260] loss = 1.9530162, step = 74100 (13.499 sec)
I0901 11:03:30.012628 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29439
I0901 11:03:30.013444 140646288807744 basic_session_run_hooks.py:260] loss = 2.657999, step = 74200 (13.709 sec)
I0901 11:03:43.424021 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.45635
I0901 11:03:43.424805 140646288807744 basic_session_run_hooks.py:260] loss = 1.8999653, step = 74300 (13.411 sec)
I0901 11:03:56.958500 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38853
I0901 11:03:56.959343 140646288807744 basic_session_run_hooks.py:260] loss = 1.9845735, step = 74400 (13.535 sec)
I0901 11:04:10.445762 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41441
I0901 11:04:10.446531 140646288807744 basic_session_run_hooks.py:260] loss = 2.2250211, step = 74500 (13.487 sec)
I0901 11:04:24.125327 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31017
I0901 11:04:24.126026 140646288807744 basic_session_run_hooks.py:260] loss = 2.4561944, step = 74600 (13.679 sec)
I0901 11:04:37.849488 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28642
I0901 11:04:37.850296 140646288807744 basic_session_run_hooks.py:260] loss = 1.8663082, step = 74700 (13.724 sec)
I0901 11:04:51.475559 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33887
I0901 11:04:51.476210 140646288807744 basic_session_run_hooks.py:260] loss = 2.4288263, step = 74800 (13.626 sec)
2019-09-01 11:05:02.352945: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:05:02.353737: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:05:02.414439: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:05:02.414893: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:05:02.415916: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:05:02.416352: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 11:05:05.071058 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35538
I0901 11:05:05.071857 140646288807744 basic_session_run_hooks.py:260] loss = 1.8770797, step = 74900 (13.596 sec)
I0901 11:05:18.356735 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 75000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:05:19.528241 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:05:19.671150 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.84927
I0901 11:05:19.671958 140646288807744 basic_session_run_hooks.py:260] loss = 1.8706092, step = 75000 (14.600 sec)
I0901 11:05:33.284600 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34568
I0901 11:05:33.285205 140646288807744 basic_session_run_hooks.py:260] loss = 2.2512991, step = 75100 (13.613 sec)
I0901 11:05:46.914943 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33658
I0901 11:05:46.915651 140646288807744 basic_session_run_hooks.py:260] loss = 2.1621969, step = 75200 (13.630 sec)
I0901 11:06:00.381121 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42601
I0901 11:06:00.381893 140646288807744 basic_session_run_hooks.py:260] loss = 1.7175527, step = 75300 (13.466 sec)
I0901 11:06:13.969824 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35905
I0901 11:06:13.970747 140646288807744 basic_session_run_hooks.py:260] loss = 2.056394, step = 75400 (13.589 sec)
I0901 11:06:27.591598 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34119
I0901 11:06:27.592266 140646288807744 basic_session_run_hooks.py:260] loss = 2.0067031, step = 75500 (13.622 sec)
I0901 11:06:41.158638 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3708
I0901 11:06:41.159306 140646288807744 basic_session_run_hooks.py:260] loss = 2.242524, step = 75600 (13.567 sec)
I0901 11:06:54.698354 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38568
I0901 11:06:54.699235 140646288807744 basic_session_run_hooks.py:260] loss = 1.8683724, step = 75700 (13.540 sec)
I0901 11:07:08.225975 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39228
I0901 11:07:08.226634 140646288807744 basic_session_run_hooks.py:260] loss = 1.9949057, step = 75800 (13.527 sec)
I0901 11:07:21.848606 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34073
I0901 11:07:21.849297 140646288807744 basic_session_run_hooks.py:260] loss = 2.0149155, step = 75900 (13.623 sec)
I0901 11:07:35.338591 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 76000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:07:36.533043 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 11:07:36.534018 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 11:07:36.647765 140646288807744 estimator.py:1145] Calling model_fn.
I0901 11:07:36.648343 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 11:07:36.648579 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 11:07:36.648646 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 11:07:36.648721 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 11:07:36.648791 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 11:07:36.648864 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 11:07:36.648930 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 11:07:36.703045 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 11:07:36.751376 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 11:07:36.835852 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 11:07:36.843410 140646288807744 t2t_model.py:2248] Building model body
I0901 11:07:39.998785 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 11:07:40.431118 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 11:07:40.442883 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T11:07:40Z
I0901 11:07:40.810461 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 11:07:40.810921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:07:40.811176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 11:07:40.811233: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 11:07:40.811260: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 11:07:40.811288: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 11:07:40.811301: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 11:07:40.811334: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 11:07:40.811361: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 11:07:40.811374: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 11:07:40.811448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:07:40.811754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:07:40.811951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 11:07:40.811989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 11:07:40.812004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 11:07:40.812010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 11:07:40.812066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:07:40.812355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:07:40.812635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 11:07:40.813486 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-76000
I0901 11:07:41.434151 140646288807744 session_manager.py:500] Running local_init_op.
I0901 11:07:41.509930 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 11:07:44.458509 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 11:07:45.678694 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 11:07:47.013631 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 11:07:48.280218 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 11:07:49.484543 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 11:07:50.763546 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 11:07:51.982279 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 11:07:53.298956 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 11:07:54.627146 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 11:07:55.646955 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 11:07:55.764431 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-11:07:55
I0901 11:07:55.764576 140646288807744 estimator.py:2039] Saving dict for global step 76000: global_step = 76000, loss = 2.1301174, metrics-translate_ende_wmt8k/targets/accuracy = 0.6074604, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.78463584, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.31132197, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.1303668, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.38700762, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.58660674
I0901 11:07:55.764956 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 76000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-76000
I0901 11:07:55.907388 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.9361
I0901 11:07:55.908160 140646288807744 basic_session_run_hooks.py:260] loss = 2.0414295, step = 76000 (34.059 sec)
I0901 11:08:09.439422 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38987
I0901 11:08:09.440194 140646288807744 basic_session_run_hooks.py:260] loss = 1.8235304, step = 76100 (13.532 sec)
I0901 11:08:22.958068 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39719
I0901 11:08:22.958826 140646288807744 basic_session_run_hooks.py:260] loss = 1.932651, step = 76200 (13.519 sec)
I0901 11:08:36.538794 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36338
I0901 11:08:36.539449 140646288807744 basic_session_run_hooks.py:260] loss = 1.868567, step = 76300 (13.581 sec)
I0901 11:08:49.994988 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43152
I0901 11:08:49.995668 140646288807744 basic_session_run_hooks.py:260] loss = 1.8051473, step = 76400 (13.456 sec)
I0901 11:09:03.543931 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38065
I0901 11:09:03.544728 140646288807744 basic_session_run_hooks.py:260] loss = 2.1118858, step = 76500 (13.549 sec)
I0901 11:09:16.954862 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.45661
I0901 11:09:16.955764 140646288807744 basic_session_run_hooks.py:260] loss = 2.2204368, step = 76600 (13.411 sec)
I0901 11:09:30.504123 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38047
I0901 11:09:30.504797 140646288807744 basic_session_run_hooks.py:260] loss = 2.0673676, step = 76700 (13.549 sec)
I0901 11:09:43.986255 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41723
I0901 11:09:43.987056 140646288807744 basic_session_run_hooks.py:260] loss = 1.7129182, step = 76800 (13.482 sec)
I0901 11:09:57.611973 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33906
I0901 11:09:57.612649 140646288807744 basic_session_run_hooks.py:260] loss = 1.9382216, step = 76900 (13.626 sec)
I0901 11:10:11.132416 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 77000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:10:12.306712 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:10:12.458011 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.7358
I0901 11:10:12.458855 140646288807744 basic_session_run_hooks.py:260] loss = 2.8479016, step = 77000 (14.846 sec)
I0901 11:10:25.937778 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41853
I0901 11:10:25.938457 140646288807744 basic_session_run_hooks.py:260] loss = 1.4638938, step = 77100 (13.480 sec)
I0901 11:10:39.649132 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29322
I0901 11:10:39.649868 140646288807744 basic_session_run_hooks.py:260] loss = 2.0307922, step = 77200 (13.711 sec)
I0901 11:10:53.469909 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.23548
I0901 11:10:53.470841 140646288807744 basic_session_run_hooks.py:260] loss = 2.4610379, step = 77300 (13.821 sec)
I0901 11:11:07.077572 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3488
I0901 11:11:07.078215 140646288807744 basic_session_run_hooks.py:260] loss = 1.4207499, step = 77400 (13.607 sec)
I0901 11:11:20.585873 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40286
I0901 11:11:20.586640 140646288807744 basic_session_run_hooks.py:260] loss = 2.2117922, step = 77500 (13.508 sec)
I0901 11:11:34.139040 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37835
I0901 11:11:34.139789 140646288807744 basic_session_run_hooks.py:260] loss = 1.8460087, step = 77600 (13.553 sec)
I0901 11:11:47.678791 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38566
I0901 11:11:47.679537 140646288807744 basic_session_run_hooks.py:260] loss = 1.9861981, step = 77700 (13.540 sec)
I0901 11:12:01.466687 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.25274
I0901 11:12:01.467324 140646288807744 basic_session_run_hooks.py:260] loss = 1.9375736, step = 77800 (13.788 sec)
I0901 11:12:15.122950 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32265
I0901 11:12:15.123661 140646288807744 basic_session_run_hooks.py:260] loss = 2.4619894, step = 77900 (13.656 sec)
I0901 11:12:28.695758 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 78000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:12:29.889604 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:12:30.017649 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.7138
I0901 11:12:30.018410 140646288807744 basic_session_run_hooks.py:260] loss = 1.7566346, step = 78000 (14.895 sec)
I0901 11:12:43.555840 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38651
I0901 11:12:43.556694 140646288807744 basic_session_run_hooks.py:260] loss = 1.7431077, step = 78100 (13.538 sec)
I0901 11:12:57.440235 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.20233
I0901 11:12:57.440861 140646288807744 basic_session_run_hooks.py:260] loss = 1.6799862, step = 78200 (13.884 sec)
I0901 11:13:10.997999 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37586
I0901 11:13:10.999170 140646288807744 basic_session_run_hooks.py:260] loss = 2.714328, step = 78300 (13.558 sec)
I0901 11:13:24.495813 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40859
I0901 11:13:24.496689 140646288807744 basic_session_run_hooks.py:260] loss = 2.106703, step = 78400 (13.498 sec)
I0901 11:13:38.078907 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36209
I0901 11:13:38.079706 140646288807744 basic_session_run_hooks.py:260] loss = 1.668632, step = 78500 (13.583 sec)
I0901 11:13:51.626331 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38148
I0901 11:13:51.627066 140646288807744 basic_session_run_hooks.py:260] loss = 1.7329804, step = 78600 (13.547 sec)
I0901 11:14:05.164069 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38676
I0901 11:14:05.164690 140646288807744 basic_session_run_hooks.py:260] loss = 2.0183237, step = 78700 (13.538 sec)
I0901 11:14:18.725691 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37375
I0901 11:14:18.726334 140646288807744 basic_session_run_hooks.py:260] loss = 2.1263802, step = 78800 (13.562 sec)
I0901 11:14:32.355045 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33711
I0901 11:14:32.355689 140646288807744 basic_session_run_hooks.py:260] loss = 2.371297, step = 78900 (13.629 sec)
I0901 11:14:46.107980 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 79000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:14:47.287514 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:14:47.430876 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.63313
I0901 11:14:47.431651 140646288807744 basic_session_run_hooks.py:260] loss = 1.9557879, step = 79000 (15.076 sec)
I0901 11:15:01.059445 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33753
I0901 11:15:01.060171 140646288807744 basic_session_run_hooks.py:260] loss = 2.6501336, step = 79100 (13.629 sec)
I0901 11:15:14.709310 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32608
I0901 11:15:14.710134 140646288807744 basic_session_run_hooks.py:260] loss = 2.2671156, step = 79200 (13.650 sec)
I0901 11:15:28.272388 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37296
I0901 11:15:28.273067 140646288807744 basic_session_run_hooks.py:260] loss = 2.0158596, step = 79300 (13.563 sec)
I0901 11:15:41.855489 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36209
I0901 11:15:41.856160 140646288807744 basic_session_run_hooks.py:260] loss = 2.40469, step = 79400 (13.583 sec)
I0901 11:15:55.906972 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.11669
I0901 11:15:55.907727 140646288807744 basic_session_run_hooks.py:260] loss = 1.8148408, step = 79500 (14.052 sec)
I0901 11:16:09.454481 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38143
I0901 11:16:09.455178 140646288807744 basic_session_run_hooks.py:260] loss = 1.9374499, step = 79600 (13.547 sec)
I0901 11:16:23.037336 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36222
I0901 11:16:23.038028 140646288807744 basic_session_run_hooks.py:260] loss = 2.168765, step = 79700 (13.583 sec)
I0901 11:16:36.445764 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.458
I0901 11:16:36.446667 140646288807744 basic_session_run_hooks.py:260] loss = 2.21913, step = 79800 (13.409 sec)
I0901 11:16:49.998221 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37873
I0901 11:16:49.999089 140646288807744 basic_session_run_hooks.py:260] loss = 2.4328523, step = 79900 (13.552 sec)
I0901 11:17:03.410576 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 80000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:17:04.575750 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:17:04.715425 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.79477
I0901 11:17:04.716139 140646288807744 basic_session_run_hooks.py:260] loss = 1.7635643, step = 80000 (14.717 sec)
I0901 11:17:18.275974 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37433
I0901 11:17:18.276638 140646288807744 basic_session_run_hooks.py:260] loss = 2.0192595, step = 80100 (13.560 sec)
I0901 11:17:31.736071 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42937
I0901 11:17:31.736792 140646288807744 basic_session_run_hooks.py:260] loss = 2.124502, step = 80200 (13.460 sec)
I0901 11:17:45.441431 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29641
I0901 11:17:45.442090 140646288807744 basic_session_run_hooks.py:260] loss = 2.0956507, step = 80300 (13.705 sec)
I0901 11:17:59.002331 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37414
I0901 11:17:59.002997 140646288807744 basic_session_run_hooks.py:260] loss = 1.4520489, step = 80400 (13.561 sec)
I0901 11:18:12.492479 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41282
I0901 11:18:12.493164 140646288807744 basic_session_run_hooks.py:260] loss = 2.598897, step = 80500 (13.490 sec)
I0901 11:18:26.150837 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32152
I0901 11:18:26.151638 140646288807744 basic_session_run_hooks.py:260] loss = 2.068015, step = 80600 (13.658 sec)
I0901 11:18:39.791429 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33106
I0901 11:18:39.792239 140646288807744 basic_session_run_hooks.py:260] loss = 1.8160273, step = 80700 (13.641 sec)
I0901 11:18:53.597257 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24331
I0901 11:18:53.598079 140646288807744 basic_session_run_hooks.py:260] loss = 1.9294881, step = 80800 (13.806 sec)
I0901 11:19:07.146924 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38026
I0901 11:19:07.147735 140646288807744 basic_session_run_hooks.py:260] loss = 1.851568, step = 80900 (13.550 sec)
I0901 11:19:20.685508 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 81000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:19:21.857718 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 11:19:21.858746 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 11:19:21.968987 140646288807744 estimator.py:1145] Calling model_fn.
I0901 11:19:21.970047 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 11:19:21.970320 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 11:19:21.970385 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 11:19:21.970463 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 11:19:21.970530 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 11:19:21.970586 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 11:19:21.970669 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 11:19:22.024592 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 11:19:22.072863 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 11:19:22.157262 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 11:19:22.164864 140646288807744 t2t_model.py:2248] Building model body
I0901 11:19:25.501119 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 11:19:25.919480 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 11:19:25.930394 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T11:19:25Z
I0901 11:19:26.291935 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 11:19:26.292412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:19:26.292692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 11:19:26.292757: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 11:19:26.292784: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 11:19:26.292818: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 11:19:26.292829: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 11:19:26.292841: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 11:19:26.292865: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 11:19:26.292893: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 11:19:26.292947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:19:26.293215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:19:26.293426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 11:19:26.293463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 11:19:26.293477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 11:19:26.293483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 11:19:26.293577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:19:26.293832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:19:26.294055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 11:19:26.294853 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-81000
I0901 11:19:26.897767 140646288807744 session_manager.py:500] Running local_init_op.
I0901 11:19:26.972136 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 11:19:32.417674 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 11:19:33.823456 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 11:19:35.114159 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 11:19:36.313885 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 11:19:37.722415 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 11:19:39.096232 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 11:19:40.389846 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 11:19:41.654347 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 11:19:42.889938 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 11:19:44.076467 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 11:19:44.198820 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-11:19:44
I0901 11:19:44.198972 140646288807744 estimator.py:2039] Saving dict for global step 81000: global_step = 81000, loss = 2.1149492, metrics-translate_ende_wmt8k/targets/accuracy = 0.60949546, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.7861593, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.31355226, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.115274, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.38886368, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.5883136
I0901 11:19:44.199336 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 81000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-81000
I0901 11:19:44.343793 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.6884
I0901 11:19:44.344564 140646288807744 basic_session_run_hooks.py:260] loss = 1.9921222, step = 81000 (37.197 sec)
I0901 11:19:57.783436 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.44068
I0901 11:19:57.784161 140646288807744 basic_session_run_hooks.py:260] loss = 1.3953084, step = 81100 (13.440 sec)
2019-09-01 11:20:03.214493: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:20:03.215087: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:20:03.305389: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:20:03.306041: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:20:03.307355: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:20:03.307992: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 11:20:11.507322 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28656
I0901 11:20:11.507978 140646288807744 basic_session_run_hooks.py:260] loss = 2.0213687, step = 81200 (13.724 sec)
I0901 11:20:25.024108 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39821
I0901 11:20:25.024907 140646288807744 basic_session_run_hooks.py:260] loss = 2.1387854, step = 81300 (13.517 sec)
I0901 11:20:38.604591 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36351
I0901 11:20:38.605264 140646288807744 basic_session_run_hooks.py:260] loss = 1.7286705, step = 81400 (13.580 sec)
I0901 11:20:52.268812 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31838
I0901 11:20:52.269475 140646288807744 basic_session_run_hooks.py:260] loss = 2.1695693, step = 81500 (13.664 sec)
I0901 11:21:06.243816 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.15564
I0901 11:21:06.244729 140646288807744 basic_session_run_hooks.py:260] loss = 1.9898574, step = 81600 (13.975 sec)
I0901 11:21:19.919394 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31231
I0901 11:21:19.920109 140646288807744 basic_session_run_hooks.py:260] loss = 2.0276392, step = 81700 (13.675 sec)
I0901 11:21:33.389691 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42374
I0901 11:21:33.390471 140646288807744 basic_session_run_hooks.py:260] loss = 2.150932, step = 81800 (13.470 sec)
I0901 11:21:47.015499 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33901
I0901 11:21:47.016377 140646288807744 basic_session_run_hooks.py:260] loss = 1.581368, step = 81900 (13.626 sec)
I0901 11:22:00.466215 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 82000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:22:01.645354 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:22:01.790470 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.7682
I0901 11:22:01.791285 140646288807744 basic_session_run_hooks.py:260] loss = 2.0515661, step = 82000 (14.775 sec)
I0901 11:22:15.431619 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33076
I0901 11:22:15.432289 140646288807744 basic_session_run_hooks.py:260] loss = 1.5181919, step = 82100 (13.641 sec)
I0901 11:22:29.013214 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3629
I0901 11:22:29.013932 140646288807744 basic_session_run_hooks.py:260] loss = 2.018652, step = 82200 (13.582 sec)
I0901 11:22:42.547749 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38851
I0901 11:22:42.548455 140646288807744 basic_session_run_hooks.py:260] loss = 1.9334172, step = 82300 (13.535 sec)
I0901 11:22:56.079912 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3898
I0901 11:22:56.080736 140646288807744 basic_session_run_hooks.py:260] loss = 1.8091457, step = 82400 (13.532 sec)
I0901 11:23:09.661425 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36295
I0901 11:23:09.662086 140646288807744 basic_session_run_hooks.py:260] loss = 2.5689058, step = 82500 (13.581 sec)
I0901 11:23:23.521649 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.21489
I0901 11:23:23.522453 140646288807744 basic_session_run_hooks.py:260] loss = 2.0526102, step = 82600 (13.860 sec)
I0901 11:23:37.180954 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32102
I0901 11:23:37.181882 140646288807744 basic_session_run_hooks.py:260] loss = 1.8844339, step = 82700 (13.659 sec)
I0901 11:23:50.716753 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38782
I0901 11:23:50.717418 140646288807744 basic_session_run_hooks.py:260] loss = 1.9061795, step = 82800 (13.536 sec)
I0901 11:24:04.324254 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34889
I0901 11:24:04.324937 140646288807744 basic_session_run_hooks.py:260] loss = 1.7164521, step = 82900 (13.608 sec)
I0901 11:24:17.748817 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 83000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:24:18.925436 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:24:19.052531 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.78966
I0901 11:24:19.053327 140646288807744 basic_session_run_hooks.py:260] loss = 1.8309797, step = 83000 (14.728 sec)
I0901 11:24:32.656178 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35097
I0901 11:24:32.656948 140646288807744 basic_session_run_hooks.py:260] loss = 2.0218441, step = 83100 (13.604 sec)
I0901 11:24:46.255124 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35351
I0901 11:24:46.255750 140646288807744 basic_session_run_hooks.py:260] loss = 1.7486501, step = 83200 (13.599 sec)
I0901 11:24:59.825440 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36902
I0901 11:24:59.826125 140646288807744 basic_session_run_hooks.py:260] loss = 1.8234042, step = 83300 (13.570 sec)
I0901 11:25:13.336547 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40132
I0901 11:25:13.337204 140646288807744 basic_session_run_hooks.py:260] loss = 2.1639569, step = 83400 (13.511 sec)
I0901 11:25:26.844695 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40294
I0901 11:25:26.845439 140646288807744 basic_session_run_hooks.py:260] loss = 1.99223, step = 83500 (13.508 sec)
I0901 11:25:40.537180 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30327
I0901 11:25:40.537850 140646288807744 basic_session_run_hooks.py:260] loss = 1.9369233, step = 83600 (13.692 sec)
I0901 11:25:54.511455 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.15601
I0901 11:25:54.512148 140646288807744 basic_session_run_hooks.py:260] loss = 3.0694256, step = 83700 (13.974 sec)
I0901 11:26:08.125137 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34555
I0901 11:26:08.125933 140646288807744 basic_session_run_hooks.py:260] loss = 2.001991, step = 83800 (13.614 sec)
I0901 11:26:21.780858 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32294
I0901 11:26:21.781508 140646288807744 basic_session_run_hooks.py:260] loss = 1.8522234, step = 83900 (13.656 sec)
I0901 11:26:35.150098 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 84000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:26:36.330377 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:26:36.473943 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.80592
I0901 11:26:36.474698 140646288807744 basic_session_run_hooks.py:260] loss = 2.144119, step = 84000 (14.693 sec)
I0901 11:26:50.067076 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35666
I0901 11:26:50.067877 140646288807744 basic_session_run_hooks.py:260] loss = 1.8429338, step = 84100 (13.593 sec)
I0901 11:27:03.819184 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27161
I0901 11:27:03.819875 140646288807744 basic_session_run_hooks.py:260] loss = 1.9064616, step = 84200 (13.752 sec)
I0901 11:27:17.264030 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43781
I0901 11:27:17.264863 140646288807744 basic_session_run_hooks.py:260] loss = 1.9784487, step = 84300 (13.445 sec)
I0901 11:27:30.808720 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38295
I0901 11:27:30.809417 140646288807744 basic_session_run_hooks.py:260] loss = 2.2265372, step = 84400 (13.545 sec)
I0901 11:27:44.360612 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37904
I0901 11:27:44.361500 140646288807744 basic_session_run_hooks.py:260] loss = 1.643291, step = 84500 (13.552 sec)
I0901 11:27:57.938104 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36513
I0901 11:27:57.938796 140646288807744 basic_session_run_hooks.py:260] loss = 2.2985597, step = 84600 (13.577 sec)
I0901 11:28:11.776332 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22636
I0901 11:28:11.777166 140646288807744 basic_session_run_hooks.py:260] loss = 2.6008813, step = 84700 (13.838 sec)
I0901 11:28:25.445708 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31562
I0901 11:28:25.446458 140646288807744 basic_session_run_hooks.py:260] loss = 2.1580594, step = 84800 (13.669 sec)
I0901 11:28:38.911753 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42609
I0901 11:28:38.912418 140646288807744 basic_session_run_hooks.py:260] loss = 2.2692895, step = 84900 (13.466 sec)
I0901 11:28:52.323609 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 85000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:28:53.495048 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:28:53.636903 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.7911
I0901 11:28:53.637722 140646288807744 basic_session_run_hooks.py:260] loss = 1.4932803, step = 85000 (14.725 sec)
I0901 11:29:07.201073 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37237
I0901 11:29:07.201819 140646288807744 basic_session_run_hooks.py:260] loss = 1.5941942, step = 85100 (13.564 sec)
I0901 11:29:20.905274 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29703
I0901 11:29:20.905994 140646288807744 basic_session_run_hooks.py:260] loss = 2.2896793, step = 85200 (13.704 sec)
I0901 11:29:34.428852 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39449
I0901 11:29:34.429771 140646288807744 basic_session_run_hooks.py:260] loss = 1.834952, step = 85300 (13.524 sec)
I0901 11:29:48.030630 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35198
I0901 11:29:48.031277 140646288807744 basic_session_run_hooks.py:260] loss = 2.4391184, step = 85400 (13.602 sec)
I0901 11:30:01.711461 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30951
I0901 11:30:01.712455 140646288807744 basic_session_run_hooks.py:260] loss = 2.1479628, step = 85500 (13.681 sec)
I0901 11:30:15.307561 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35504
I0901 11:30:15.308341 140646288807744 basic_session_run_hooks.py:260] loss = 1.5955528, step = 85600 (13.596 sec)
I0901 11:30:28.893392 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36061
I0901 11:30:28.894088 140646288807744 basic_session_run_hooks.py:260] loss = 1.9524113, step = 85700 (13.586 sec)
I0901 11:30:42.515265 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34114
I0901 11:30:42.516132 140646288807744 basic_session_run_hooks.py:260] loss = 2.9408932, step = 85800 (13.622 sec)
I0901 11:30:56.152449 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33289
I0901 11:30:56.153200 140646288807744 basic_session_run_hooks.py:260] loss = 2.38348, step = 85900 (13.637 sec)
I0901 11:31:09.751988 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 86000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:31:10.936080 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 11:31:10.937151 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 11:31:11.049419 140646288807744 estimator.py:1145] Calling model_fn.
I0901 11:31:11.049994 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 11:31:11.050228 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 11:31:11.050281 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 11:31:11.050394 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 11:31:11.050449 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 11:31:11.050546 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 11:31:11.050597 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 11:31:11.105703 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 11:31:11.154025 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 11:31:11.238619 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 11:31:11.246248 140646288807744 t2t_model.py:2248] Building model body
I0901 11:31:14.445887 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 11:31:14.867341 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 11:31:14.878561 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T11:31:14Z
I0901 11:31:15.233082 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 11:31:15.233547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:31:15.233826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 11:31:15.233879: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 11:31:15.233906: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 11:31:15.233938: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 11:31:15.233950: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 11:31:15.233975: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 11:31:15.234008: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 11:31:15.234021: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 11:31:15.234105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:31:15.234446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:31:15.234669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 11:31:15.234721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 11:31:15.234728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 11:31:15.234740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 11:31:15.234834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:31:15.235063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:31:15.235266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 11:31:15.236161 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-86000
I0901 11:31:15.862728 140646288807744 session_manager.py:500] Running local_init_op.
I0901 11:31:15.939062 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 11:31:18.781838 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 11:31:19.956185 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 11:31:21.216411 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 11:31:22.521591 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 11:31:23.699709 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 11:31:24.863858 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 11:31:26.117597 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 11:31:27.271360 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 11:31:28.525787 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 11:31:29.739147 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 11:31:29.853807 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-11:31:29
I0901 11:31:29.853985 140646288807744 estimator.py:2039] Saving dict for global step 86000: global_step = 86000, loss = 2.0984488, metrics-translate_ende_wmt8k/targets/accuracy = 0.61068916, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.7891607, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.313748, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.0985782, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.38991016, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.589373
I0901 11:31:29.854373 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 86000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-86000
I0901 11:31:29.993944 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.95495
I0901 11:31:29.994638 140646288807744 basic_session_run_hooks.py:260] loss = 1.7106495, step = 86000 (33.841 sec)
I0901 11:31:43.424245 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.44585
I0901 11:31:43.424975 140646288807744 basic_session_run_hooks.py:260] loss = 2.3619764, step = 86100 (13.430 sec)
2019-09-01 11:31:44.873337: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:31:44.873867: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 11:31:56.983112 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37525
I0901 11:31:56.983828 140646288807744 basic_session_run_hooks.py:260] loss = 2.108304, step = 86200 (13.559 sec)
I0901 11:32:10.634689 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.32516
I0901 11:32:10.635341 140646288807744 basic_session_run_hooks.py:260] loss = 2.0159686, step = 86300 (13.652 sec)
I0901 11:32:24.103170 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42474
I0901 11:32:24.103800 140646288807744 basic_session_run_hooks.py:260] loss = 1.9830483, step = 86400 (13.468 sec)
I0901 11:32:37.576948 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42183
I0901 11:32:37.577731 140646288807744 basic_session_run_hooks.py:260] loss = 1.539484, step = 86500 (13.474 sec)
I0901 11:32:51.065163 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41388
I0901 11:32:51.066119 140646288807744 basic_session_run_hooks.py:260] loss = 1.993834, step = 86600 (13.488 sec)
I0901 11:33:04.692966 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33794
I0901 11:33:04.693752 140646288807744 basic_session_run_hooks.py:260] loss = 2.102545, step = 86700 (13.628 sec)
I0901 11:33:18.315104 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34099
I0901 11:33:18.315883 140646288807744 basic_session_run_hooks.py:260] loss = 1.9358686, step = 86800 (13.622 sec)
I0901 11:33:31.882602 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37056
I0901 11:33:31.883381 140646288807744 basic_session_run_hooks.py:260] loss = 1.6322172, step = 86900 (13.567 sec)
I0901 11:33:45.315119 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 87000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:33:46.501152 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:33:46.646310 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.77337
I0901 11:33:46.647088 140646288807744 basic_session_run_hooks.py:260] loss = 1.9621497, step = 87000 (14.764 sec)
I0901 11:34:00.337918 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30375
I0901 11:34:00.338736 140646288807744 basic_session_run_hooks.py:260] loss = 2.0462496, step = 87100 (13.692 sec)
I0901 11:34:13.869809 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38995
I0901 11:34:13.870518 140646288807744 basic_session_run_hooks.py:260] loss = 1.9800581, step = 87200 (13.532 sec)
I0901 11:34:27.586359 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29046
I0901 11:34:27.587078 140646288807744 basic_session_run_hooks.py:260] loss = 1.8943192, step = 87300 (13.717 sec)
I0901 11:34:41.221312 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33409
I0901 11:34:41.222133 140646288807744 basic_session_run_hooks.py:260] loss = 2.0360932, step = 87400 (13.635 sec)
I0901 11:34:54.899688 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31081
I0901 11:34:54.900390 140646288807744 basic_session_run_hooks.py:260] loss = 1.7650682, step = 87500 (13.678 sec)
I0901 11:35:08.390433 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41249
I0901 11:35:08.391198 140646288807744 basic_session_run_hooks.py:260] loss = 2.049552, step = 87600 (13.491 sec)
I0901 11:35:21.876851 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41487
I0901 11:35:21.877503 140646288807744 basic_session_run_hooks.py:260] loss = 1.8811021, step = 87700 (13.486 sec)
I0901 11:35:35.475877 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35347
I0901 11:35:35.476728 140646288807744 basic_session_run_hooks.py:260] loss = 2.2295446, step = 87800 (13.599 sec)
I0901 11:35:48.978671 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40588
I0901 11:35:48.979372 140646288807744 basic_session_run_hooks.py:260] loss = 2.3345268, step = 87900 (13.503 sec)
I0901 11:36:02.384915 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 88000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:36:03.589746 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:36:03.734532 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.77697
I0901 11:36:03.735246 140646288807744 basic_session_run_hooks.py:260] loss = 2.4198117, step = 88000 (14.756 sec)
I0901 11:36:17.412569 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31099
I0901 11:36:17.413197 140646288807744 basic_session_run_hooks.py:260] loss = 1.9095688, step = 88100 (13.678 sec)
I0901 11:36:31.106872 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30231
I0901 11:36:31.107561 140646288807744 basic_session_run_hooks.py:260] loss = 2.0016103, step = 88200 (13.694 sec)
I0901 11:36:44.793165 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30658
I0901 11:36:44.794014 140646288807744 basic_session_run_hooks.py:260] loss = 1.5836705, step = 88300 (13.686 sec)
I0901 11:36:58.344899 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37913
I0901 11:36:58.345707 140646288807744 basic_session_run_hooks.py:260] loss = 1.7532051, step = 88400 (13.552 sec)
I0901 11:37:11.927930 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36213
I0901 11:37:11.928579 140646288807744 basic_session_run_hooks.py:260] loss = 2.0349166, step = 88500 (13.583 sec)
I0901 11:37:25.493814 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37143
I0901 11:37:25.494503 140646288807744 basic_session_run_hooks.py:260] loss = 2.0141406, step = 88600 (13.566 sec)
I0901 11:37:39.104655 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34708
I0901 11:37:39.105520 140646288807744 basic_session_run_hooks.py:260] loss = 1.962138, step = 88700 (13.611 sec)
I0901 11:37:52.822348 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28985
I0901 11:37:52.823085 140646288807744 basic_session_run_hooks.py:260] loss = 2.0202787, step = 88800 (13.718 sec)
I0901 11:38:06.371986 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38027
I0901 11:38:06.372788 140646288807744 basic_session_run_hooks.py:260] loss = 2.171143, step = 88900 (13.550 sec)
I0901 11:38:19.837279 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 89000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:38:21.007803 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:38:21.158619 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.76286
I0901 11:38:21.159392 140646288807744 basic_session_run_hooks.py:260] loss = 2.6546395, step = 89000 (14.787 sec)
I0901 11:38:35.041805 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.20298
I0901 11:38:35.043438 140646288807744 basic_session_run_hooks.py:260] loss = 1.9907383, step = 89100 (13.884 sec)
I0901 11:38:48.560567 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39711
I0901 11:38:48.561453 140646288807744 basic_session_run_hooks.py:260] loss = 2.0334635, step = 89200 (13.518 sec)
I0901 11:39:02.512362 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.16753
I0901 11:39:02.513205 140646288807744 basic_session_run_hooks.py:260] loss = 1.9777533, step = 89300 (13.952 sec)
I0901 11:39:16.106214 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35627
I0901 11:39:16.106819 140646288807744 basic_session_run_hooks.py:260] loss = 1.8383474, step = 89400 (13.594 sec)
I0901 11:39:29.801853 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30159
I0901 11:39:29.802639 140646288807744 basic_session_run_hooks.py:260] loss = 2.127524, step = 89500 (13.696 sec)
I0901 11:39:43.391174 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35872
I0901 11:39:43.391871 140646288807744 basic_session_run_hooks.py:260] loss = 2.2643826, step = 89600 (13.589 sec)
I0901 11:39:56.972573 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36301
I0901 11:39:56.973239 140646288807744 basic_session_run_hooks.py:260] loss = 2.188673, step = 89700 (13.581 sec)
I0901 11:40:10.588383 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3444
I0901 11:40:10.589061 140646288807744 basic_session_run_hooks.py:260] loss = 1.7787955, step = 89800 (13.616 sec)
I0901 11:40:24.390995 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24501
I0901 11:40:24.391668 140646288807744 basic_session_run_hooks.py:260] loss = 2.203474, step = 89900 (13.803 sec)
I0901 11:40:37.792318 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 90000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:40:38.967740 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:40:39.108751 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.79451
I0901 11:40:39.109534 140646288807744 basic_session_run_hooks.py:260] loss = 1.9999998, step = 90000 (14.718 sec)
I0901 11:40:52.659679 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37957
I0901 11:40:52.660417 140646288807744 basic_session_run_hooks.py:260] loss = 1.8574623, step = 90100 (13.551 sec)
I0901 11:41:06.138232 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41919
I0901 11:41:06.138852 140646288807744 basic_session_run_hooks.py:260] loss = 1.9785573, step = 90200 (13.478 sec)
I0901 11:41:19.761482 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34039
I0901 11:41:19.762114 140646288807744 basic_session_run_hooks.py:260] loss = 1.9104455, step = 90300 (13.623 sec)
I0901 11:41:33.388286 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33848
I0901 11:41:33.388955 140646288807744 basic_session_run_hooks.py:260] loss = 2.1999147, step = 90400 (13.627 sec)
I0901 11:41:46.912685 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39404
I0901 11:41:46.913368 140646288807744 basic_session_run_hooks.py:260] loss = 1.7312552, step = 90500 (13.524 sec)
I0901 11:42:00.533661 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34162
I0901 11:42:00.534382 140646288807744 basic_session_run_hooks.py:260] loss = 1.7260467, step = 90600 (13.621 sec)
I0901 11:42:14.137641 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35079
I0901 11:42:14.138495 140646288807744 basic_session_run_hooks.py:260] loss = 1.5825759, step = 90700 (13.604 sec)
I0901 11:42:27.763850 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3388
I0901 11:42:27.764684 140646288807744 basic_session_run_hooks.py:260] loss = 2.1970472, step = 90800 (13.626 sec)
I0901 11:42:41.226696 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42785
I0901 11:42:41.227405 140646288807744 basic_session_run_hooks.py:260] loss = 2.7242382, step = 90900 (13.463 sec)
I0901 11:42:54.739911 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 91000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:42:55.927976 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 11:42:55.928962 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 11:42:56.040273 140646288807744 estimator.py:1145] Calling model_fn.
I0901 11:42:56.040886 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 11:42:56.041127 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 11:42:56.041213 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 11:42:56.041302 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 11:42:56.041375 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 11:42:56.041464 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 11:42:56.041555 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 11:42:56.094081 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 11:42:56.140677 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 11:42:56.225792 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 11:42:56.233537 140646288807744 t2t_model.py:2248] Building model body
I0901 11:42:59.592725 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 11:43:00.017591 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 11:43:00.028928 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T11:43:00Z
I0901 11:43:00.392899 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 11:43:00.393367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:43:00.393656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 11:43:00.393715: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 11:43:00.393732: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 11:43:00.393744: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 11:43:00.393755: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 11:43:00.393766: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 11:43:00.393798: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 11:43:00.393838: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 11:43:00.393911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:43:00.394181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:43:00.394436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 11:43:00.394474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 11:43:00.394481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 11:43:00.394486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 11:43:00.394646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:43:00.394885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:43:00.395121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 11:43:00.395926 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-91000
I0901 11:43:01.014762 140646288807744 session_manager.py:500] Running local_init_op.
I0901 11:43:01.089166 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 11:43:04.013414 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 11:43:05.244759 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 11:43:06.513007 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 11:43:07.861595 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 11:43:09.147063 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 11:43:10.428741 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 11:43:11.613092 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 11:43:12.987269 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 11:43:14.265024 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 11:43:15.381849 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 11:43:15.496649 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-11:43:15
I0901 11:43:15.496795 140646288807744 estimator.py:2039] Saving dict for global step 91000: global_step = 91000, loss = 2.0823379, metrics-translate_ende_wmt8k/targets/accuracy = 0.6129289, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.7901726, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.3179008, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.0825768, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.3931075, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.5919505
I0901 11:43:15.497308 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 91000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-91000
I0901 11:43:15.628367 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.90684
I0901 11:43:15.629306 140646288807744 basic_session_run_hooks.py:260] loss = 2.067448, step = 91000 (34.402 sec)
I0901 11:43:29.077752 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43528
I0901 11:43:29.078431 140646288807744 basic_session_run_hooks.py:260] loss = 2.129534, step = 91100 (13.449 sec)
I0901 11:43:42.782007 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.297
I0901 11:43:42.782745 140646288807744 basic_session_run_hooks.py:260] loss = 1.9553452, step = 91200 (13.704 sec)
I0901 11:43:56.338888 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37633
I0901 11:43:56.339540 140646288807744 basic_session_run_hooks.py:260] loss = 2.0991557, step = 91300 (13.557 sec)
I0901 11:44:09.860396 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39562
I0901 11:44:09.861047 140646288807744 basic_session_run_hooks.py:260] loss = 1.5414593, step = 91400 (13.522 sec)
I0901 11:44:23.441473 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36319
I0901 11:44:23.442099 140646288807744 basic_session_run_hooks.py:260] loss = 1.1648228, step = 91500 (13.581 sec)
I0901 11:44:37.055261 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34549
I0901 11:44:37.055927 140646288807744 basic_session_run_hooks.py:260] loss = 1.8263105, step = 91600 (13.614 sec)
I0901 11:44:50.853045 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24754
I0901 11:44:50.853711 140646288807744 basic_session_run_hooks.py:260] loss = 1.8297746, step = 91700 (13.798 sec)
I0901 11:45:04.796151 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.172
I0901 11:45:04.796858 140646288807744 basic_session_run_hooks.py:260] loss = 1.467667, step = 91800 (13.943 sec)
I0901 11:45:18.314732 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39723
I0901 11:45:18.315659 140646288807744 basic_session_run_hooks.py:260] loss = 2.2067912, step = 91900 (13.519 sec)
2019-09-01 11:45:23.761509: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:45:23.761998: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:45:23.805245: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:45:23.805967: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:45:23.807116: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:45:23.807750: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 11:45:31.745383 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 92000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:45:32.947490 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:45:33.086997 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.76944
I0901 11:45:33.087854 140646288807744 basic_session_run_hooks.py:260] loss = 2.1462944, step = 92000 (14.772 sec)
I0901 11:45:46.760475 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31343
I0901 11:45:46.761096 140646288807744 basic_session_run_hooks.py:260] loss = 1.9280149, step = 92100 (13.673 sec)
I0901 11:46:00.478936 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28945
I0901 11:46:00.479622 140646288807744 basic_session_run_hooks.py:260] loss = 1.7466507, step = 92200 (13.719 sec)
I0901 11:46:14.029919 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37954
I0901 11:46:14.030524 140646288807744 basic_session_run_hooks.py:260] loss = 1.6783875, step = 92300 (13.551 sec)
I0901 11:46:27.547290 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39789
I0901 11:46:27.547980 140646288807744 basic_session_run_hooks.py:260] loss = 2.332078, step = 92400 (13.517 sec)
I0901 11:46:41.025325 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41948
I0901 11:46:41.026041 140646288807744 basic_session_run_hooks.py:260] loss = 1.9259026, step = 92500 (13.478 sec)
I0901 11:46:54.616842 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35753
I0901 11:46:54.617470 140646288807744 basic_session_run_hooks.py:260] loss = 1.9392444, step = 92600 (13.591 sec)
I0901 11:47:08.091907 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42112
I0901 11:47:08.092640 140646288807744 basic_session_run_hooks.py:260] loss = 1.9018574, step = 92700 (13.475 sec)
I0901 11:47:21.691333 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35325
I0901 11:47:21.692049 140646288807744 basic_session_run_hooks.py:260] loss = 2.3517869, step = 92800 (13.599 sec)
I0901 11:47:35.253582 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37342
I0901 11:47:35.254457 140646288807744 basic_session_run_hooks.py:260] loss = 1.8190453, step = 92900 (13.562 sec)
I0901 11:47:48.635506 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 93000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:47:49.810376 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:47:49.962758 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.79847
I0901 11:47:49.963548 140646288807744 basic_session_run_hooks.py:260] loss = 2.8577895, step = 93000 (14.709 sec)
I0901 11:48:03.666857 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29709
I0901 11:48:03.667589 140646288807744 basic_session_run_hooks.py:260] loss = 1.9957535, step = 93100 (13.704 sec)
I0901 11:48:17.169557 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40592
I0901 11:48:17.170357 140646288807744 basic_session_run_hooks.py:260] loss = 2.3356993, step = 93200 (13.503 sec)
I0901 11:48:30.664427 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.41022
I0901 11:48:30.665249 140646288807744 basic_session_run_hooks.py:260] loss = 1.6320999, step = 93300 (13.495 sec)
I0901 11:48:44.255301 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35788
I0901 11:48:44.255979 140646288807744 basic_session_run_hooks.py:260] loss = 2.0273929, step = 93400 (13.591 sec)
I0901 11:48:57.860168 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35031
I0901 11:48:57.860912 140646288807744 basic_session_run_hooks.py:260] loss = 1.6145455, step = 93500 (13.605 sec)
I0901 11:49:11.479805 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34234
I0901 11:49:11.480810 140646288807744 basic_session_run_hooks.py:260] loss = 1.8162946, step = 93600 (13.620 sec)
I0901 11:49:25.241006 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26681
I0901 11:49:25.241753 140646288807744 basic_session_run_hooks.py:260] loss = 2.2090247, step = 93700 (13.761 sec)
I0901 11:49:38.996888 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26962
I0901 11:49:38.997576 140646288807744 basic_session_run_hooks.py:260] loss = 1.7904212, step = 93800 (13.756 sec)
I0901 11:49:52.741535 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.27556
I0901 11:49:52.742413 140646288807744 basic_session_run_hooks.py:260] loss = 1.8462567, step = 93900 (13.745 sec)
I0901 11:50:06.268729 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 94000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:50:07.431181 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:50:07.576315 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.74091
I0901 11:50:07.577002 140646288807744 basic_session_run_hooks.py:260] loss = 1.9403137, step = 94000 (14.835 sec)
I0901 11:50:21.184553 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34849
I0901 11:50:21.185211 140646288807744 basic_session_run_hooks.py:260] loss = 1.6969606, step = 94100 (13.608 sec)
I0901 11:50:34.752759 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37017
I0901 11:50:34.753647 140646288807744 basic_session_run_hooks.py:260] loss = 1.9088287, step = 94200 (13.568 sec)
I0901 11:50:48.333451 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36339
I0901 11:50:48.334151 140646288807744 basic_session_run_hooks.py:260] loss = 2.14274, step = 94300 (13.581 sec)
I0901 11:51:02.018188 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30741
I0901 11:51:02.018849 140646288807744 basic_session_run_hooks.py:260] loss = 1.767337, step = 94400 (13.685 sec)
I0901 11:51:15.733678 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.29103
I0901 11:51:15.734488 140646288807744 basic_session_run_hooks.py:260] loss = 1.9135773, step = 94500 (13.716 sec)
I0901 11:51:29.267681 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3888
I0901 11:51:29.268382 140646288807744 basic_session_run_hooks.py:260] loss = 2.524045, step = 94600 (13.534 sec)
2019-09-01 11:51:37.089607: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.090094: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.126721: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.127177: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.227138: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.227607: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.228658: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.229111: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.285612: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.286102: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.287162: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 11:51:37.287628: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 11:51:42.828830 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.374
I0901 11:51:42.829631 140646288807744 basic_session_run_hooks.py:260] loss = 2.4079719, step = 94700 (13.561 sec)
I0901 11:51:56.407758 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36435
I0901 11:51:56.408402 140646288807744 basic_session_run_hooks.py:260] loss = 2.4388144, step = 94800 (13.579 sec)
I0901 11:52:09.995346 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35966
I0901 11:52:09.996047 140646288807744 basic_session_run_hooks.py:260] loss = 2.0685651, step = 94900 (13.588 sec)
I0901 11:52:23.279285 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 95000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:52:24.448601 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:52:24.578985 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.857
I0901 11:52:24.579837 140646288807744 basic_session_run_hooks.py:260] loss = 2.631077, step = 95000 (14.584 sec)
I0901 11:52:38.343731 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26494
I0901 11:52:38.344478 140646288807744 basic_session_run_hooks.py:260] loss = 2.0284276, step = 95100 (13.765 sec)
I0901 11:52:52.030555 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30629
I0901 11:52:52.031386 140646288807744 basic_session_run_hooks.py:260] loss = 2.1409302, step = 95200 (13.687 sec)
I0901 11:53:05.759195 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.28404
I0901 11:53:05.760011 140646288807744 basic_session_run_hooks.py:260] loss = 1.8487206, step = 95300 (13.729 sec)
I0901 11:53:19.327760 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36998
I0901 11:53:19.328649 140646288807744 basic_session_run_hooks.py:260] loss = 2.3663316, step = 95400 (13.569 sec)
I0901 11:53:32.959483 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33583
I0901 11:53:32.960374 140646288807744 basic_session_run_hooks.py:260] loss = 1.8274378, step = 95500 (13.632 sec)
I0901 11:53:46.806009 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.22203
I0901 11:53:46.806769 140646288807744 basic_session_run_hooks.py:260] loss = 2.4073572, step = 95600 (13.846 sec)
I0901 11:54:00.577420 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.26142
I0901 11:54:00.578442 140646288807744 basic_session_run_hooks.py:260] loss = 2.5616565, step = 95700 (13.772 sec)
I0901 11:54:14.126993 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38031
I0901 11:54:14.127635 140646288807744 basic_session_run_hooks.py:260] loss = 2.040267, step = 95800 (13.549 sec)
I0901 11:54:27.639129 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40075
I0901 11:54:27.639922 140646288807744 basic_session_run_hooks.py:260] loss = 2.6167688, step = 95900 (13.512 sec)
I0901 11:54:41.051419 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 96000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:54:42.222305 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 11:54:42.223292 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 11:54:42.339991 140646288807744 estimator.py:1145] Calling model_fn.
I0901 11:54:42.340731 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 11:54:42.341052 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 11:54:42.341139 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 11:54:42.341220 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 11:54:42.341289 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 11:54:42.341368 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 11:54:42.341430 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 11:54:42.396810 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 11:54:42.446167 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 11:54:42.531424 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 11:54:42.539470 140646288807744 t2t_model.py:2248] Building model body
I0901 11:54:45.738180 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 11:54:46.156525 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 11:54:46.167969 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T11:54:46Z
I0901 11:54:46.529772 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 11:54:46.530275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:54:46.530695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 11:54:46.530778: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 11:54:46.530813: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 11:54:46.530830: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 11:54:46.530846: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 11:54:46.530890: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 11:54:46.530913: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 11:54:46.530959: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 11:54:46.531029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:54:46.531394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:54:46.531684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 11:54:46.531730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 11:54:46.531743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 11:54:46.531753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 11:54:46.531860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:54:46.532236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 11:54:46.532669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 11:54:46.533786 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-96000
I0901 11:54:47.152655 140646288807744 session_manager.py:500] Running local_init_op.
I0901 11:54:47.227520 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 11:54:50.159990 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 11:54:51.396944 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 11:54:52.626711 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 11:54:53.887234 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 11:54:55.048027 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 11:54:56.307956 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 11:54:57.602956 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 11:54:58.871729 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 11:55:00.209239 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 11:55:01.411556 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 11:55:01.561769 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-11:55:01
I0901 11:55:01.561945 140646288807744 estimator.py:2039] Saving dict for global step 96000: global_step = 96000, loss = 2.0583313, metrics-translate_ende_wmt8k/targets/accuracy = 0.61654425, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.794345, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.32101533, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.05861, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.3966524, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.5947908
I0901 11:55:01.562626 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 96000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-96000
I0901 11:55:01.689185 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 2.93685
I0901 11:55:01.689849 140646288807744 basic_session_run_hooks.py:260] loss = 2.134412, step = 96000 (34.050 sec)
I0901 11:55:15.323318 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33453
I0901 11:55:15.324145 140646288807744 basic_session_run_hooks.py:260] loss = 1.8990538, step = 96100 (13.634 sec)
I0901 11:55:28.782557 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.42984
I0901 11:55:28.783211 140646288807744 basic_session_run_hooks.py:260] loss = 1.4142107, step = 96200 (13.459 sec)
I0901 11:55:42.286605 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.40519
I0901 11:55:42.287421 140646288807744 basic_session_run_hooks.py:260] loss = 1.9294393, step = 96300 (13.504 sec)
I0901 11:55:55.898803 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34635
I0901 11:55:55.899625 140646288807744 basic_session_run_hooks.py:260] loss = 2.5409734, step = 96400 (13.612 sec)
I0901 11:56:09.568481 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31546
I0901 11:56:09.569368 140646288807744 basic_session_run_hooks.py:260] loss = 1.4267175, step = 96500 (13.670 sec)
I0901 11:56:23.375962 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.24245
I0901 11:56:23.376672 140646288807744 basic_session_run_hooks.py:260] loss = 2.0407512, step = 96600 (13.807 sec)
I0901 11:56:36.966585 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35802
I0901 11:56:36.967386 140646288807744 basic_session_run_hooks.py:260] loss = 2.5317097, step = 96700 (13.591 sec)
I0901 11:56:50.592895 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33874
I0901 11:56:50.593541 140646288807744 basic_session_run_hooks.py:260] loss = 2.057877, step = 96800 (13.626 sec)
I0901 11:57:04.225694 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.33525
I0901 11:57:04.226495 140646288807744 basic_session_run_hooks.py:260] loss = 1.74396, step = 96900 (13.633 sec)
I0901 11:57:17.846411 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 97000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:57:19.033668 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:57:19.178814 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.68757
I0901 11:57:19.179543 140646288807744 basic_session_run_hooks.py:260] loss = 2.0750716, step = 97000 (14.953 sec)
I0901 11:57:32.751369 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36781
I0901 11:57:32.752241 140646288807744 basic_session_run_hooks.py:260] loss = 1.8862269, step = 97100 (13.573 sec)
I0901 11:57:46.336233 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36113
I0901 11:57:46.336955 140646288807744 basic_session_run_hooks.py:260] loss = 2.0685751, step = 97200 (13.585 sec)
I0901 11:57:59.784384 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.43597
I0901 11:57:59.785089 140646288807744 basic_session_run_hooks.py:260] loss = 2.131245, step = 97300 (13.448 sec)
I0901 11:58:13.478585 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.30236
I0901 11:58:13.479350 140646288807744 basic_session_run_hooks.py:260] loss = 2.1410906, step = 97400 (13.694 sec)
I0901 11:58:27.080619 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35184
I0901 11:58:27.081480 140646288807744 basic_session_run_hooks.py:260] loss = 2.002448, step = 97500 (13.602 sec)
I0901 11:58:40.686413 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34981
I0901 11:58:40.687204 140646288807744 basic_session_run_hooks.py:260] loss = 2.1106093, step = 97600 (13.606 sec)
I0901 11:58:54.271431 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36105
I0901 11:58:54.272172 140646288807744 basic_session_run_hooks.py:260] loss = 1.8295892, step = 97700 (13.585 sec)
I0901 11:59:07.855421 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36161
I0901 11:59:07.856076 140646288807744 basic_session_run_hooks.py:260] loss = 2.0749257, step = 97800 (13.584 sec)
I0901 11:59:21.413411 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37573
I0901 11:59:21.414112 140646288807744 basic_session_run_hooks.py:260] loss = 2.2136152, step = 97900 (13.558 sec)
I0901 11:59:35.141723 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 98000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 11:59:36.346510 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 11:59:36.491779 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.63202
I0901 11:59:36.492463 140646288807744 basic_session_run_hooks.py:260] loss = 1.9437912, step = 98000 (15.078 sec)
I0901 11:59:50.099336 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34886
I0901 11:59:50.100042 140646288807744 basic_session_run_hooks.py:260] loss = 2.1368985, step = 98100 (13.608 sec)
I0901 12:00:03.708766 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34785
I0901 12:00:03.709437 140646288807744 basic_session_run_hooks.py:260] loss = 1.9787345, step = 98200 (13.609 sec)
I0901 12:00:17.290316 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36293
I0901 12:00:17.290998 140646288807744 basic_session_run_hooks.py:260] loss = 1.3948002, step = 98300 (13.582 sec)
I0901 12:00:30.847782 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37601
I0901 12:00:30.848912 140646288807744 basic_session_run_hooks.py:260] loss = 2.0303652, step = 98400 (13.558 sec)
I0901 12:00:44.372674 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39378
I0901 12:00:44.373412 140646288807744 basic_session_run_hooks.py:260] loss = 1.6035773, step = 98500 (13.525 sec)
I0901 12:00:57.987116 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34514
I0901 12:00:57.987921 140646288807744 basic_session_run_hooks.py:260] loss = 1.6932747, step = 98600 (13.615 sec)
I0901 12:01:11.659935 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.31378
I0901 12:01:11.660636 140646288807744 basic_session_run_hooks.py:260] loss = 2.1822357, step = 98700 (13.673 sec)
I0901 12:01:25.201237 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38481
I0901 12:01:25.201946 140646288807744 basic_session_run_hooks.py:260] loss = 2.298559, step = 98800 (13.541 sec)
I0901 12:01:38.766374 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.37184
I0901 12:01:38.767139 140646288807744 basic_session_run_hooks.py:260] loss = 1.6047935, step = 98900 (13.565 sec)
I0901 12:01:52.383871 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 99000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 12:01:53.552790 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 12:01:53.695453 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 6.69834
I0901 12:01:53.696084 140646288807744 basic_session_run_hooks.py:260] loss = 2.119825, step = 99000 (14.929 sec)
I0901 12:02:07.266522 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.36863
I0901 12:02:07.267236 140646288807744 basic_session_run_hooks.py:260] loss = 1.8340839, step = 99100 (13.571 sec)
I0901 12:02:20.821787 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3772
I0901 12:02:20.822600 140646288807744 basic_session_run_hooks.py:260] loss = 2.3117816, step = 99200 (13.555 sec)
I0901 12:02:34.492321 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.315
I0901 12:02:34.493119 140646288807744 basic_session_run_hooks.py:260] loss = 1.8561549, step = 99300 (13.671 sec)
I0901 12:02:48.162486 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.3152
I0901 12:02:48.163225 140646288807744 basic_session_run_hooks.py:260] loss = 2.8626041, step = 99400 (13.670 sec)
I0901 12:03:01.770497 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.34861
I0901 12:03:01.771265 140646288807744 basic_session_run_hooks.py:260] loss = 2.2074642, step = 99500 (13.608 sec)
I0901 12:03:15.292738 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.39523
I0901 12:03:15.293431 140646288807744 basic_session_run_hooks.py:260] loss = 2.8310466, step = 99600 (13.522 sec)
I0901 12:03:28.836388 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38353
I0901 12:03:28.837023 140646288807744 basic_session_run_hooks.py:260] loss = 1.7836773, step = 99700 (13.544 sec)
I0901 12:03:42.383139 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.38184
I0901 12:03:42.383753 140646288807744 basic_session_run_hooks.py:260] loss = 1.9582697, step = 99800 (13.547 sec)
2019-09-01 12:03:43.340711: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.349619: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.370408: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.370869: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.468402: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.468863: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.469775: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.470215: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.526217: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.526673: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.527593: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-09-01 12:03:43.528038: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 3.40G (3649670144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
I0901 12:03:55.980486 140646288807744 basic_session_run_hooks.py:692] global_step/sec: 7.35438
I0901 12:03:55.981163 140646288807744 basic_session_run_hooks.py:260] loss = 1.7435118, step = 99900 (13.597 sec)
I0901 12:04:09.339596 140646288807744 basic_session_run_hooks.py:606] Saving checkpoints for 100000 into /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt.
I0901 12:04:10.504586 140646288807744 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).
I0901 12:04:10.510404 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 12:04:10.511541 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 12:04:10.627543 140646288807744 estimator.py:1145] Calling model_fn.
I0901 12:04:10.628091 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 12:04:10.628300 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 12:04:10.628357 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 12:04:10.628411 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 12:04:10.628458 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 12:04:10.628508 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 12:04:10.628551 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 12:04:10.683146 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 12:04:10.731283 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 12:04:10.814723 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 12:04:10.822324 140646288807744 t2t_model.py:2248] Building model body
I0901 12:04:13.956989 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 12:04:14.608412 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 12:04:14.619723 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T12:04:14Z
I0901 12:04:14.979910 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 12:04:14.980356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 12:04:14.980623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 12:04:14.980691: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 12:04:14.980708: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 12:04:14.980724: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 12:04:14.980767: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 12:04:14.980783: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 12:04:14.980798: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 12:04:14.980841: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 12:04:14.980903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 12:04:14.981169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 12:04:14.981355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 12:04:14.981378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 12:04:14.981386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 12:04:14.981394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 12:04:14.981451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 12:04:14.981743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 12:04:14.981963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 12:04:14.982845 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-100000
I0901 12:04:15.598729 140646288807744 session_manager.py:500] Running local_init_op.
I0901 12:04:15.675855 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 12:04:18.556652 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 12:04:19.882018 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 12:04:24.409132 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 12:04:26.747909 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 12:04:27.886849 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 12:04:29.165812 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 12:04:30.548408 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 12:04:31.859529 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 12:04:33.209971 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 12:04:34.471204 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 12:04:34.602770 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-12:04:34
I0901 12:04:34.602945 140646288807744 estimator.py:2039] Saving dict for global step 100000: global_step = 100000, loss = 2.0508974, metrics-translate_ende_wmt8k/targets/accuracy = 0.61738557, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.79396987, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.3206876, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.0512145, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.39692298, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.5956372
I0901 12:04:34.603305 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 100000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-100000
I0901 12:04:34.781380 140646288807744 estimator.py:368] Loss for final step: 1.8501374.
I0901 12:04:34.786996 140646288807744 problem.py:644] Reading data files from /home/chrisf/t2t_data/translate_ende_wmt8k-dev*
I0901 12:04:34.788046 140646288807744 problem.py:670] partition: 0 num_data_files: 1
I0901 12:04:34.900575 140646288807744 estimator.py:1145] Calling model_fn.
I0901 12:04:34.901090 140646288807744 t2t_model.py:2248] Setting T2TModel mode to 'eval'
I0901 12:04:34.901285 140646288807744 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 12:04:34.901334 140646288807744 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 12:04:34.901377 140646288807744 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 12:04:34.901448 140646288807744 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 12:04:34.901509 140646288807744 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 12:04:34.901544 140646288807744 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
I0901 12:04:34.954118 140646288807744 api.py:255] Using variable initializer: uniform_unit_scaling
I0901 12:04:35.000710 140646288807744 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 12:04:35.083054 140646288807744 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 12:04:35.090671 140646288807744 t2t_model.py:2248] Building model body
I0901 12:04:37.832828 140646288807744 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 12:04:38.836714 140646288807744 estimator.py:1147] Done calling model_fn.
I0901 12:04:38.847553 140646288807744 evaluation.py:255] Starting evaluation at 2019-09-01T12:04:38Z
I0901 12:04:39.189042 140646288807744 monitored_session.py:240] Graph was finalized.
2019-09-01 12:04:39.189516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 12:04:39.189767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-09-01 12:04:39.189818: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-09-01 12:04:39.189830: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-09-01 12:04:39.189869: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-09-01 12:04:39.189879: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-09-01 12:04:39.189890: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-09-01 12:04:39.189901: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-09-01 12:04:39.189912: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-01 12:04:39.189980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 12:04:39.190297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 12:04:39.190505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-09-01 12:04:39.190541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-01 12:04:39.190547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-09-01 12:04:39.190553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-09-01 12:04:39.190603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 12:04:39.190842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-01 12:04:39.191072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7575 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
I0901 12:04:39.191920 140646288807744 saver.py:1280] Restoring parameters from /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-100000
I0901 12:04:39.805370 140646288807744 session_manager.py:500] Running local_init_op.
I0901 12:04:39.881188 140646288807744 session_manager.py:502] Done running local_init_op.
I0901 12:04:42.844996 140646288807744 evaluation.py:167] Evaluation [10/100]
I0901 12:04:44.075551 140646288807744 evaluation.py:167] Evaluation [20/100]
I0901 12:04:45.385326 140646288807744 evaluation.py:167] Evaluation [30/100]
I0901 12:04:46.574983 140646288807744 evaluation.py:167] Evaluation [40/100]
I0901 12:04:47.806945 140646288807744 evaluation.py:167] Evaluation [50/100]
I0901 12:04:49.051443 140646288807744 evaluation.py:167] Evaluation [60/100]
I0901 12:04:50.188848 140646288807744 evaluation.py:167] Evaluation [70/100]
I0901 12:04:51.450147 140646288807744 evaluation.py:167] Evaluation [80/100]
I0901 12:04:52.766132 140646288807744 evaluation.py:167] Evaluation [90/100]
I0901 12:04:54.019645 140646288807744 evaluation.py:167] Evaluation [100/100]
I0901 12:04:54.132277 140646288807744 evaluation.py:275] Finished evaluation at 2019-09-01-12:04:54
I0901 12:04:54.132433 140646288807744 estimator.py:2039] Saving dict for global step 100000: global_step = 100000, loss = 2.0508974, metrics-translate_ende_wmt8k/targets/accuracy = 0.61738557, metrics-translate_ende_wmt8k/targets/accuracy_per_sequence = 0.0, metrics-translate_ende_wmt8k/targets/accuracy_top5 = 0.79396987, metrics-translate_ende_wmt8k/targets/approx_bleu_score = 0.3206876, metrics-translate_ende_wmt8k/targets/neg_log_perplexity = -2.0512145, metrics-translate_ende_wmt8k/targets/rouge_2_fscore = 0.39692298, metrics-translate_ende_wmt8k/targets/rouge_L_fscore = 0.5956372
I0901 12:04:54.650030 140646288807744 estimator.py:2099] Saving 'checkpoint_path' summary for global step 100000: /home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312/model.ckpt-100000





HPARAMS3!!










INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632







INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632


WARNING: Logging before flag parsing goes to stderr.
W0901 12:04:58.253840 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/expert_utils.py:68: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0901 12:04:59.255329 140073317975872 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0901 12:05:00.667707 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/adafactor.py:27: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0901 12:05:00.668302 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/multistep_optimizer.py:32: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0901 12:05:00.714398 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/mesh_tensorflow/ops.py:4237: The name tf.train.CheckpointSaverListener is deprecated. Please use tf.estimator.CheckpointSaverListener instead.

W0901 12:05:00.714581 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/mesh_tensorflow/ops.py:4260: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

W0901 12:05:00.745993 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/models/research/neural_stack.py:38: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

W0901 12:05:00.819901 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/rl/gym_utils.py:235: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

W0901 12:05:00.856518 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:111: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0901 12:05:00.892116 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_gan/python/contrib_utils.py:305: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.

W0901 12:05:00.892273 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_gan/python/contrib_utils.py:310: The name tf.estimator.tpu.TPUEstimatorSpec is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimatorSpec instead.

W0901 12:05:01.768132 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-decoder:16: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W0901 12:05:01.768287 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-decoder:16: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W0901 12:05:01.768410 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-decoder:17: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

W0901 12:05:01.768840 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:839: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

I0901 12:05:01.769295 140073317975872 usr_dir.py:43] Importing user module Language_Model_April2019_Restart from path /home/chrisf/t2t_user_dir/DEFENSE_langage_model_experiements
W0901 12:05:01.773139 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/data_generators/text_encoder.py:938: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.

W0901 12:05:01.773268 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/data_generators/text_encoder.py:940: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.

W0901 12:05:01.795454 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:123: The name tf.GraphOptions is deprecated. Please use tf.compat.v1.GraphOptions instead.

W0901 12:05:01.795653 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:129: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0901 12:05:01.795799 140073317975872 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:242: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.
Instructions for updating:
When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.
I0901 12:05:01.795920 140073317975872 trainer_lib.py:265] Configuring DataParallelism to replicate the model.
I0901 12:05:01.795965 140073317975872 devices.py:76] schedule=continuous_train_and_eval
I0901 12:05:01.796011 140073317975872 devices.py:77] worker_gpu=1
I0901 12:05:01.796046 140073317975872 devices.py:78] sync=False
W0901 12:05:01.796091 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/devices.py:139: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.

W0901 12:05:01.796158 140073317975872 devices.py:141] Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.
I0901 12:05:01.800675 140073317975872 devices.py:170] datashard_devices: ['gpu:0']
I0901 12:05:01.800820 140073317975872 devices.py:171] caching_devices: None
I0901 12:05:01.800917 140073317975872 devices.py:172] ps_devices: ['gpu:0']
I0901 12:05:01.801322 140073317975872 estimator.py:209] Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f64f931e890>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {
  per_process_gpu_memory_fraction: 0.95
}
allow_soft_placement: true
graph_options {
  optimizer_options {
    global_jit_level: OFF
  }
}
isolate_session_state: true
, '_save_checkpoints_steps': 1000, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/home/chrisf/t2t_train/translate_ende_wmt8k/conv_transformer_april2019-conv_transformer_exp1_ctweqnumparams2-newtest_exp_debug_decoding_general_newSept_312', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f64f931e910>}
W0901 12:05:01.801469 140073317975872 model_fn.py:630] Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7f64f938eb00>) includes params argument, but params are not passed to Estimator.
I0901 12:05:01.801561 140073317975872 decoding.py:404] decode_hp.batch_size not specified; default=32
I0901 12:05:01.801634 140073317975872 decoding.py:415] Performing decoding from file (/home/chrisf/t2t_data/newstest2014.en).
I0901 12:05:01.801682 140073317975872 decoding.py:860] Getting sorted inputs
I0901 12:05:01.818815 140073317975872 decoding.py:673]  batch 86
I0901 12:05:01.818901 140073317975872 decoding.py:675] Decoding batch 0
W0901 12:05:01.824447 140073317975872 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/decoding.py:617: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0901 12:05:01.825883 140073317975872 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/decoding.py:950: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0901 12:05:01.829036 140073317975872 estimator.py:1000] Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.
I0901 12:05:01.829301 140073317975872 estimator.py:1145] Calling model_fn.
I0901 12:05:01.829833 140073317975872 t2t_model.py:2248] Setting T2TModel mode to 'infer'
I0901 12:05:01.830055 140073317975872 t2t_model.py:2248] Setting hparams.dropout to 0.0
I0901 12:05:01.830131 140073317975872 t2t_model.py:2248] Setting hparams.label_smoothing to 0.0
I0901 12:05:01.830208 140073317975872 t2t_model.py:2248] Setting hparams.layer_prepostprocess_dropout to 0.0
I0901 12:05:01.830278 140073317975872 t2t_model.py:2248] Setting hparams.symbol_dropout to 0.0
I0901 12:05:01.830349 140073317975872 t2t_model.py:2248] Setting hparams.attention_dropout to 0.0
I0901 12:05:01.830398 140073317975872 t2t_model.py:2248] Setting hparams.relu_dropout to 0.0
W0901 12:05:01.870337 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py:244: The name tf.summary.text is deprecated. Please use tf.compat.v1.summary.text instead.

I0901 12:05:01.877750 140073317975872 t2t_model.py:2248] Beam Decoding with beam size 4
W0901 12:05:01.915647 140073317975872 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/beam_search.py:745: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
I0901 12:05:02.405908 140073317975872 api.py:255] Using variable initializer: uniform_unit_scaling
W0901 12:05:02.430367 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

I0901 12:05:02.610320 140073317975872 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_8113_304.bottom
I0901 12:05:02.702304 140073317975872 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_8113_304.targets_bottom
I0901 12:05:02.710683 140073317975872 t2t_model.py:2248] Building model body
W0901 12:05:02.730011 140073317975872 deprecation.py:506] From /home/chrisf/t2t_user_dir/DEFENSE_langage_model_experiements/Language_Model_April2019_Restart/ConvTransformer_T2TApril2019_2.py:2677: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0901 12:05:02.733997 140073317975872 deprecation.py:323] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/expert_utils.py:621: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0901 12:05:02.751931 140073317975872 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/layers/common_layers.py:3077: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.

W0901 12:05:02.922347 140073317975872 deprecation.py:506] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0901 12:05:05.860945 140073317975872 t2t_model.py:2248] Transforming body output with symbol_modality_8113_304.top
I0901 12:05:05.957027 140073317975872 control_flow_util.py:349] Cannot use 'while/Pad' as input to 'Identity_121' because 'while/Pad' is in a while loop.

Identity_121 while context: None
while/Pad while context: while/while_context

Traceback for Identity_121:
  File "/home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-decoder", line 17, in <module>
    tf.app.run()
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/absl/app.py", line 300, in run
    _run_main(main, args)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/absl/app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "/home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-decoder", line 12, in main
    t2t_decoder.main(argv)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/bin/t2t_decoder.py", line 205, in main
    decode(estimator, hp, decode_hp)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/bin/t2t_decoder.py", line 94, in decode
    checkpoint_path=FLAGS.checkpoint_path)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/decoding.py", line 474, in decode_from_file
    for elapsed_time, result in timer(result_iter):
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/decoding.py", line 468, in timer
    item = next(gen)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 619, in predict
    features, None, ModeKeys.PREDICT, self.config)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1146, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 1415, in wrapping_model_fn
    use_tpu=use_tpu)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 1472, in estimator_model_fn
    return model.estimator_spec_predict(features, use_tpu=use_tpu)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 1708, in estimator_spec_predict
    features[name] = tf.identity(feature)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py", line 180, in wrapper
    return target(*args, **kwargs)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py", line 86, in identity
    ret = gen_array_ops.identity(input, name=name)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py", line 4253, in identity
    "Identity", input=input, name=name)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 3616, in create_op
    op_def=op_def)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

Traceback for while/Pad:
  File "/home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-decoder", line 17, in <module>
    tf.app.run()
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/absl/app.py", line 300, in run
    _run_main(main, args)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/absl/app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "/home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-decoder", line 12, in main
    t2t_decoder.main(argv)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/bin/t2t_decoder.py", line 205, in main
    decode(estimator, hp, decode_hp)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/bin/t2t_decoder.py", line 94, in decode
    checkpoint_path=FLAGS.checkpoint_path)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/decoding.py", line 474, in decode_from_file
    for elapsed_time, result in timer(result_iter):
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/decoding.py", line 468, in timer
    item = next(gen)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 619, in predict
    features, None, ModeKeys.PREDICT, self.config)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1146, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 1415, in wrapping_model_fn
    use_tpu=use_tpu)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 1472, in estimator_model_fn
    return model.estimator_spec_predict(features, use_tpu=use_tpu)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 1696, in estimator_spec_predict
    use_tpu=use_tpu)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 817, in infer
    top_beams, alpha, use_tpu)
  File "/home/chrisf/t2t_user_dir/DEFENSE_langage_model_experiements/Language_Model_April2019_Restart/ConvTransformer_T2TApril2019_2.py", line 3030, in _beam_decode
    top_beams, alpha, use_tpu)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 940, in _beam_decode_slow
    use_tpu=use_tpu)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/beam_search.py", line 800, in beam_search
    back_prop=False)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 3501, in while_loop
    return_same_structure)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 3012, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2937, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/beam_search.py", line 717, in inner_loop
    i, alive_seq, alive_log_probs, states)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/beam_search.py", line 597, in grow_topk
    flat_logits = symbols_to_logits_fn(flat_ids)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 873, in symbols_to_logits_fn
    ids = tf.pad(ids[:, 1:], [[0, 0], [0, 1], [0, 0], [0, 0]])
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py", line 2367, in pad
    result = gen_array_ops.pad(tensor, paddings, name=name)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py", line 5993, in pad
    "Pad", input=input, paddings=paddings, name=name)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 3616, in create_op
    op_def=op_def)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()







HPARAMS3!!










INIT CONVT!!










TRANSFORMER PREPARE ENCODER!!










TRANSFORMER PREPARE DECODER!!!






NUMBER OF PARAMTERS: 
61136632


Traceback (most recent call last):
  File "/home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-decoder", line 17, in <module>
    tf.app.run()
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/absl/app.py", line 300, in run
    _run_main(main, args)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/absl/app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "/home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-decoder", line 12, in main
    t2t_decoder.main(argv)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/bin/t2t_decoder.py", line 205, in main
    decode(estimator, hp, decode_hp)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/bin/t2t_decoder.py", line 94, in decode
    checkpoint_path=FLAGS.checkpoint_path)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/decoding.py", line 474, in decode_from_file
    for elapsed_time, result in timer(result_iter):
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/decoding.py", line 468, in timer
    item = next(gen)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 619, in predict
    features, None, ModeKeys.PREDICT, self.config)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1146, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 1415, in wrapping_model_fn
    use_tpu=use_tpu)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 1472, in estimator_model_fn
    return model.estimator_spec_predict(features, use_tpu=use_tpu)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/t2t_model.py", line 1708, in estimator_spec_predict
    features[name] = tf.identity(feature)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py", line 180, in wrapper
    return target(*args, **kwargs)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py", line 86, in identity
    ret = gen_array_ops.identity(input, name=name)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py", line 4253, in identity
    "Identity", input=input, name=name)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 3616, in create_op
    op_def=op_def)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 2043, in __init__
    self._control_flow_post_processing()
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 2052, in _control_flow_post_processing
    control_flow_util.CheckInputFromValidContext(self, input_tensor.op)
  File "/home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_util.py", line 350, in CheckInputFromValidContext
    raise ValueError(error_msg + " See info log for more details.")
ValueError: Cannot use 'while/Pad' as input to 'Identity_121' because 'while/Pad' is in a while loop. See info log for more details.
WARNING: Logging before flag parsing goes to stderr.
W0901 12:05:08.231672 139731499132736 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-bleu:17: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W0901 12:05:08.231832 139731499132736 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-bleu:17: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W0901 12:05:08.231978 139731499132736 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/bin/t2t-bleu:18: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

W0901 12:05:08.232341 139731499132736 deprecation_wrapper.py:119] From /home/chrisf/anaconda3/envs/thesis_tf/lib/python3.7/site-packages/tensor2tensor/utils/bleu_hook.py:205: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.

BLEU_uncased =  16.52
BLEU_cased =  16.13
